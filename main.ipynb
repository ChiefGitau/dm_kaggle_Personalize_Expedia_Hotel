{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 10.0% of the data based on srch_id...\n",
      "Sampled data shape: (496228, 55)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Experiment Notebook for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb # Keep for type hints if needed, but direct use will be less\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# Import the modularized model functions\n",
    "import lightgbm_ranker_model as lgbm_model\n",
    "import warnings # For managing warnings from Optuna/LightGBM if needed\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATA_DIR = '../data.nosync'\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'train_imputed.csv')\n",
    "TEST_FILE = os.path.join(DATA_DIR, 'test.csv') # Defined TEST_FILE path\n",
    "SUBMISSION_FILENAME = 'submission_modular.csv' # Defined submission filename\n",
    "\n",
    "SAMPLE_FRACTION = 0.1 # Use 10% of the data for faster runs during development\n",
    "N_FOLDS_CV = 5         # Number of folds for general cross-validation\n",
    "N_FOLDS_TUNING = 3     # Number of folds for Optuna trials (can be smaller for speed)\n",
    "N_OPTUNA_TRIALS = 20   # Number of Optuna trials\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training file not found at {TRAIN_FILE}\")\n",
    "    df_train_full = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    df_train_full = None\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2 # Map to 0, 1, 2 for label_gain [0,1,5]\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation as df_train_full is None.\")\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "df_sample = None\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    if len(unique_srch_ids) > 0:\n",
    "        sampled_srch_ids_count = int(len(unique_srch_ids) * SAMPLE_FRACTION)\n",
    "        if sampled_srch_ids_count > 0:\n",
    "            sampled_srch_ids = np.random.choice(unique_srch_ids, size=sampled_srch_ids_count, replace=False)\n",
    "            df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "            print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "        else:\n",
    "            print(\"Sample fraction resulted in zero search IDs. Check SAMPLE_FRACTION or dataset size.\")\n",
    "            df_sample = df_train_full.copy() # Fallback to full if sample is too small\n",
    "            print(f\"Using full dataset instead. Shape: {df_sample.shape}\")\n",
    "    else:\n",
    "        print(\"No unique search IDs found in the training data.\")\n",
    "    del df_train_full # Optional: free up memory\n",
    "else:\n",
    "    print(\"Skipping sampling as df_train_full is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>7814</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>10881</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>12510</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>17122</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>18012</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958140</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>30933</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958141</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>46986</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>156.75</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958142</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>62314</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958143</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>91660</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958144</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>92352</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496228 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "119           12  2013-03-15 14:55:16        5                          219   \n",
       "120           12  2013-03-15 14:55:16        5                          219   \n",
       "121           12  2013-03-15 14:55:16        5                          219   \n",
       "122           12  2013-03-15 14:55:16        5                          219   \n",
       "123           12  2013-03-15 14:55:16        5                          219   \n",
       "...          ...                  ...      ...                          ...   \n",
       "4958140   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958141   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958142   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958143   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958144   332763  2012-12-04 18:55:04       32                          220   \n",
       "\n",
       "         visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  \\\n",
       "119                          NaN                   NaN              158   \n",
       "120                          NaN                   NaN              158   \n",
       "121                          NaN                   NaN              158   \n",
       "122                          NaN                   NaN              158   \n",
       "123                          NaN                   NaN              158   \n",
       "...                          ...                   ...              ...   \n",
       "4958140                      NaN                   NaN               81   \n",
       "4958141                      NaN                   NaN               81   \n",
       "4958142                      NaN                   NaN               81   \n",
       "4958143                      NaN                   NaN               81   \n",
       "4958144                      NaN                   NaN               81   \n",
       "\n",
       "         prop_id  prop_starrating  prop_review_score  ...  comp7_rate  \\\n",
       "119         7814                3                3.5  ...         NaN   \n",
       "120        10881                3                3.0  ...         NaN   \n",
       "121        12510                4                0.0  ...         NaN   \n",
       "122        17122                4                3.5  ...         NaN   \n",
       "123        18012                5                4.5  ...         NaN   \n",
       "...          ...              ...                ...  ...         ...   \n",
       "4958140    30933                3                2.5  ...         NaN   \n",
       "4958141    46986                3                3.0  ...         NaN   \n",
       "4958142    62314                4                3.0  ...         NaN   \n",
       "4958143    91660                3                0.0  ...         NaN   \n",
       "4958144    92352                4                4.0  ...         NaN   \n",
       "\n",
       "         comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "119            NaN                      NaN         NaN        NaN   \n",
       "120            NaN                      NaN         NaN        NaN   \n",
       "121            NaN                      NaN         NaN        NaN   \n",
       "122            NaN                      NaN         NaN        NaN   \n",
       "123            NaN                      NaN         NaN        NaN   \n",
       "...            ...                      ...         ...        ...   \n",
       "4958140        NaN                      NaN         NaN        NaN   \n",
       "4958141        NaN                      NaN         NaN        NaN   \n",
       "4958142        NaN                      NaN         NaN        NaN   \n",
       "4958143        NaN                      NaN         NaN        NaN   \n",
       "4958144        NaN                      NaN         NaN        NaN   \n",
       "\n",
       "         comp8_rate_percent_diff  click_bool  gross_bookings_usd  \\\n",
       "119                          NaN           0                 NaN   \n",
       "120                          NaN           0                 NaN   \n",
       "121                          NaN           0                 NaN   \n",
       "122                          NaN           0                 NaN   \n",
       "123                          NaN           0                 NaN   \n",
       "...                          ...         ...                 ...   \n",
       "4958140                      NaN           0                 NaN   \n",
       "4958141                      NaN           1              156.75   \n",
       "4958142                      NaN           0                 NaN   \n",
       "4958143                      NaN           0                 NaN   \n",
       "4958144                      NaN           0                 NaN   \n",
       "\n",
       "         booking_bool  relevance  \n",
       "119                 0          0  \n",
       "120                 0          0  \n",
       "121                 0          0  \n",
       "122                 0          0  \n",
       "123                 0          0  \n",
       "...               ...        ...  \n",
       "4958140             0          0  \n",
       "4958141             1          2  \n",
       "4958142             0          0  \n",
       "4958143             0          0  \n",
       "4958144             0          0  \n",
       "\n",
       "[496228 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set and preparing X, y, groups...\n",
      "Performing basic median imputation for numerical features in X...\n",
      "Selected 11 features: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'orig_destination_distance']\n",
      "Shape of X: (496228, 11), Shape of y: (496228,)\n",
      "NaNs remaining in X after imputation: 0\n",
      "Number of unique groups for splitting: 19979\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>prop_log_historical_price</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>promotion_flag</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>4.46</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9270.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>4.46</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9285.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>4.41</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9288.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>4.22</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9278.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>5.66</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9286.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     visitor_location_country_id  prop_country_id  prop_starrating  \\\n",
       "119                          219              158                3   \n",
       "120                          219              158                3   \n",
       "121                          219              158                4   \n",
       "122                          219              158                4   \n",
       "123                          219              158                5   \n",
       "\n",
       "     prop_review_score  prop_brand_bool  prop_location_score1  \\\n",
       "119                3.5                0                  0.00   \n",
       "120                3.0                0                  3.14   \n",
       "121                0.0                0                  1.10   \n",
       "122                3.5                0                  2.64   \n",
       "123                4.5                0                  3.22   \n",
       "\n",
       "     prop_location_score2  prop_log_historical_price  price_usd  \\\n",
       "119              0.047600                       4.46       43.0   \n",
       "120              0.299300                       4.46       44.0   \n",
       "121              0.127639                       4.41       51.0   \n",
       "122              0.127639                       4.22       53.0   \n",
       "123              0.190900                       5.66      221.0   \n",
       "\n",
       "     promotion_flag  orig_destination_distance  \n",
       "119               1                    9270.55  \n",
       "120               0                    9285.88  \n",
       "121               0                    9288.15  \n",
       "122               0                    9278.60  \n",
       "123               0                    9286.02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Initial Feature Selection & Preparation ---\n",
    "X = None\n",
    "y = None\n",
    "groups_for_splitting = None # This will be df_sample['srch_id'] for GroupKFold.split\n",
    "feature_columns = [] # Initialize\n",
    "\n",
    "if df_sample is not None:\n",
    "    print(\"\\nDefining initial feature set and preparing X, y, groups...\")\n",
    "    feature_columns = [\n",
    "        'visitor_location_country_id', 'prop_country_id',\n",
    "        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "        'price_usd', 'promotion_flag', 'orig_destination_distance'\n",
    "        # Add more features after EDA and proper missing value handling from EDA notebook\n",
    "    ]\n",
    "\n",
    "    # Ensure all selected feature columns exist in df_sample\n",
    "    existing_feature_columns = [col for col in feature_columns if col in df_sample.columns]\n",
    "    if len(existing_feature_columns) != len(feature_columns):\n",
    "        print(f\"Warning: Some feature columns not found. Using: {existing_feature_columns}\")\n",
    "    feature_columns = existing_feature_columns\n",
    "\n",
    "    if not feature_columns:\n",
    "        print(\"Error: No feature columns selected or available. Stopping.\")\n",
    "    else:\n",
    "        X = df_sample[feature_columns].copy()\n",
    "        y = df_sample['relevance'].copy()\n",
    "        groups_for_splitting = df_sample['srch_id'] # Used by GroupKFold for splitting\n",
    "\n",
    "        # Basic Imputation (should ideally be done based on EDA insights and training set stats)\n",
    "        # This imputation is done on the *sampled* data (X).\n",
    "        # For test set imputation later, medians from this X will be used.\n",
    "        print(\"Performing basic median imputation for numerical features in X...\")\n",
    "        for col in X.columns:\n",
    "            if X[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    median_val = X[col].median()\n",
    "                    X[col].fillna(median_val, inplace=True)\n",
    "                    # print(f\"Imputed NaNs in '{col}' with median: {median_val}\")\n",
    "                # Add mode imputation for categorical if any, or use a placeholder string\n",
    "        \n",
    "        print(f\"Selected {len(feature_columns)} features: {feature_columns}\")\n",
    "        print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "        print(f\"NaNs remaining in X after imputation: {X.isnull().sum().sum()}\")\n",
    "        if groups_for_splitting is not None:\n",
    "            print(f\"Number of unique groups for splitting: {groups_for_splitting.nunique()}\")\n",
    "else:\n",
    "    print(\"Skipping feature selection as df_sample is None.\")\n",
    "\n",
    "# Display X's head to verify\n",
    "if X is not None:\n",
    "    display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5-Fold Cross-Validation using modular function ---\n",
      "\\n--- Performing 5-Fold Cross-Validation ---\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 NDCG@5: 0.3428\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 NDCG@5: 0.3423\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 NDCG@5: 0.3455\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 NDCG@5: 0.3530\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 NDCG@5: 0.3536\n",
      "Mean NDCG@5 across 5 folds: 0.3475 +/- 0.0049\n",
      "\n",
      "Cross-Validation Mean NDCG@5: 0.3475 +/- 0.0049\n",
      "\n",
      "Average Feature Importances from CV:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           15831.198095\n",
       "price_usd                      11461.798124\n",
       "prop_location_score1            7153.288665\n",
       "prop_starrating                 5405.245977\n",
       "prop_log_historical_price       5379.496554\n",
       "prop_review_score               2834.064519\n",
       "promotion_flag                  2247.297013\n",
       "orig_destination_distance       1730.336422\n",
       "prop_country_id                 1018.822620\n",
       "visitor_location_country_id      507.976540\n",
       "prop_brand_bool                  428.788319\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 5. Cross-Validation (using modular function) ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*')\n",
    "mean_cv_ndcg = 0\n",
    "std_cv_ndcg = 0\n",
    "cv_feature_importances = pd.Series(dtype=float) # Initialize as an empty Series\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    # Basic LGBM params for initial CV\n",
    "    # These will be merged with/override defaults in the perform_cross_validation function\n",
    "    initial_lgbm_params = {\n",
    "        'n_estimators': 100, # Example: function's default might be different\n",
    "        'learning_rate': 0.1, # Example\n",
    "        'random_state': RANDOM_STATE\n",
    "        # The modular function defines other necessary defaults like objective, metric, label_gain, eval_at etc.\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Performing {N_FOLDS_CV}-Fold Cross-Validation using modular function ---\")\n",
    "    mean_cv_ndcg, std_cv_ndcg, cv_feature_importances = lgbm_model.perform_cross_validation(\n",
    "        X, y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # Pass df_sample, as it contains 'srch_id' needed for group counts\n",
    "        n_folds=N_FOLDS_CV,\n",
    "        lgbm_params=initial_lgbm_params\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCross-Validation Mean NDCG@5: {mean_cv_ndcg:.4f} +/- {std_cv_ndcg:.4f}\")\n",
    "    if not cv_feature_importances.empty:\n",
    "        print(\"\\nAverage Feature Importances from CV:\")\n",
    "        with pd.option_context('display.max_rows', 30): # Display top 20 or all if less than 20\n",
    "            display(cv_feature_importances.head(20))\n",
    "else:\n",
    "    print(\"\\nSkipping Cross-Validation due to missing X, y, groups_for_splitting, or df_sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 15:11:15,776] A new study created in memory with name: lgbm_ranker_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) using modular function ---\n",
      "\\n--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 15:11:19,215] Trial 0 finished with value: 0.3483959691013827 and parameters: {'n_estimators': 350, 'learning_rate': 0.10148640916203272, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.5604481619303492, 'colsample_bytree': 0.7128952769140711, 'reg_alpha': 5.632050975812048, 'reg_lambda': 0.01393055140628837}. Best is trial 0 with value: 0.3483959691013827.\n",
      "[I 2025-05-14 15:11:22,750] Trial 1 finished with value: 0.3487982556281389 and parameters: {'n_estimators': 700, 'learning_rate': 0.07738743799224009, 'num_leaves': 50, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.6195409659098858, 'colsample_bytree': 0.9036249138323806, 'reg_alpha': 7.446041659683735, 'reg_lambda': 6.2495614061886355}. Best is trial 1 with value: 0.3487982556281389.\n",
      "[I 2025-05-14 15:11:24,863] Trial 2 finished with value: 0.35174829820481407 and parameters: {'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:27,550] Trial 3 finished with value: 0.3444268791354786 and parameters: {'n_estimators': 700, 'learning_rate': 0.048487561299753845, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 61, 'subsample': 0.5188748007776129, 'colsample_bytree': 0.8970206485636432, 'reg_alpha': 2.3691588622455257, 'reg_lambda': 1.2654777765757108}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:30,274] Trial 4 finished with value: 0.3354143619650662 and parameters: {'n_estimators': 600, 'learning_rate': 0.1519837982748114, 'num_leaves': 103, 'max_depth': 12, 'min_child_samples': 73, 'subsample': 0.6551003922424845, 'colsample_bytree': 0.5916291042615215, 'reg_alpha': 0.6750851601772996, 'reg_lambda': 0.018937668512042222}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:32,655] Trial 5 finished with value: 0.3458824661214159 and parameters: {'n_estimators': 150, 'learning_rate': 0.04752011450715754, 'num_leaves': 33, 'max_depth': 8, 'min_child_samples': 81, 'subsample': 0.6114376813883363, 'colsample_bytree': 0.8639411728449384, 'reg_alpha': 1.1358835087291093, 'reg_lambda': 0.0011821749270259797}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:35,443] Trial 6 finished with value: 0.335001575572149 and parameters: {'n_estimators': 100, 'learning_rate': 0.13049478057898115, 'num_leaves': 121, 'max_depth': 8, 'min_child_samples': 33, 'subsample': 0.7017967493102486, 'colsample_bytree': 0.7708521939849334, 'reg_alpha': 0.001018306399097641, 'reg_lambda': 0.13489391247397012}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:38,100] Trial 7 finished with value: 0.34362440215036366 and parameters: {'n_estimators': 200, 'learning_rate': 0.03268816446547107, 'num_leaves': 79, 'max_depth': 6, 'min_child_samples': 78, 'subsample': 0.9946455105737699, 'colsample_bytree': 0.8106489362165528, 'reg_alpha': 0.0010833647706765762, 'reg_lambda': 0.006844506750138019}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:39,493] Trial 8 finished with value: 0.33508241592841775 and parameters: {'n_estimators': 450, 'learning_rate': 0.01811125891212415, 'num_leaves': 85, 'max_depth': 3, 'min_child_samples': 9, 'subsample': 0.8523469965366194, 'colsample_bytree': 0.7370293883755854, 'reg_alpha': 1.5117893548494583, 'reg_lambda': 0.007620996501664811}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:41,359] Trial 9 finished with value: 0.3466650358242032 and parameters: {'n_estimators': 550, 'learning_rate': 0.09049688789687663, 'num_leaves': 130, 'max_depth': 3, 'min_child_samples': 19, 'subsample': 0.9439161838484604, 'colsample_bytree': 0.5316873806840319, 'reg_alpha': 2.1981254082152226, 'reg_lambda': 3.086369792229482}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:44,525] Trial 10 finished with value: 0.33905585153938095 and parameters: {'n_estimators': 300, 'learning_rate': 0.010404350542921773, 'num_leaves': 59, 'max_depth': 5, 'min_child_samples': 99, 'subsample': 0.8334839813499626, 'colsample_bytree': 0.995303594578808, 'reg_alpha': 0.022451539482305733, 'reg_lambda': 0.3234171141198142}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:47,319] Trial 11 finished with value: 0.34415990248628003 and parameters: {'n_estimators': 450, 'learning_rate': 0.18647449756091486, 'num_leaves': 54, 'max_depth': 9, 'min_child_samples': 41, 'subsample': 0.7978184305269341, 'colsample_bytree': 0.6586238368426612, 'reg_alpha': 0.045909965650061293, 'reg_lambda': 8.184046048299765}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:50,589] Trial 12 finished with value: 0.3438220985734841 and parameters: {'n_estimators': 550, 'learning_rate': 0.06384336264081673, 'num_leaves': 57, 'max_depth': 6, 'min_child_samples': 48, 'subsample': 0.7518016518295703, 'colsample_bytree': 0.9684698047952724, 'reg_alpha': 0.008914755539095355, 'reg_lambda': 0.6254579384253999}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:53,088] Trial 13 finished with value: 0.3419578691728356 and parameters: {'n_estimators': 700, 'learning_rate': 0.08076941553129545, 'num_leaves': 47, 'max_depth': 10, 'min_child_samples': 28, 'subsample': 0.8868954181653865, 'colsample_bytree': 0.6506124957391568, 'reg_alpha': 0.18012989958463674, 'reg_lambda': 0.07018880903867619}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:56,434] Trial 14 finished with value: 0.34647328602833927 and parameters: {'n_estimators': 250, 'learning_rate': 0.028964002311892487, 'num_leaves': 69, 'max_depth': 5, 'min_child_samples': 58, 'subsample': 0.6769958887341323, 'colsample_bytree': 0.8846020681752738, 'reg_alpha': 0.17116049415121967, 'reg_lambda': 4.921870260187403}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:58,904] Trial 15 finished with value: 0.34660598408048143 and parameters: {'n_estimators': 400, 'learning_rate': 0.11633518958804226, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 43, 'subsample': 0.6027155439348006, 'colsample_bytree': 0.8079119133765934, 'reg_alpha': 0.003919327580051043, 'reg_lambda': 1.5941627170463581}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:03,065] Trial 16 finished with value: 0.3406229403882976 and parameters: {'n_estimators': 600, 'learning_rate': 0.17623824371839195, 'num_leaves': 146, 'max_depth': 7, 'min_child_samples': 24, 'subsample': 0.763483966050713, 'colsample_bytree': 0.9383930439396932, 'reg_alpha': 9.16963999098815, 'reg_lambda': 0.06892047179344488}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:05,511] Trial 17 finished with value: 0.3483133994900416 and parameters: {'n_estimators': 500, 'learning_rate': 0.06701697257749081, 'num_leaves': 22, 'max_depth': 5, 'min_child_samples': 65, 'subsample': 0.9426753464983313, 'colsample_bytree': 0.6788679480647025, 'reg_alpha': 0.38748523103320637, 'reg_lambda': 0.3019796937274395}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:08,375] Trial 18 finished with value: 0.34100238193015925 and parameters: {'n_estimators': 350, 'learning_rate': 0.03161122760104547, 'num_leaves': 102, 'max_depth': 4, 'min_child_samples': 36, 'subsample': 0.5141088206231738, 'colsample_bytree': 0.7921666231144328, 'reg_alpha': 0.05053661340176915, 'reg_lambda': 0.0021491028926194818}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:11,796] Trial 19 finished with value: 0.3403809351977362 and parameters: {'n_estimators': 300, 'learning_rate': 0.19945578181896698, 'num_leaves': 68, 'max_depth': 7, 'min_child_samples': 51, 'subsample': 0.7134205521477647, 'colsample_bytree': 0.8418259219149622, 'reg_alpha': 0.004277146050865765, 'reg_lambda': 1.9070453292383973}. Best is trial 2 with value: 0.35174829820481407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study finished. Best trial NDCG@5: 0.3517\n",
      "Best parameters: {'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}\n",
      "\n",
      "Best parameters found by Optuna:\n",
      "    n_estimators: 400\n",
      "    learning_rate: 0.1734440198649952\n",
      "    num_leaves: 36\n",
      "    max_depth: 4\n",
      "    min_child_samples: 58\n",
      "    subsample: 0.994009316145284\n",
      "    colsample_bytree: 0.7229225094690845\n",
      "    reg_alpha: 0.002544358343653164\n",
      "    reg_lambda: 0.1930789012781852\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Hyperparameter Tuning with Optuna (using modular function) ---\n",
    "best_params_from_tuning = {} # Initialize\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    print(f\"\\n--- Tuning Hyperparameters with Optuna ({N_OPTUNA_TRIALS} trials, {N_FOLDS_TUNING} CV folds each) using modular function ---\")\n",
    "    \n",
    "    # Suppress Optuna's verbosity if it's too much, and LightGBM warnings during tuning.\n",
    "    # import optuna # Optuna is imported within lgbm_model.py where tune_hyperparameters_optuna is defined.\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARNING) # You can set this in lgbm_model.py if desired globally for the function\n",
    "    \n",
    "    # It's good practice to manage warnings that might clutter the output during tuning.\n",
    "    # The lgbm_model.py file could also handle these internally if preferred.\n",
    "    warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*') # Suppress LightGBM's specific warning\n",
    "    warnings.filterwarnings('ignore', message='Overriding the init_model argument.*') # Another potential LightGBM warning\n",
    "\n",
    "    best_params_from_tuning = lgbm_model.tune_hyperparameters_optuna(\n",
    "        X, \n",
    "        y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # df_sample for calculating group sizes within folds\n",
    "        n_trials=N_OPTUNA_TRIALS, \n",
    "        n_cv_folds=N_FOLDS_TUNING\n",
    "    )\n",
    "    \n",
    "    if best_params_from_tuning:\n",
    "        print(\"\\nBest parameters found by Optuna:\")\n",
    "        for key, value in best_params_from_tuning.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\nOptuna tuning did not return parameters. Using default parameters for the final model evaluation.\")\n",
    "        # Fallback to some sensible defaults if tuning fails or is skipped\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "            'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "            'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "            # Add other necessary LGBM parameters if not covered by the module's defaults\n",
    "        } \n",
    "else:\n",
    "    print(\"\\nSkipping Hyperparameter Tuning due to missing X, y, groups_for_splitting, or df_sample.\")\n",
    "    # Fallback parameters if tuning is skipped\n",
    "    best_params_from_tuning = { \n",
    "        'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "        'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "        'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "    }\n",
    "\n",
    "# Reset warnings to default behavior if they were changed\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Training Final Model with Best Tuned Parameters ---\n",
      "\\n--- Training Final Model ---\n",
      "Final model parameters for training:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1, 'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model with early stopping on 90/10 split of training data.\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's ndcg@5: 0.363617\n",
      "Final model training completed.\n",
      "Final model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Train Final Model on Full Sampled Data (using modular function) ---\n",
    "final_trained_model = None\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None and best_params_from_tuning:\n",
    "    print(\"\\\\n--- Training Final Model with Best Tuned Parameters ---\")\n",
    "    \n",
    "    # groups_train_full is needed for the lgbm_model.train_final_model function\n",
    "    # It should represent the group sizes for the entire X, y that's being passed\n",
    "    # This X is df_sample[feature_columns]\n",
    "    groups_train_full = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "    if len(groups_train_full) > 0:\n",
    "        final_trained_model = lgbm_model.train_final_model(\n",
    "            X_train_full=X,  # This is the full X from df_sample\n",
    "            y_train_full=y,  # This is the full y from df_sample\n",
    "            groups_train_full=groups_train_full,\n",
    "            df_full_for_group_counts=df_sample, # df_sample contains 'srch_id' for early stopping split\n",
    "            best_params=best_params_from_tuning\n",
    "        )\n",
    "        if final_trained_model:\n",
    "            print(\"Final model successfully trained.\")\n",
    "        else:\n",
    "            print(\"Final model training failed or returned None.\")\n",
    "    else:\n",
    "        print(\"Cannot train final model: No groups found in the training data.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping final model training due to missing data, groups, or best_params_from_tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Preparing Test Data and Generating Kaggle Submission ---\n",
      "Loading test data from: ../data.nosync/test.csv...\n",
      "Loaded test dataset with shape: (4959183, 50)\n",
      "\\nPreprocessing test data...\n",
      "Imputing missing values in test data using training set medians...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/ww14kyzx7zq1bdp4kybt0_800000gn/T/ipykernel_87350/802724624.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(train_median, inplace=True)\n",
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs remaining in X_test after imputation: 0\n",
      "\\n--- Predicting on Test Data and Formatting Submission ---\n",
      "Submission file 'submission_modular.csv' created. Top 5 rows:\n",
      "   SearchId  PropertyId\n",
      "0         1       54937\n",
      "1         1       99484\n",
      "2         1       61934\n",
      "3         1       24194\n",
      "4         1       28181\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission (using modular function) ---\n",
    "\n",
    "if final_trained_model is not None and X is not None and 'feature_columns' in locals() and feature_columns:\n",
    "    print(\"\\\\n--- Preparing Test Data and Generating Kaggle Submission ---\")\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    print(f\"Loading test data from: {TEST_FILE}...\")\n",
    "    try:\n",
    "        df_test_raw = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test_raw.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Test file not found at {TEST_FILE}\")\n",
    "        df_test_raw = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        df_test_raw = None\n",
    "\n",
    "    if df_test_raw is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\\\nPreprocessing test data...\")\n",
    "        \n",
    "        # Ensure all selected feature columns exist in df_test_raw\n",
    "        # and handle any missing columns gracefully if necessary (e.g. by creating them with NaNs)\n",
    "        X_test_list = []\n",
    "        for col in feature_columns:\n",
    "            if col not in df_test_raw.columns:\n",
    "                print(f\"Warning: Feature column '{col}' not found in test data. Creating it with NaNs.\")\n",
    "                df_test_raw[col] = np.nan \n",
    "        \n",
    "        X_test = df_test_raw[feature_columns].copy()\n",
    "\n",
    "        # Impute missing values in X_test using medians from the TRAINING sample (X)\n",
    "        # X should be the dataframe of features used for training the final_trained_model\n",
    "        print(\"Imputing missing values in test data using training set medians...\")\n",
    "        nan_counts_before_imputation = X_test.isnull().sum()\n",
    "\n",
    "        for col in X_test.columns:\n",
    "            if X_test[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                    if col in X.columns: # Ensure the column exists in the training features X\n",
    "                        train_median = X[col].median() # Calculate median from the TRAIN features (X)\n",
    "                        X_test[col].fillna(train_median, inplace=True)\n",
    "                        # print(f\"Imputed NaNs in test column '{col}' with training median: {train_median}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Column '{col}' for median imputation not found in training X. Test NaNs may remain.\")\n",
    "                # else: # For categorical, use mode from training X\n",
    "                    # if col in X.columns:\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col].fillna(train_mode, inplace=True)\n",
    "                    # else:\n",
    "                    #     print(f\"Warning: Column '{col}' for mode imputation not found in training X. Test NaNs may remain.\")\n",
    "        \n",
    "        nan_counts_after_imputation = X_test.isnull().sum().sum()\n",
    "        print(f\"NaNs remaining in X_test after imputation: {nan_counts_after_imputation}\")\n",
    "        if nan_counts_after_imputation > 0:\n",
    "            print(\"Warning: Some NaNs remain in test features after imputation. Review missing columns or imputation logic.\")\n",
    "            print(X_test.isnull().sum()[X_test.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "        # --- 8c. Generate Submission File ---\n",
    "        # The df_test_raw contains 'srch_id' and 'prop_id' needed by the submission function\n",
    "        lgbm_model.predict_and_format_submission(\n",
    "            model=final_trained_model,\n",
    "            X_test=X_test,\n",
    "            df_test_original_ids=df_test_raw, # Pass the raw test df for srch_id and prop_id\n",
    "            submission_filename=SUBMISSION_FILENAME\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping submission generation as test data could not be loaded.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping Kaggle submission: final_trained_model, X, or feature_columns not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
