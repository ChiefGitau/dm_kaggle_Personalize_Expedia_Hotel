{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 10.0% of the data based on srch_id...\n",
      "Sampled data shape: (496228, 55)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Experiment Notebook for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb # Keep for type hints if needed, but direct use will be less\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# Import the modularized model functions\n",
    "import lightgbm_ranker_model as lgbm_model\n",
    "import warnings # For managing warnings from Optuna/LightGBM if needed\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATA_DIR = '../data.nosync'\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'train_imputed.csv')\n",
    "TEST_FILE = os.path.join(DATA_DIR, 'test.csv') # Defined TEST_FILE path\n",
    "SUBMISSION_FILENAME = 'submission_modular.csv' # Defined submission filename\n",
    "\n",
    "SAMPLE_FRACTION = 0.1 # Use 10% of the data for faster runs during development\n",
    "N_FOLDS_CV = 5         # Number of folds for general cross-validation\n",
    "N_FOLDS_TUNING = 3     # Number of folds for Optuna trials (can be smaller for speed)\n",
    "N_OPTUNA_TRIALS = 20   # Number of Optuna trials\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training file not found at {TRAIN_FILE}\")\n",
    "    df_train_full = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    df_train_full = None\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2 # Map to 0, 1, 2 for label_gain [0,1,5]\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation as df_train_full is None.\")\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "df_sample = None\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    if len(unique_srch_ids) > 0:\n",
    "        sampled_srch_ids_count = int(len(unique_srch_ids) * SAMPLE_FRACTION)\n",
    "        if sampled_srch_ids_count > 0:\n",
    "            sampled_srch_ids = np.random.choice(unique_srch_ids, size=sampled_srch_ids_count, replace=False)\n",
    "            df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "            print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "        else:\n",
    "            print(\"Sample fraction resulted in zero search IDs. Check SAMPLE_FRACTION or dataset size.\")\n",
    "            df_sample = df_train_full.copy() # Fallback to full if sample is too small\n",
    "            print(f\"Using full dataset instead. Shape: {df_sample.shape}\")\n",
    "    else:\n",
    "        print(\"No unique search IDs found in the training data.\")\n",
    "    del df_train_full # Optional: free up memory\n",
    "else:\n",
    "    print(\"Skipping sampling as df_train_full is None.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 10.0% of the data based on srch_id...\n",
      "Sampled data shape: (496212, 55)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main Experiment Notebook for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATA_DIR = '../data.nosync'\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'train_imputed.csv')\n",
    "\n",
    "SAMPLE_FRACTION = 0.1 # Use 4% of the data\n",
    "N_FOLDS = 5 # For GroupKFold cross-validation\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    # 5 for booking, 1 for click (and not booked), 0 otherwise\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation due to data loading issues.\")\n",
    "\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    sampled_srch_ids = np.random.choice(unique_srch_ids, size=int(len(unique_srch_ids) * SAMPLE_FRACTION), replace=False)\n",
    "\n",
    "    df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "    print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "    # Free up memory from the full dataframe if no longer needed for this notebook scope\n",
    "    # del df_train_full \n",
    "else:\n",
    "    print(\"Skipping sampling due to data loading issues.\")\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>7814</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>10881</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>12510</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>17122</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>12</td>\n",
       "      <td>2013-03-15 14:55:16</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>158</td>\n",
       "      <td>18012</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958140</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>30933</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958141</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>46986</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>156.75</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958142</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>62314</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958143</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>91660</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958144</th>\n",
       "      <td>332763</td>\n",
       "      <td>2012-12-04 18:55:04</td>\n",
       "      <td>32</td>\n",
       "      <td>220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "      <td>92352</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496228 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "119           12  2013-03-15 14:55:16        5                          219   \n",
       "120           12  2013-03-15 14:55:16        5                          219   \n",
       "121           12  2013-03-15 14:55:16        5                          219   \n",
       "122           12  2013-03-15 14:55:16        5                          219   \n",
       "123           12  2013-03-15 14:55:16        5                          219   \n",
       "...          ...                  ...      ...                          ...   \n",
       "4958140   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958141   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958142   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958143   332763  2012-12-04 18:55:04       32                          220   \n",
       "4958144   332763  2012-12-04 18:55:04       32                          220   \n",
       "\n",
       "         visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  \\\n",
       "119                          NaN                   NaN              158   \n",
       "120                          NaN                   NaN              158   \n",
       "121                          NaN                   NaN              158   \n",
       "122                          NaN                   NaN              158   \n",
       "123                          NaN                   NaN              158   \n",
       "...                          ...                   ...              ...   \n",
       "4958140                      NaN                   NaN               81   \n",
       "4958141                      NaN                   NaN               81   \n",
       "4958142                      NaN                   NaN               81   \n",
       "4958143                      NaN                   NaN               81   \n",
       "4958144                      NaN                   NaN               81   \n",
       "\n",
       "         prop_id  prop_starrating  prop_review_score  ...  comp7_rate  \\\n",
       "119         7814                3                3.5  ...         NaN   \n",
       "120        10881                3                3.0  ...         NaN   \n",
       "121        12510                4                0.0  ...         NaN   \n",
       "122        17122                4                3.5  ...         NaN   \n",
       "123        18012                5                4.5  ...         NaN   \n",
       "...          ...              ...                ...  ...         ...   \n",
       "4958140    30933                3                2.5  ...         NaN   \n",
       "4958141    46986                3                3.0  ...         NaN   \n",
       "4958142    62314                4                3.0  ...         NaN   \n",
       "4958143    91660                3                0.0  ...         NaN   \n",
       "4958144    92352                4                4.0  ...         NaN   \n",
       "\n",
       "         comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "119            NaN                      NaN         NaN        NaN   \n",
       "120            NaN                      NaN         NaN        NaN   \n",
       "121            NaN                      NaN         NaN        NaN   \n",
       "122            NaN                      NaN         NaN        NaN   \n",
       "123            NaN                      NaN         NaN        NaN   \n",
       "...            ...                      ...         ...        ...   \n",
       "4958140        NaN                      NaN         NaN        NaN   \n",
       "4958141        NaN                      NaN         NaN        NaN   \n",
       "4958142        NaN                      NaN         NaN        NaN   \n",
       "4958143        NaN                      NaN         NaN        NaN   \n",
       "4958144        NaN                      NaN         NaN        NaN   \n",
       "\n",
       "         comp8_rate_percent_diff  click_bool  gross_bookings_usd  \\\n",
       "119                          NaN           0                 NaN   \n",
       "120                          NaN           0                 NaN   \n",
       "121                          NaN           0                 NaN   \n",
       "122                          NaN           0                 NaN   \n",
       "123                          NaN           0                 NaN   \n",
       "...                          ...         ...                 ...   \n",
       "4958140                      NaN           0                 NaN   \n",
       "4958141                      NaN           1              156.75   \n",
       "4958142                      NaN           0                 NaN   \n",
       "4958143                      NaN           0                 NaN   \n",
       "4958144                      NaN           0                 NaN   \n",
       "\n",
       "         booking_bool  relevance  \n",
       "119                 0          0  \n",
       "120                 0          0  \n",
       "121                 0          0  \n",
       "122                 0          0  \n",
       "123                 0          0  \n",
       "...               ...        ...  \n",
       "4958140             0          0  \n",
       "4958141             1          2  \n",
       "4958142             0          0  \n",
       "4958143             0          0  \n",
       "4958144             0          0  \n",
       "\n",
       "[496228 rows x 55 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set and preparing X, y, groups...\n",
      "Performing basic median imputation for numerical features in X...\n",
      "Selected 11 features: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'orig_destination_distance']\n",
      "Shape of X: (496228, 11), Shape of y: (496228,)\n",
      "NaNs remaining in X after imputation: 0\n",
      "Number of unique groups for splitting: 19979\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>prop_log_historical_price</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>promotion_flag</th>\n",
       "      <th>orig_destination_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.047600</td>\n",
       "      <td>4.46</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9270.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.14</td>\n",
       "      <td>0.299300</td>\n",
       "      <td>4.46</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9285.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>4.41</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9288.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.127639</td>\n",
       "      <td>4.22</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9278.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>219</td>\n",
       "      <td>158</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>0</td>\n",
       "      <td>3.22</td>\n",
       "      <td>0.190900</td>\n",
       "      <td>5.66</td>\n",
       "      <td>221.0</td>\n",
       "      <td>0</td>\n",
       "      <td>9286.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     visitor_location_country_id  prop_country_id  prop_starrating  \\\n",
       "119                          219              158                3   \n",
       "120                          219              158                3   \n",
       "121                          219              158                4   \n",
       "122                          219              158                4   \n",
       "123                          219              158                5   \n",
       "\n",
       "     prop_review_score  prop_brand_bool  prop_location_score1  \\\n",
       "119                3.5                0                  0.00   \n",
       "120                3.0                0                  3.14   \n",
       "121                0.0                0                  1.10   \n",
       "122                3.5                0                  2.64   \n",
       "123                4.5                0                  3.22   \n",
       "\n",
       "     prop_location_score2  prop_log_historical_price  price_usd  \\\n",
       "119              0.047600                       4.46       43.0   \n",
       "120              0.299300                       4.46       44.0   \n",
       "121              0.127639                       4.41       51.0   \n",
       "122              0.127639                       4.22       53.0   \n",
       "123              0.190900                       5.66      221.0   \n",
       "\n",
       "     promotion_flag  orig_destination_distance  \n",
       "119               1                    9270.55  \n",
       "120               0                    9285.88  \n",
       "121               0                    9288.15  \n",
       "122               0                    9278.60  \n",
       "123               0                    9286.02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 4. Initial Feature Selection & Preparation ---\n",
    "X = None\n",
    "y = None\n",
    "groups_for_splitting = None # This will be df_sample['srch_id'] for GroupKFold.split\n",
    "feature_columns = [] # Initialize\n",
    "\n",
    "if df_sample is not None:\n",
    "    print(\"\\nDefining initial feature set and preparing X, y, groups...\")\n",
    "    feature_columns = [\n",
    "        'visitor_location_country_id', 'prop_country_id',\n",
    "        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "        'price_usd', 'promotion_flag', 'orig_destination_distance'\n",
    "        # Add more features after EDA and proper missing value handling from EDA notebook\n",
    "    ]\n",
    "\n",
    "    # Ensure all selected feature columns exist in df_sample\n",
    "    existing_feature_columns = [col for col in feature_columns if col in df_sample.columns]\n",
    "    if len(existing_feature_columns) != len(feature_columns):\n",
    "        print(f\"Warning: Some feature columns not found. Using: {existing_feature_columns}\")\n",
    "    feature_columns = existing_feature_columns\n",
    "\n",
    "    if not feature_columns:\n",
    "        print(\"Error: No feature columns selected or available. Stopping.\")\n",
    "    else:\n",
    "        X = df_sample[feature_columns].copy()\n",
    "        y = df_sample['relevance'].copy()\n",
    "        groups_for_splitting = df_sample['srch_id'] # Used by GroupKFold for splitting\n",
    "\n",
    "        # Basic Imputation (should ideally be done based on EDA insights and training set stats)\n",
    "        # This imputation is done on the *sampled* data (X).\n",
    "        # For test set imputation later, medians from this X will be used.\n",
    "        print(\"Performing basic median imputation for numerical features in X...\")\n",
    "        for col in X.columns:\n",
    "            if X[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                    median_val = X[col].median()\n",
    "                    X[col].fillna(median_val, inplace=True)\n",
    "                    # print(f\"Imputed NaNs in '{col}' with median: {median_val}\")\n",
    "                # Add mode imputation for categorical if any, or use a placeholder string\n",
    "        \n",
    "        print(f\"Selected {len(feature_columns)} features: {feature_columns}\")\n",
    "        print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "        print(f\"NaNs remaining in X after imputation: {X.isnull().sum().sum()}\")\n",
    "        if groups_for_splitting is not None:\n",
    "            print(f\"Number of unique groups for splitting: {groups_for_splitting.nunique()}\")\n",
    "else:\n",
    "    print(\"Skipping feature selection as df_sample is None.\")\n",
    "\n",
    "# Display X's head to verify\n",
    "if X is not None:\n",
    "    display(X.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5-Fold Cross-Validation using modular function ---\n",
      "\\n--- Performing 5-Fold Cross-Validation ---\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 NDCG@5: 0.3428\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 NDCG@5: 0.3423\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 NDCG@5: 0.3455\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 NDCG@5: 0.3530\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 NDCG@5: 0.3536\n",
      "Mean NDCG@5 across 5 folds: 0.3475 +/- 0.0049\n",
      "\n",
      "Cross-Validation Mean NDCG@5: 0.3475 +/- 0.0049\n",
      "\n",
      "Average Feature Importances from CV:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           15831.198095\n",
       "price_usd                      11461.798124\n",
       "prop_location_score1            7153.288665\n",
       "prop_starrating                 5405.245977\n",
       "prop_log_historical_price       5379.496554\n",
       "prop_review_score               2834.064519\n",
       "promotion_flag                  2247.297013\n",
       "orig_destination_distance       1730.336422\n",
       "prop_country_id                 1018.822620\n",
       "visitor_location_country_id      507.976540\n",
       "prop_brand_bool                  428.788319\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 5. Cross-Validation (using modular function) ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*')\n",
    "mean_cv_ndcg = 0\n",
    "std_cv_ndcg = 0\n",
    "cv_feature_importances = pd.Series(dtype=float) # Initialize as an empty Series\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    # Basic LGBM params for initial CV\n",
    "    # These will be merged with/override defaults in the perform_cross_validation function\n",
    "    initial_lgbm_params = {\n",
    "        'n_estimators': 100, # Example: function's default might be different\n",
    "        'learning_rate': 0.1, # Example\n",
    "        'random_state': RANDOM_STATE\n",
    "        # The modular function defines other necessary defaults like objective, metric, label_gain, eval_at etc.\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Performing {N_FOLDS_CV}-Fold Cross-Validation using modular function ---\")\n",
    "    mean_cv_ndcg, std_cv_ndcg, cv_feature_importances = lgbm_model.perform_cross_validation(\n",
    "        X, y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # Pass df_sample, as it contains 'srch_id' needed for group counts\n",
    "        n_folds=N_FOLDS_CV,\n",
    "        lgbm_params=initial_lgbm_params\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCross-Validation Mean NDCG@5: {mean_cv_ndcg:.4f} +/- {std_cv_ndcg:.4f}\")\n",
    "    if not cv_feature_importances.empty:\n",
    "        print(\"\\nAverage Feature Importances from CV:\")\n",
    "        with pd.option_context('display.max_rows', 30): # Display top 20 or all if less than 20\n",
    "            display(cv_feature_importances.head(20))\n",
    "else:\n",
    "    print(\"\\nSkipping Cross-Validation due to missing X, y, groups_for_splitting, or df_sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 15:11:15,776] A new study created in memory with name: lgbm_ranker_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) using modular function ---\n",
      "\\n--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 15:11:19,215] Trial 0 finished with value: 0.3483959691013827 and parameters: {'n_estimators': 350, 'learning_rate': 0.10148640916203272, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 6, 'subsample': 0.5604481619303492, 'colsample_bytree': 0.7128952769140711, 'reg_alpha': 5.632050975812048, 'reg_lambda': 0.01393055140628837}. Best is trial 0 with value: 0.3483959691013827.\n",
      "[I 2025-05-14 15:11:22,750] Trial 1 finished with value: 0.3487982556281389 and parameters: {'n_estimators': 700, 'learning_rate': 0.07738743799224009, 'num_leaves': 50, 'max_depth': 8, 'min_child_samples': 30, 'subsample': 0.6195409659098858, 'colsample_bytree': 0.9036249138323806, 'reg_alpha': 7.446041659683735, 'reg_lambda': 6.2495614061886355}. Best is trial 1 with value: 0.3487982556281389.\n",
      "[I 2025-05-14 15:11:24,863] Trial 2 finished with value: 0.35174829820481407 and parameters: {'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:27,550] Trial 3 finished with value: 0.3444268791354786 and parameters: {'n_estimators': 700, 'learning_rate': 0.048487561299753845, 'num_leaves': 31, 'max_depth': 12, 'min_child_samples': 61, 'subsample': 0.5188748007776129, 'colsample_bytree': 0.8970206485636432, 'reg_alpha': 2.3691588622455257, 'reg_lambda': 1.2654777765757108}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:30,274] Trial 4 finished with value: 0.3354143619650662 and parameters: {'n_estimators': 600, 'learning_rate': 0.1519837982748114, 'num_leaves': 103, 'max_depth': 12, 'min_child_samples': 73, 'subsample': 0.6551003922424845, 'colsample_bytree': 0.5916291042615215, 'reg_alpha': 0.6750851601772996, 'reg_lambda': 0.018937668512042222}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:32,655] Trial 5 finished with value: 0.3458824661214159 and parameters: {'n_estimators': 150, 'learning_rate': 0.04752011450715754, 'num_leaves': 33, 'max_depth': 8, 'min_child_samples': 81, 'subsample': 0.6114376813883363, 'colsample_bytree': 0.8639411728449384, 'reg_alpha': 1.1358835087291093, 'reg_lambda': 0.0011821749270259797}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:35,443] Trial 6 finished with value: 0.335001575572149 and parameters: {'n_estimators': 100, 'learning_rate': 0.13049478057898115, 'num_leaves': 121, 'max_depth': 8, 'min_child_samples': 33, 'subsample': 0.7017967493102486, 'colsample_bytree': 0.7708521939849334, 'reg_alpha': 0.001018306399097641, 'reg_lambda': 0.13489391247397012}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:38,100] Trial 7 finished with value: 0.34362440215036366 and parameters: {'n_estimators': 200, 'learning_rate': 0.03268816446547107, 'num_leaves': 79, 'max_depth': 6, 'min_child_samples': 78, 'subsample': 0.9946455105737699, 'colsample_bytree': 0.8106489362165528, 'reg_alpha': 0.0010833647706765762, 'reg_lambda': 0.006844506750138019}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:39,493] Trial 8 finished with value: 0.33508241592841775 and parameters: {'n_estimators': 450, 'learning_rate': 0.01811125891212415, 'num_leaves': 85, 'max_depth': 3, 'min_child_samples': 9, 'subsample': 0.8523469965366194, 'colsample_bytree': 0.7370293883755854, 'reg_alpha': 1.5117893548494583, 'reg_lambda': 0.007620996501664811}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:41,359] Trial 9 finished with value: 0.3466650358242032 and parameters: {'n_estimators': 550, 'learning_rate': 0.09049688789687663, 'num_leaves': 130, 'max_depth': 3, 'min_child_samples': 19, 'subsample': 0.9439161838484604, 'colsample_bytree': 0.5316873806840319, 'reg_alpha': 2.1981254082152226, 'reg_lambda': 3.086369792229482}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:44,525] Trial 10 finished with value: 0.33905585153938095 and parameters: {'n_estimators': 300, 'learning_rate': 0.010404350542921773, 'num_leaves': 59, 'max_depth': 5, 'min_child_samples': 99, 'subsample': 0.8334839813499626, 'colsample_bytree': 0.995303594578808, 'reg_alpha': 0.022451539482305733, 'reg_lambda': 0.3234171141198142}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:47,319] Trial 11 finished with value: 0.34415990248628003 and parameters: {'n_estimators': 450, 'learning_rate': 0.18647449756091486, 'num_leaves': 54, 'max_depth': 9, 'min_child_samples': 41, 'subsample': 0.7978184305269341, 'colsample_bytree': 0.6586238368426612, 'reg_alpha': 0.045909965650061293, 'reg_lambda': 8.184046048299765}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:50,589] Trial 12 finished with value: 0.3438220985734841 and parameters: {'n_estimators': 550, 'learning_rate': 0.06384336264081673, 'num_leaves': 57, 'max_depth': 6, 'min_child_samples': 48, 'subsample': 0.7518016518295703, 'colsample_bytree': 0.9684698047952724, 'reg_alpha': 0.008914755539095355, 'reg_lambda': 0.6254579384253999}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:53,088] Trial 13 finished with value: 0.3419578691728356 and parameters: {'n_estimators': 700, 'learning_rate': 0.08076941553129545, 'num_leaves': 47, 'max_depth': 10, 'min_child_samples': 28, 'subsample': 0.8868954181653865, 'colsample_bytree': 0.6506124957391568, 'reg_alpha': 0.18012989958463674, 'reg_lambda': 0.07018880903867619}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:56,434] Trial 14 finished with value: 0.34647328602833927 and parameters: {'n_estimators': 250, 'learning_rate': 0.028964002311892487, 'num_leaves': 69, 'max_depth': 5, 'min_child_samples': 58, 'subsample': 0.6769958887341323, 'colsample_bytree': 0.8846020681752738, 'reg_alpha': 0.17116049415121967, 'reg_lambda': 4.921870260187403}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:11:58,904] Trial 15 finished with value: 0.34660598408048143 and parameters: {'n_estimators': 400, 'learning_rate': 0.11633518958804226, 'num_leaves': 39, 'max_depth': 10, 'min_child_samples': 43, 'subsample': 0.6027155439348006, 'colsample_bytree': 0.8079119133765934, 'reg_alpha': 0.003919327580051043, 'reg_lambda': 1.5941627170463581}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:03,065] Trial 16 finished with value: 0.3406229403882976 and parameters: {'n_estimators': 600, 'learning_rate': 0.17623824371839195, 'num_leaves': 146, 'max_depth': 7, 'min_child_samples': 24, 'subsample': 0.763483966050713, 'colsample_bytree': 0.9383930439396932, 'reg_alpha': 9.16963999098815, 'reg_lambda': 0.06892047179344488}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:05,511] Trial 17 finished with value: 0.3483133994900416 and parameters: {'n_estimators': 500, 'learning_rate': 0.06701697257749081, 'num_leaves': 22, 'max_depth': 5, 'min_child_samples': 65, 'subsample': 0.9426753464983313, 'colsample_bytree': 0.6788679480647025, 'reg_alpha': 0.38748523103320637, 'reg_lambda': 0.3019796937274395}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:08,375] Trial 18 finished with value: 0.34100238193015925 and parameters: {'n_estimators': 350, 'learning_rate': 0.03161122760104547, 'num_leaves': 102, 'max_depth': 4, 'min_child_samples': 36, 'subsample': 0.5141088206231738, 'colsample_bytree': 0.7921666231144328, 'reg_alpha': 0.05053661340176915, 'reg_lambda': 0.0021491028926194818}. Best is trial 2 with value: 0.35174829820481407.\n",
      "[I 2025-05-14 15:12:11,796] Trial 19 finished with value: 0.3403809351977362 and parameters: {'n_estimators': 300, 'learning_rate': 0.19945578181896698, 'num_leaves': 68, 'max_depth': 7, 'min_child_samples': 51, 'subsample': 0.7134205521477647, 'colsample_bytree': 0.8418259219149622, 'reg_alpha': 0.004277146050865765, 'reg_lambda': 1.9070453292383973}. Best is trial 2 with value: 0.35174829820481407.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study finished. Best trial NDCG@5: 0.3517\n",
      "Best parameters: {'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}\n",
      "\n",
      "Best parameters found by Optuna:\n",
      "    n_estimators: 400\n",
      "    learning_rate: 0.1734440198649952\n",
      "    num_leaves: 36\n",
      "    max_depth: 4\n",
      "    min_child_samples: 58\n",
      "    subsample: 0.994009316145284\n",
      "    colsample_bytree: 0.7229225094690845\n",
      "    reg_alpha: 0.002544358343653164\n",
      "    reg_lambda: 0.1930789012781852\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Hyperparameter Tuning with Optuna (using modular function) ---\n",
    "best_params_from_tuning = {} # Initialize\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    print(f\"\\n--- Tuning Hyperparameters with Optuna ({N_OPTUNA_TRIALS} trials, {N_FOLDS_TUNING} CV folds each) using modular function ---\")\n",
    "    \n",
    "    # Suppress Optuna's verbosity if it's too much, and LightGBM warnings during tuning.\n",
    "    # import optuna # Optuna is imported within lgbm_model.py where tune_hyperparameters_optuna is defined.\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARNING) # You can set this in lgbm_model.py if desired globally for the function\n",
    "    \n",
    "    # It's good practice to manage warnings that might clutter the output during tuning.\n",
    "    # The lgbm_model.py file could also handle these internally if preferred.\n",
    "    warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*') # Suppress LightGBM's specific warning\n",
    "    warnings.filterwarnings('ignore', message='Overriding the init_model argument.*') # Another potential LightGBM warning\n",
    "\n",
    "    best_params_from_tuning = lgbm_model.tune_hyperparameters_optuna(\n",
    "        X, \n",
    "        y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # df_sample for calculating group sizes within folds\n",
    "        n_trials=N_OPTUNA_TRIALS, \n",
    "        n_cv_folds=N_FOLDS_TUNING\n",
    "    )\n",
    "    \n",
    "    if best_params_from_tuning:\n",
    "        print(\"\\nBest parameters found by Optuna:\")\n",
    "        for key, value in best_params_from_tuning.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\nOptuna tuning did not return parameters. Using default parameters for the final model evaluation.\")\n",
    "        # Fallback to some sensible defaults if tuning fails or is skipped\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "            'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "            'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "            # Add other necessary LGBM parameters if not covered by the module's defaults\n",
    "        } \n",
    "else:\n",
    "    print(\"\\nSkipping Hyperparameter Tuning due to missing X, y, groups_for_splitting, or df_sample.\")\n",
    "    # Fallback parameters if tuning is skipped\n",
    "    best_params_from_tuning = { \n",
    "        'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "        'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "        'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "    }\n",
    "\n",
    "# Reset warnings to default behavior if they were changed\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Training Final Model with Best Tuned Parameters ---\n",
      "\\n--- Training Final Model ---\n",
      "Final model parameters for training:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1, 'n_estimators': 400, 'learning_rate': 0.1734440198649952, 'num_leaves': 36, 'max_depth': 4, 'min_child_samples': 58, 'subsample': 0.994009316145284, 'colsample_bytree': 0.7229225094690845, 'reg_alpha': 0.002544358343653164, 'reg_lambda': 0.1930789012781852}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model with early stopping on 90/10 split of training data.\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[58]\tvalid_0's ndcg@5: 0.363617\n",
      "Final model training completed.\n",
      "Final model successfully trained.\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Train Final Model on Full Sampled Data (using modular function) ---\n",
    "final_trained_model = None\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None and best_params_from_tuning:\n",
    "    print(\"\\\\n--- Training Final Model with Best Tuned Parameters ---\")\n",
    "    \n",
    "    # groups_train_full is needed for the lgbm_model.train_final_model function\n",
    "    # It should represent the group sizes for the entire X, y that's being passed\n",
    "    # This X is df_sample[feature_columns]\n",
    "    groups_train_full = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "    if len(groups_train_full) > 0:\n",
    "        final_trained_model = lgbm_model.train_final_model(\n",
    "            X_train_full=X,  # This is the full X from df_sample\n",
    "            y_train_full=y,  # This is the full y from df_sample\n",
    "            groups_train_full=groups_train_full,\n",
    "            df_full_for_group_counts=df_sample, # df_sample contains 'srch_id' for early stopping split\n",
    "            best_params=best_params_from_tuning\n",
    "        )\n",
    "        if final_trained_model:\n",
    "            print(\"Final model successfully trained.\")\n",
    "        else:\n",
    "            print(\"Final model training failed or returned None.\")\n",
    "    else:\n",
    "        print(\"Cannot train final model: No groups found in the training data.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping final model training due to missing data, groups, or best_params_from_tuning.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Preparing Test Data and Generating Kaggle Submission ---\n",
      "Loading test data from: ../data.nosync/test.csv...\n",
      "Loaded test dataset with shape: (4959183, 50)\n",
      "\\nPreprocessing test data...\n",
      "Imputing missing values in test data using training set medians...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/ww14kyzx7zq1bdp4kybt0_800000gn/T/ipykernel_87350/802724624.py:42: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_test[col].fillna(train_median, inplace=True)\n",
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaNs remaining in X_test after imputation: 0\n",
      "\\n--- Predicting on Test Data and Formatting Submission ---\n",
      "Submission file 'submission_modular.csv' created. Top 5 rows:\n",
      "   SearchId  PropertyId\n",
      "0         1       54937\n",
      "1         1       99484\n",
      "2         1       61934\n",
      "3         1       24194\n",
      "4         1       28181\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission (using modular function) ---\n",
    "\n",
    "if final_trained_model is not None and X is not None and 'feature_columns' in locals() and feature_columns:\n",
    "    print(\"\\\\n--- Preparing Test Data and Generating Kaggle Submission ---\")\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    print(f\"Loading test data from: {TEST_FILE}...\")\n",
    "    try:\n",
    "        df_test_raw = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test_raw.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Test file not found at {TEST_FILE}\")\n",
    "        df_test_raw = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        df_test_raw = None\n",
    "\n",
    "    if df_test_raw is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\\\nPreprocessing test data...\")\n",
    "        \n",
    "        # Ensure all selected feature columns exist in df_test_raw\n",
    "        # and handle any missing columns gracefully if necessary (e.g. by creating them with NaNs)\n",
    "        X_test_list = []\n",
    "        for col in feature_columns:\n",
    "            if col not in df_test_raw.columns:\n",
    "                print(f\"Warning: Feature column '{col}' not found in test data. Creating it with NaNs.\")\n",
    "                df_test_raw[col] = np.nan \n",
    "        \n",
    "        X_test = df_test_raw[feature_columns].copy()\n",
    "\n",
    "        # Impute missing values in X_test using medians from the TRAINING sample (X)\n",
    "        # X should be the dataframe of features used for training the final_trained_model\n",
    "        print(\"Imputing missing values in test data using training set medians...\")\n",
    "        nan_counts_before_imputation = X_test.isnull().sum()\n",
    "\n",
    "        for col in X_test.columns:\n",
    "            if X_test[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                    if col in X.columns: # Ensure the column exists in the training features X\n",
    "                        train_median = X[col].median() # Calculate median from the TRAIN features (X)\n",
    "                        X_test[col].fillna(train_median, inplace=True)\n",
    "                        # print(f\"Imputed NaNs in test column '{col}' with training median: {train_median}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Column '{col}' for median imputation not found in training X. Test NaNs may remain.\")\n",
    "                # else: # For categorical, use mode from training X\n",
    "                    # if col in X.columns:\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col].fillna(train_mode, inplace=True)\n",
    "                    # else:\n",
    "                    #     print(f\"Warning: Column '{col}' for mode imputation not found in training X. Test NaNs may remain.\")\n",
    "        \n",
    "        nan_counts_after_imputation = X_test.isnull().sum().sum()\n",
    "        print(f\"NaNs remaining in X_test after imputation: {nan_counts_after_imputation}\")\n",
    "        if nan_counts_after_imputation > 0:\n",
    "            print(\"Warning: Some NaNs remain in test features after imputation. Review missing columns or imputation logic.\")\n",
    "            print(X_test.isnull().sum()[X_test.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "        # --- 8c. Generate Submission File ---\n",
    "        # The df_test_raw contains 'srch_id' and 'prop_id' needed by the submission function\n",
    "        lgbm_model.predict_and_format_submission(\n",
    "            model=final_trained_model,\n",
    "            X_test=X_test,\n",
    "            df_test_original_ids=df_test_raw, # Pass the raw test df for srch_id and prop_id\n",
    "            submission_filename=SUBMISSION_FILENAME\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping submission generation as test data could not be loaded.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping Kaggle submission: final_trained_model, X, or feature_columns not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set...\n",
      "Selected 11 features.\n",
      "Feature columns: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'orig_destination_distance']\n",
      "Shape of X: (496212, 11)\n",
      "Shape of y: (496212,)\n",
      "Number of groups: 19979, Min group size: 5, Max group size: 38\n",
      "NaNs remaining in X 0\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "# --- 4. Initial Feature Selection (Placeholder) ---\n",
    "# This will be refined by your friend. For now, using a subset of potentially useful features.\n",
    "if 'df_sample' in locals() and df_sample is not None:\n",
    "    print(\"\\nDefining initial feature set...\")\n",
    "    # Features identified as potentially important or commonly used, excluding IDs and target-leaking columns\n",
    "    # Also excluding competitor columns with high missing rates for now, and user history due to high missingness\n",
    "    feature_columns = [\n",
    "        'visitor_location_country_id', 'prop_country_id',\n",
    "        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "        'price_usd', 'promotion_flag', 'orig_destination_distance'\n",
    "        # 'srch_query_affinity_score' # high missingness\n",
    "        # Add more features here as EDA suggests and after handling missing values\n",
    "    ]\n",
    "\n",
    "    # For ranking, we need features (X), relevance (y), and group/query_id\n",
    "    X = df_sample[feature_columns]\n",
    "    y = df_sample['relevance']\n",
    "    groups = df_sample.groupby('srch_id').size().to_numpy() # Size of each group\n",
    "\n",
    "    print(f\"Selected {len(feature_columns)} features.\")\n",
    "    print(\"Feature columns:\", feature_columns)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    print(f\"Number of groups: {len(groups)}, Min group size: {groups.min()}, Max group size: {groups.max()}\")\n",
    "    '''\n",
    "    # Handle Missing Values (Simple Imputation for now)\n",
    "    # For a proper model, more sophisticated imputation or feature engineering for missingness is needed.\n",
    "    print(\"\\nHandling missing values (simple median imputation for numerical)...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "                print(f\"Imputed missing values in {col} with median.\")\n",
    "            # else: # For categorical, fill with mode or a specific placeholder\n",
    "            #     X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "    # Check if any NaNs remain (should ideally be none for numeric after this)\n",
    "    '''\n",
    "else:\n",
    "    print(\"Skipping feature selection due to data sampling issues.\")\n",
    "print(\"NaNs remaining in X\", X.isnull().sum().sum())\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Cross-Validation with GroupKFold ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "X_train shape: (396968, 11), y_train shape: (396968,), Num train groups: 15983\n",
      "X_val shape: (99244, 11), y_val shape: (99244,), Num val groups: 3996\n",
      "Training LGBMRanker for fold 1...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[10]\tvalid_0's ndcg@1: 0.175776\tvalid_0's ndcg@2: 0.243204\tvalid_0's ndcg@3: 0.286728\tvalid_0's ndcg@4: 0.322332\tvalid_0's ndcg@5: 0.347355\n",
      "Fold 1 NDCG@5: 0.3465\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "X_train shape: (396973, 11), y_train shape: (396973,), Num train groups: 15984\n",
      "X_val shape: (99239, 11), y_val shape: (99239,), Num val groups: 3995\n",
      "Training LGBMRanker for fold 2...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[13]\tvalid_0's ndcg@1: 0.181477\tvalid_0's ndcg@2: 0.249574\tvalid_0's ndcg@3: 0.294896\tvalid_0's ndcg@4: 0.329653\tvalid_0's ndcg@5: 0.354915\n",
      "Fold 2 NDCG@5: 0.3558\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training LGBMRanker for fold 3...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[36]\tvalid_0's ndcg@1: 0.176226\tvalid_0's ndcg@2: 0.241887\tvalid_0's ndcg@3: 0.290873\tvalid_0's ndcg@4: 0.322676\tvalid_0's ndcg@5: 0.347047\n",
      "Fold 3 NDCG@5: 0.3471\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training LGBMRanker for fold 4...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[40]\tvalid_0's ndcg@1: 0.172322\tvalid_0's ndcg@2: 0.244413\tvalid_0's ndcg@3: 0.286435\tvalid_0's ndcg@4: 0.3178\tvalid_0's ndcg@5: 0.343897\n",
      "Fold 4 NDCG@5: 0.3449\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training LGBMRanker for fold 5...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's ndcg@1: 0.177878\tvalid_0's ndcg@2: 0.248641\tvalid_0's ndcg@3: 0.292467\tvalid_0's ndcg@4: 0.327237\tvalid_0's ndcg@5: 0.353378\n",
      "Fold 5 NDCG@5: 0.3517\n",
      "\n",
      "Mean NDCG@5 across 5 successfully evaluated folds: 0.3492 +/- 0.0040\n",
      "\n",
      "Average Feature Importances (Cross-Validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           14327.886284\n",
       "price_usd                       9679.488317\n",
       "prop_location_score1            5469.095838\n",
       "prop_starrating                 4524.345251\n",
       "prop_log_historical_price       3634.961211\n",
       "promotion_flag                  2230.896334\n",
       "prop_review_score               2186.490114\n",
       "orig_destination_distance       1154.970879\n",
       "prop_country_id                  679.057501\n",
       "visitor_location_country_id      240.637339\n",
       "prop_brand_bool                  180.776899\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 5. Model Training (LGBMRanker) ---\n",
    "# This section will contain the training loop using GroupKFold\n",
    "\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None: # Use df_sample for srch_id\n",
    "    print(\"\\n--- 5. Cross-Validation with GroupKFold ---\")\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    \n",
    "    fold_ndcg_scores = []\n",
    "    all_feature_importances = pd.DataFrame()\n",
    "\n",
    "    # The groups parameter for gkf.split should be the srch_id for each row in X\n",
    "    # It ensures that rows with the same srch_id are not split across train/test in a fold.\n",
    "    unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Group counts for the current training and validation fold\n",
    "        # Need to use the original df_sample with srch_id to correctly form groups for the subsets\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "        \n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "        \n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "\n",
    "        ranker_cv = lgb.LGBMRanker(\n",
    "            objective='lambdarank',\n",
    "            metric='ndcg',\n",
    "            label_gain=[0, 1, 5], # Corresponds to relevance 0, 1, 5\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1,\n",
    "            importance_type='gain',\n",
    "            random_state=RANDOM_STATE + fold, \n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        print(f\"Training LGBMRanker for fold {fold+1}...\")\n",
    "        ranker_cv.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg', \n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if ranker_cv.evals_result_ and 'valid_0' in ranker_cv.evals_result_ and 'ndcg@5' in ranker_cv.evals_result_['valid_0']:\n",
    "            ndcg_at_5 = ranker_cv.evals_result_['valid_0']['ndcg@5'][-1] \n",
    "            fold_ndcg_scores.append(ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} NDCG@5: {ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': ranker_cv.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            all_feature_importances = pd.concat([all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1}. Skipping score for this fold.\")\n",
    "\n",
    "\n",
    "    if fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 across {len(fold_ndcg_scores)} successfully evaluated folds: {np.mean(fold_ndcg_scores):.4f} +/- {np.std(fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not all_feature_importances.empty:\n",
    "            mean_feature_importances = all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Cross-Validation):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed with NDCG scores.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping model training and cross-validation due to earlier data processing issues.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:15,778] A new study created in memory with name: lgbm_ranker_optimization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Hyperparameter Tuning with Optuna ---\n",
      "Starting Optuna hyperparameter tuning with 20 trials...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:18,278] Trial 0 finished with value: 0.3504321633173449 and parameters: {'n_estimators': 450, 'learning_rate': 0.11408934177550577, 'num_leaves': 141, 'max_depth': 3, 'min_child_samples': 89, 'subsample': 0.742713365157357, 'colsample_bytree': 0.5320066224512843, 'reg_alpha': 0.04821567442910248, 'reg_lambda': 3.223570664091658}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 finished with avg NDCG@5: 0.3504 for params: {'n_estimators': 450, 'learning_rate': 0.11408934177550577, 'num_leaves': 141, 'max_depth': 3, 'min_child_samples': 89, 'subsample': 0.742713365157357, 'colsample_bytree': 0.5320066224512843, 'reg_alpha': 0.04821567442910248, 'reg_lambda': 3.223570664091658}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:21,937] Trial 1 finished with value: 0.3422898201628106 and parameters: {'n_estimators': 500, 'learning_rate': 0.023883932378738804, 'num_leaves': 94, 'max_depth': 9, 'min_child_samples': 45, 'subsample': 0.9700038221215604, 'colsample_bytree': 0.6083726787277264, 'reg_alpha': 0.5414737974882534, 'reg_lambda': 0.16431215973222515}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 finished with avg NDCG@5: 0.3423 for params: {'n_estimators': 500, 'learning_rate': 0.023883932378738804, 'num_leaves': 94, 'max_depth': 9, 'min_child_samples': 45, 'subsample': 0.9700038221215604, 'colsample_bytree': 0.6083726787277264, 'reg_alpha': 0.5414737974882534, 'reg_lambda': 0.16431215973222515}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:26,689] Trial 2 finished with value: 0.34334889630304505 and parameters: {'n_estimators': 650, 'learning_rate': 0.011460995266279054, 'num_leaves': 68, 'max_depth': 8, 'min_child_samples': 96, 'subsample': 0.9320762121405267, 'colsample_bytree': 0.835334689821756, 'reg_alpha': 1.2410789768716448, 'reg_lambda': 0.010488226621413828}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 finished with avg NDCG@5: 0.3433 for params: {'n_estimators': 650, 'learning_rate': 0.011460995266279054, 'num_leaves': 68, 'max_depth': 8, 'min_child_samples': 96, 'subsample': 0.9320762121405267, 'colsample_bytree': 0.835334689821756, 'reg_alpha': 1.2410789768716448, 'reg_lambda': 0.010488226621413828}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:29,385] Trial 3 finished with value: 0.34446429747725 and parameters: {'n_estimators': 400, 'learning_rate': 0.020422862992246896, 'num_leaves': 119, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.9859988258949455, 'colsample_bytree': 0.7882728241038102, 'reg_alpha': 0.001773388068494822, 'reg_lambda': 1.3288807072298627}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 finished with avg NDCG@5: 0.3445 for params: {'n_estimators': 400, 'learning_rate': 0.020422862992246896, 'num_leaves': 119, 'max_depth': 5, 'min_child_samples': 19, 'subsample': 0.9859988258949455, 'colsample_bytree': 0.7882728241038102, 'reg_alpha': 0.001773388068494822, 'reg_lambda': 1.3288807072298627}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:36,913] Trial 4 finished with value: 0.34130853034132386 and parameters: {'n_estimators': 500, 'learning_rate': 0.05114946266836074, 'num_leaves': 119, 'max_depth': 12, 'min_child_samples': 62, 'subsample': 0.7523177275144902, 'colsample_bytree': 0.6353110156582764, 'reg_alpha': 0.4966644373135883, 'reg_lambda': 0.0031925998125138548}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 finished with avg NDCG@5: 0.3413 for params: {'n_estimators': 500, 'learning_rate': 0.05114946266836074, 'num_leaves': 119, 'max_depth': 12, 'min_child_samples': 62, 'subsample': 0.7523177275144902, 'colsample_bytree': 0.6353110156582764, 'reg_alpha': 0.4966644373135883, 'reg_lambda': 0.0031925998125138548}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:40,844] Trial 5 finished with value: 0.34902853241388737 and parameters: {'n_estimators': 650, 'learning_rate': 0.1867730266650586, 'num_leaves': 37, 'max_depth': 5, 'min_child_samples': 6, 'subsample': 0.8952857035223396, 'colsample_bytree': 0.9394224688614666, 'reg_alpha': 1.8220558586250621, 'reg_lambda': 0.0033285803696310157}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 finished with avg NDCG@5: 0.3490 for params: {'n_estimators': 650, 'learning_rate': 0.1867730266650586, 'num_leaves': 37, 'max_depth': 5, 'min_child_samples': 6, 'subsample': 0.8952857035223396, 'colsample_bytree': 0.9394224688614666, 'reg_alpha': 1.8220558586250621, 'reg_lambda': 0.0033285803696310157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:43,799] Trial 6 finished with value: 0.3472267476583742 and parameters: {'n_estimators': 400, 'learning_rate': 0.05771832260300673, 'num_leaves': 36, 'max_depth': 12, 'min_child_samples': 40, 'subsample': 0.8544706074506001, 'colsample_bytree': 0.7686597943736168, 'reg_alpha': 3.6007680684135273, 'reg_lambda': 0.0038557128045937443}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6 finished with avg NDCG@5: 0.3472 for params: {'n_estimators': 400, 'learning_rate': 0.05771832260300673, 'num_leaves': 36, 'max_depth': 12, 'min_child_samples': 40, 'subsample': 0.8544706074506001, 'colsample_bytree': 0.7686597943736168, 'reg_alpha': 3.6007680684135273, 'reg_lambda': 0.0038557128045937443}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:47,831] Trial 7 finished with value: 0.34637040381962886 and parameters: {'n_estimators': 150, 'learning_rate': 0.02898696540038131, 'num_leaves': 58, 'max_depth': 4, 'min_child_samples': 9, 'subsample': 0.5273621471489346, 'colsample_bytree': 0.7953613676662146, 'reg_alpha': 0.02919579194727492, 'reg_lambda': 0.002407489331568478}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 finished with avg NDCG@5: 0.3464 for params: {'n_estimators': 150, 'learning_rate': 0.02898696540038131, 'num_leaves': 58, 'max_depth': 4, 'min_child_samples': 9, 'subsample': 0.5273621471489346, 'colsample_bytree': 0.7953613676662146, 'reg_alpha': 0.02919579194727492, 'reg_lambda': 0.002407489331568478}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:54,040] Trial 8 finished with value: 0.34649392407993274 and parameters: {'n_estimators': 100, 'learning_rate': 0.09138350288839198, 'num_leaves': 59, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.9271397411020879, 'colsample_bytree': 0.8985484051015569, 'reg_alpha': 7.915355296001399, 'reg_lambda': 0.1134434971599035}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8 finished with avg NDCG@5: 0.3465 for params: {'n_estimators': 100, 'learning_rate': 0.09138350288839198, 'num_leaves': 59, 'max_depth': 11, 'min_child_samples': 16, 'subsample': 0.9271397411020879, 'colsample_bytree': 0.8985484051015569, 'reg_alpha': 7.915355296001399, 'reg_lambda': 0.1134434971599035}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:35:58,497] Trial 9 finished with value: 0.3396182190155428 and parameters: {'n_estimators': 350, 'learning_rate': 0.014277687851517654, 'num_leaves': 106, 'max_depth': 9, 'min_child_samples': 73, 'subsample': 0.7253424780133684, 'colsample_bytree': 0.8242562128537649, 'reg_alpha': 0.04690920987986248, 'reg_lambda': 0.0072856035793310995}. Best is trial 0 with value: 0.3504321633173449.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9 finished with avg NDCG@5: 0.3396 for params: {'n_estimators': 350, 'learning_rate': 0.014277687851517654, 'num_leaves': 106, 'max_depth': 9, 'min_child_samples': 73, 'subsample': 0.7253424780133684, 'colsample_bytree': 0.8242562128537649, 'reg_alpha': 0.04690920987986248, 'reg_lambda': 0.0072856035793310995}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:01,760] Trial 10 finished with value: 0.35097621989821665 and parameters: {'n_estimators': 250, 'learning_rate': 0.19678085320205846, 'num_leaves': 144, 'max_depth': 3, 'min_child_samples': 96, 'subsample': 0.6005514711808235, 'colsample_bytree': 0.5002926412539057, 'reg_alpha': 0.003205944770438764, 'reg_lambda': 5.229994576420763}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 finished with avg NDCG@5: 0.3510 for params: {'n_estimators': 250, 'learning_rate': 0.19678085320205846, 'num_leaves': 144, 'max_depth': 3, 'min_child_samples': 96, 'subsample': 0.6005514711808235, 'colsample_bytree': 0.5002926412539057, 'reg_alpha': 0.003205944770438764, 'reg_lambda': 5.229994576420763}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:03,650] Trial 11 finished with value: 0.3500807466476193 and parameters: {'n_estimators': 250, 'learning_rate': 0.18936480103618095, 'num_leaves': 150, 'max_depth': 3, 'min_child_samples': 99, 'subsample': 0.5676878704016968, 'colsample_bytree': 0.5039312049704797, 'reg_alpha': 0.004342778532384315, 'reg_lambda': 8.045625028445603}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 11 finished with avg NDCG@5: 0.3501 for params: {'n_estimators': 250, 'learning_rate': 0.18936480103618095, 'num_leaves': 150, 'max_depth': 3, 'min_child_samples': 99, 'subsample': 0.5676878704016968, 'colsample_bytree': 0.5039312049704797, 'reg_alpha': 0.004342778532384315, 'reg_lambda': 8.045625028445603}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:05,868] Trial 12 finished with value: 0.34974115980416576 and parameters: {'n_estimators': 250, 'learning_rate': 0.10097812296423751, 'num_leaves': 149, 'max_depth': 3, 'min_child_samples': 83, 'subsample': 0.6279780895866396, 'colsample_bytree': 0.5079338671091401, 'reg_alpha': 0.012226775152515613, 'reg_lambda': 7.4086301913364485}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 finished with avg NDCG@5: 0.3497 for params: {'n_estimators': 250, 'learning_rate': 0.10097812296423751, 'num_leaves': 149, 'max_depth': 3, 'min_child_samples': 83, 'subsample': 0.6279780895866396, 'colsample_bytree': 0.5079338671091401, 'reg_alpha': 0.012226775152515613, 'reg_lambda': 7.4086301913364485}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:12,124] Trial 13 finished with value: 0.34525103317223094 and parameters: {'n_estimators': 500, 'learning_rate': 0.1240600518337226, 'num_leaves': 131, 'max_depth': 6, 'min_child_samples': 83, 'subsample': 0.6861765626698988, 'colsample_bytree': 0.6189955340483645, 'reg_alpha': 0.1607115771839014, 'reg_lambda': 1.133130334323549}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 13 finished with avg NDCG@5: 0.3453 for params: {'n_estimators': 500, 'learning_rate': 0.1240600518337226, 'num_leaves': 131, 'max_depth': 6, 'min_child_samples': 83, 'subsample': 0.6861765626698988, 'colsample_bytree': 0.6189955340483645, 'reg_alpha': 0.1607115771839014, 'reg_lambda': 1.133130334323549}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:16,504] Trial 14 finished with value: 0.34284177868202637 and parameters: {'n_estimators': 300, 'learning_rate': 0.13326234853568378, 'num_leaves': 133, 'max_depth': 6, 'min_child_samples': 85, 'subsample': 0.80994357822961, 'colsample_bytree': 0.5688778167925024, 'reg_alpha': 0.0012361045529986699, 'reg_lambda': 1.0114461760154043}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 14 finished with avg NDCG@5: 0.3428 for params: {'n_estimators': 300, 'learning_rate': 0.13326234853568378, 'num_leaves': 133, 'max_depth': 6, 'min_child_samples': 85, 'subsample': 0.80994357822961, 'colsample_bytree': 0.5688778167925024, 'reg_alpha': 0.0012361045529986699, 'reg_lambda': 1.0114461760154043}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:19,034] Trial 15 finished with value: 0.3471746349948053 and parameters: {'n_estimators': 200, 'learning_rate': 0.07274464976639444, 'num_leaves': 86, 'max_depth': 3, 'min_child_samples': 63, 'subsample': 0.6204563314152839, 'colsample_bytree': 0.6754314569540577, 'reg_alpha': 0.007994509655092024, 'reg_lambda': 3.138200250800137}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 15 finished with avg NDCG@5: 0.3472 for params: {'n_estimators': 200, 'learning_rate': 0.07274464976639444, 'num_leaves': 86, 'max_depth': 3, 'min_child_samples': 63, 'subsample': 0.6204563314152839, 'colsample_bytree': 0.6754314569540577, 'reg_alpha': 0.007994509655092024, 'reg_lambda': 3.138200250800137}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:21,688] Trial 16 finished with value: 0.34688387396004594 and parameters: {'n_estimators': 550, 'learning_rate': 0.035278452493391226, 'num_leaves': 134, 'max_depth': 5, 'min_child_samples': 100, 'subsample': 0.6652624381620467, 'colsample_bytree': 0.7060326985391507, 'reg_alpha': 0.09736880164662497, 'reg_lambda': 0.21259500328747996}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 16 finished with avg NDCG@5: 0.3469 for params: {'n_estimators': 550, 'learning_rate': 0.035278452493391226, 'num_leaves': 134, 'max_depth': 5, 'min_child_samples': 100, 'subsample': 0.6652624381620467, 'colsample_bytree': 0.7060326985391507, 'reg_alpha': 0.09736880164662497, 'reg_lambda': 0.21259500328747996}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:25,949] Trial 17 finished with value: 0.3369179974978604 and parameters: {'n_estimators': 300, 'learning_rate': 0.14478424379017577, 'num_leaves': 106, 'max_depth': 7, 'min_child_samples': 74, 'subsample': 0.7648492368159003, 'colsample_bytree': 0.5464300555743251, 'reg_alpha': 0.017800257736990985, 'reg_lambda': 0.03496888757423233}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 17 finished with avg NDCG@5: 0.3369 for params: {'n_estimators': 300, 'learning_rate': 0.14478424379017577, 'num_leaves': 106, 'max_depth': 7, 'min_child_samples': 74, 'subsample': 0.7648492368159003, 'colsample_bytree': 0.5464300555743251, 'reg_alpha': 0.017800257736990985, 'reg_lambda': 0.03496888757423233}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:29,667] Trial 18 finished with value: 0.35061364106352766 and parameters: {'n_estimators': 400, 'learning_rate': 0.09453914199038851, 'num_leaves': 120, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.5236709081241935, 'colsample_bytree': 0.5620903563546057, 'reg_alpha': 0.0035082659900084233, 'reg_lambda': 0.4218739754022182}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 18 finished with avg NDCG@5: 0.3506 for params: {'n_estimators': 400, 'learning_rate': 0.09453914199038851, 'num_leaves': 120, 'max_depth': 4, 'min_child_samples': 33, 'subsample': 0.5236709081241935, 'colsample_bytree': 0.5620903563546057, 'reg_alpha': 0.0035082659900084233, 'reg_lambda': 0.4218739754022182}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-14 14:36:32,484] Trial 19 finished with value: 0.3488388570225307 and parameters: {'n_estimators': 350, 'learning_rate': 0.07442784086487549, 'num_leaves': 117, 'max_depth': 4, 'min_child_samples': 32, 'subsample': 0.5153028502978984, 'colsample_bytree': 0.7090245334690404, 'reg_alpha': 0.003506378672987832, 'reg_lambda': 0.44821600478665585}. Best is trial 10 with value: 0.35097621989821665.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 19 finished with avg NDCG@5: 0.3488 for params: {'n_estimators': 350, 'learning_rate': 0.07442784086487549, 'num_leaves': 117, 'max_depth': 4, 'min_child_samples': 32, 'subsample': 0.5153028502978984, 'colsample_bytree': 0.7090245334690404, 'reg_alpha': 0.003506378672987832, 'reg_lambda': 0.44821600478665585}\n",
      "\n",
      "Optuna study statistics:\n",
      "  Number of finished trials: 20\n",
      "  Best trial value (NDCG@5): 0.3510\n",
      "  Best parameters found by Optuna:\n",
      "    n_estimators: 250\n",
      "    learning_rate: 0.19678085320205846\n",
      "    num_leaves: 144\n",
      "    max_depth: 3\n",
      "    min_child_samples: 96\n",
      "    subsample: 0.6005514711808235\n",
      "    colsample_bytree: 0.5002926412539057\n",
      "    reg_alpha: 0.003205944770438764\n",
      "    reg_lambda: 5.229994576420763\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 6. Hyperparameter Tuning with Optuna ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None:\n",
    "    print(\"\\n--- 6. Hyperparameter Tuning with Optuna ---\")\n",
    "    import optuna\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARNING) # Reduce Optuna's verbosity if needed\n",
    "\n",
    "    # Define the objective function for Optuna\n",
    "    def objective(trial):\n",
    "        # Define search space for hyperparameters\n",
    "        params = {\n",
    "            'objective': 'lambdarank',\n",
    "            'metric': 'ndcg',\n",
    "            'label_gain': [0, 1, 5], # If using remapped (0,1,2) relevance\n",
    "            'eval_at': [5],\n",
    "            'random_state': RANDOM_STATE,\n",
    "            'n_jobs': -1, # Use all available cores for LGBM training\n",
    "            'verbosity': -1, # Suppress LightGBM's own verbosity during tuning trials\n",
    "            'boosting_type': 'gbdt', # Default, but can be tuned\n",
    "\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 100, 700, step=50),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n",
    "            'num_leaves': trial.suggest_int('num_leaves', 20, 150),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n",
    "            'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "            'reg_alpha': trial.suggest_float('reg_alpha', 1e-3, 10.0, log=True),    # L1 regularization\n",
    "            'reg_lambda': trial.suggest_float('reg_lambda', 1e-3, 10.0, log=True), # L2 regularization\n",
    "        }\n",
    "\n",
    "        gkf_for_optuna = GroupKFold(n_splits=3) # Use 3 folds for faster tuning trials\n",
    "        fold_ndcg_scores = []\n",
    "        \n",
    "        # Ensure unique_group_ids_for_splitting is available in this scope\n",
    "        # It should be df_sample['srch_id']\n",
    "        current_groups_for_splitting = df_sample['srch_id']\n",
    "\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(gkf_for_optuna.split(X, y, groups=current_groups_for_splitting)):\n",
    "            X_train_trial, X_val_trial = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_trial, y_val_trial = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            train_groups_trial = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "            val_groups_trial = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "            \n",
    "            if len(train_groups_trial) == 0 or len(val_groups_trial) == 0 or X_train_trial.empty or X_val_trial.empty:\n",
    "                # print(f\"Skipping fold {fold+1} in trial {trial.number} due to empty groups/data.\")\n",
    "                # Return a very low score if a fold fails, to penalize these hyperparams\n",
    "                return -1.0 # Or handle as per Optuna's pruning/failure guidelines\n",
    "\n",
    "            model_trial = lgb.LGBMRanker(**params)\n",
    "            \n",
    "            model_trial.fit(\n",
    "                X_train_trial, y_train_trial, group=train_groups_trial,\n",
    "                eval_set=[(X_val_trial, y_val_trial)],\n",
    "                eval_group=[val_groups_trial],\n",
    "                eval_metric='ndcg', # LightGBM will use metric from params, this is for eval_set\n",
    "                callbacks=[lgb.early_stopping(10, verbose=False)]\n",
    "            )\n",
    "            \n",
    "            if model_trial.evals_result_ and 'valid_0' in model_trial.evals_result_ and 'ndcg@5' in model_trial.evals_result_['valid_0']:\n",
    "                score = model_trial.evals_result_['valid_0']['ndcg@5'][-1]\n",
    "                fold_ndcg_scores.append(score)\n",
    "            else:\n",
    "                # print(f\"NDCG@5 not found for fold {fold+1} in trial {trial.number}.\")\n",
    "                fold_ndcg_scores.append(0.0) # Penalize if score not found\n",
    "\n",
    "            # Optuna Pruning (optional, but good for long searches)\n",
    "            # trial.report(score, fold)\n",
    "            # if trial.should_prune():\n",
    "            #     raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        avg_ndcg = np.mean(fold_ndcg_scores) if fold_ndcg_scores else 0.0\n",
    "        print(f\"Trial {trial.number} finished with avg NDCG@5: {avg_ndcg:.4f} for params: {trial.params}\")\n",
    "        return avg_ndcg\n",
    "\n",
    "    # Create a study object and optimize the objective function.\n",
    "    # n_trials: number of hyperparameter combinations to test.\n",
    "    # Adjust based on available time (e.g., 20-50 for a decent search, 100+ for more thorough).\n",
    "    N_OPTUNA_TRIALS = 20 # Start with a smaller number for testing\n",
    "    # Suppress the eval_at warning from LightGBM\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*')\n",
    "    study = optuna.create_study(direction='maximize', study_name='lgbm_ranker_optimization')\n",
    "    \n",
    "    print(f\"Starting Optuna hyperparameter tuning with {N_OPTUNA_TRIALS} trials...\")\n",
    "    try:\n",
    "        study.optimize(objective, n_trials=N_OPTUNA_TRIALS, timeout=None) # timeout in seconds if needed\n",
    "\n",
    "        print(\"\\nOptuna study statistics:\")\n",
    "        print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "        \n",
    "        best_trial = study.best_trial\n",
    "        print(f\"  Best trial value (NDCG@5): {best_trial.value:.4f}\")\n",
    "        print(\"  Best parameters found by Optuna:\")\n",
    "        for key, value in best_trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "        \n",
    "        best_params_from_tuning = best_trial.params\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Optuna optimization: {e}\")\n",
    "        print(\"Falling back to default parameters for the final model evaluation.\")\n",
    "        best_params_from_tuning = { # Default fallback\n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31,\n",
    "            'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "            'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0\n",
    "        }\n",
    "else:\n",
    "    print(\"\\nSkipping Optuna hyperparameter tuning due to earlier data processing issues.\")\n",
    "    best_params_from_tuning = { # Default fallback\n",
    "        'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31,\n",
    "        'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0\n",
    "    }\n",
    "\n",
    "# The rest of your notebook (Section 7: Detailed Evaluation, Section 8: Submission)\n",
    "# should now use `best_params_from_tuning` obtained from Optuna.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\n",
      "\n",
      "Final model parameters for evaluation:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 250, 'learning_rate': 0.19678085320205846, 'num_leaves': 144, 'max_depth': 3, 'min_child_samples': 96, 'subsample': 0.6005514711808235, 'colsample_bytree': 0.5002926412539057, 'reg_alpha': 0.003205944770438764, 'reg_lambda': 5.229994576420763}\n",
      "\n",
      "--- Final Model Evaluation: Fold 1/5 ---\n",
      "X_train shape: (396968, 11), y_train shape: (396968,), Num train groups: 15983\n",
      "X_val shape: (99244, 11), y_val shape: (99244,), Num val groups: 3996\n",
      "Training final tuned LGBMRanker for fold 1...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[33]\tvalid_0's ndcg@5: 0.351708\n",
      "Fold 1 (Tuned Model) NDCG@5: 0.3492\n",
      "\n",
      "--- Final Model Evaluation: Fold 2/5 ---\n",
      "X_train shape: (396973, 11), y_train shape: (396973,), Num train groups: 15984\n",
      "X_val shape: (99239, 11), y_val shape: (99239,), Num val groups: 3995\n",
      "Training final tuned LGBMRanker for fold 2...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[23]\tvalid_0's ndcg@5: 0.355623\n",
      "Fold 2 (Tuned Model) NDCG@5: 0.3529\n",
      "\n",
      "--- Final Model Evaluation: Fold 3/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training final tuned LGBMRanker for fold 3...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[37]\tvalid_0's ndcg@5: 0.351017\n",
      "Fold 3 (Tuned Model) NDCG@5: 0.3496\n",
      "\n",
      "--- Final Model Evaluation: Fold 4/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training final tuned LGBMRanker for fold 4...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[51]\tvalid_0's ndcg@5: 0.34763\n",
      "Fold 4 (Tuned Model) NDCG@5: 0.3471\n",
      "\n",
      "--- Final Model Evaluation: Fold 5/5 ---\n",
      "X_train shape: (396969, 11), y_train shape: (396969,), Num train groups: 15983\n",
      "X_val shape: (99243, 11), y_val shape: (99243,), Num val groups: 3996\n",
      "Training final tuned LGBMRanker for fold 5...\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[45]\tvalid_0's ndcg@5: 0.354096\n",
      "Fold 5 (Tuned Model) NDCG@5: 0.3525\n",
      "\n",
      "Mean NDCG@5 for Tuned Model across 5 successfully evaluated folds: 0.3503 +/- 0.0022\n",
      "\n",
      "Average Feature Importances (Tuned Model):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           8815.952521\n",
       "price_usd                      5964.114531\n",
       "prop_starrating                2783.595308\n",
       "prop_location_score1           1967.285754\n",
       "prop_review_score              1584.426287\n",
       "prop_log_historical_price      1452.657886\n",
       "promotion_flag                 1415.325307\n",
       "orig_destination_distance       298.491524\n",
       "prop_country_id                 152.657004\n",
       "prop_brand_bool                  92.884254\n",
       "visitor_location_country_id       0.000000\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# --- 7. Detailed Evaluation of Best Tuned Model ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None and best_params_from_tuning:\n",
    "    print(\"\\n--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\")\n",
    "\n",
    "    final_gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    final_fold_ndcg_scores = []\n",
    "    final_all_feature_importances = pd.DataFrame()\n",
    "    \n",
    "    # Ensure unique_group_ids_for_splitting is available\n",
    "    if 'unique_group_ids_for_splitting' not in locals():\n",
    "        unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "\n",
    "    final_ranker_params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'label_gain': [0, 1, 5],\n",
    "        'eval_at': [5],\n",
    "        'importance_type': 'gain',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    # Update with tuned parameters, ensuring they are valid\n",
    "    for key, value in best_params_from_tuning.items():\n",
    "        final_ranker_params[key] = value\n",
    "    \n",
    "    # Ensure n_estimators is present if not tuned or set to a low value by tuning\n",
    "    if 'n_estimators' not in final_ranker_params or final_ranker_params['n_estimators'] < 50:\n",
    "         final_ranker_params['n_estimators'] = 300 # Default if not well-tuned by a short search\n",
    "\n",
    "    print(\"\\nFinal model parameters for evaluation:\")\n",
    "    print(final_ranker_params)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(final_gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Final Model Evaluation: Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "\n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "            \n",
    "        final_ranker = lgb.LGBMRanker(**final_ranker_params)\n",
    "\n",
    "        print(f\"Training final tuned LGBMRanker for fold {fold+1}...\")\n",
    "        final_ranker.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg',\n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if final_ranker.evals_result_ and 'valid_0' in final_ranker.evals_result_ and 'ndcg@5' in final_ranker.evals_result_['valid_0']:\n",
    "            final_ndcg_at_5 = final_ranker.evals_result_['valid_0']['ndcg@5'][-1]\n",
    "            final_fold_ndcg_scores.append(final_ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} (Tuned Model) NDCG@5: {final_ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': final_ranker.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            final_all_feature_importances = pd.concat([final_all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1} of the tuned model.\")\n",
    "\n",
    "\n",
    "    if final_fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 for Tuned Model across {len(final_fold_ndcg_scores)} successfully evaluated folds: {np.mean(final_fold_ndcg_scores):.4f} +/- {np.std(final_fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not final_all_feature_importances.empty:\n",
    "            final_mean_feature_importances = final_all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Tuned Model):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(final_mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed for the final tuned model evaluation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nSkipping final model evaluation due to earlier issues or no tuned parameters found.\")\n",
    "\n",
    "# --- End of Notebook ---\n",
    "# Next steps would involve:\n",
    "# 1. More sophisticated feature engineering and selection.\n",
    "# 2. Training the best model on the full (sampled) data or even a larger fraction.\n",
    "# 3. Preparing the test data similarly.\n",
    "# 4. Generating predictions for the test set.\n",
    "# 5. Creating the Kaggle submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Test Data Preparation and Kaggle Submission ---\n",
      "Loading test data...\n",
      "Loaded test dataset with shape: (4959183, 50)\n",
      "\n",
      "Preprocessing test data...\n",
      "Using feature columns: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'orig_destination_distance']\n",
      "Imputing missing values in test data using training set medians...\n",
      "NaNs remaining in X_test after imputation: 0\n",
      "\n",
      "Training final model on the full sampled training data (df_sample)...\n",
      "Final model parameters for prediction model:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 250, 'learning_rate': 0.19678085320205846, 'num_leaves': 144, 'max_depth': 3, 'min_child_samples': 96, 'subsample': 0.6005514711808235, 'colsample_bytree': 0.5002926412539057, 'reg_alpha': 0.003205944770438764, 'reg_lambda': 5.229994576420763}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model on 446495 samples, validating on 49717 samples.\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\tvalid_0's ndcg@5: 0.345282\n",
      "Final model training completed.\n",
      "\n",
      "Making predictions on the test set...\n",
      "\n",
      "Formatting predictions for submission...\n",
      "\n",
      "Submission file 'submission.csv' created successfully.\n",
      "   srch_id  prop_id\n",
      "0        1    99484\n",
      "1        1    54937\n",
      "2        1    61934\n",
      "3        1    28181\n",
      "4        1    24194\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission ---\n",
    "\n",
    "if 'df_sample' in locals() and df_sample is not None and \\\n",
    "   'X' in locals() and X is not None and \\\n",
    "   'best_params_from_tuning' in locals() and best_params_from_tuning:\n",
    "\n",
    "    print(\"\\n--- 8. Test Data Preparation and Kaggle Submission ---\")\n",
    "    DATA_DIR = '../data.nosync'\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    TEST_FILE = os.path.join(DATA_DIR, 'test.csv')\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "\n",
    "        df_test = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Test file not found at {TEST_FILE}\")\n",
    "        df_test = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during test data loading: {e}\")\n",
    "        df_test = None\n",
    "\n",
    "    if df_test is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\nPreprocessing test data...\")\n",
    "        # Use the same feature_columns as defined for training\n",
    "        if 'feature_columns' not in locals() or not feature_columns:\n",
    "            print(\"Error: feature_columns not defined. Cannot preprocess test data.\")\n",
    "            df_test_processed = None\n",
    "        else:\n",
    "            print(f\"Using feature columns: {feature_columns}\")\n",
    "            X_test = df_test[feature_columns].copy() # Ensure it's a copy\n",
    "\n",
    "            # Impute missing values using medians from the TRAINING sample (X)\n",
    "            # This is crucial to prevent data leakage.\n",
    "            print(\"Imputing missing values in test data using training set medians...\")\n",
    "            for col in X_test.columns:\n",
    "                if X_test[col].isnull().any():\n",
    "                    if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                        # Get median from the original X (training sample before it was split into folds)\n",
    "                        train_median = X[col].median() # X should be the full sample used for training/tuning\n",
    "                        X_test[col] = X_test[col].fillna(train_median)\n",
    "                        # print(f\"Imputed missing values in test column {col} with training median: {train_median}\")\n",
    "                    # else: # For categorical, use mode from training set\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col] = X_test[col].fillna(train_mode)\n",
    "            \n",
    "            print(\"NaNs remaining in X_test after imputation:\", X_test.isnull().sum().sum())\n",
    "            df_test_processed = True\n",
    "\n",
    "\n",
    "        if df_test_processed:\n",
    "            # --- 8c. Train Final Model on Full Sampled Data (df_sample) ---\n",
    "            print(\"\\nTraining final model on the full sampled training data (df_sample)...\")\n",
    "            \n",
    "            # Parameters for the final model\n",
    "            final_model_params = {\n",
    "                'objective': 'lambdarank',\n",
    "                'metric': 'ndcg',\n",
    "                'label_gain': [0, 1, 5], # If using remapped (0,1,2) relevance, gain still [0,1,5]\n",
    "                'eval_at': [5],\n",
    "                'importance_type': 'gain',\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "            final_model_params.update(best_params_from_tuning)\n",
    "            if 'n_estimators' not in final_model_params or final_model_params['n_estimators'] < 50:\n",
    "                final_model_params['n_estimators'] = 300 # A reasonable default for early stopping\n",
    "            \n",
    "            print(\"Final model parameters for prediction model:\")\n",
    "            print(final_model_params)\n",
    "\n",
    "            # Data for final model training\n",
    "            X_full_sample = X # This is df_sample[feature_columns] with imputations\n",
    "            y_full_sample = y # This is df_sample['relevance'] (remapped to 0,1,2 if you implemented that)\n",
    "            groups_full_sample = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            final_model = lgb.LGBMRanker(**final_model_params)\n",
    "            \n",
    "            # For the final model, we can use a small portion of df_sample as an eval set for early stopping\n",
    "            # This is better than no early stopping.\n",
    "            temp_df_for_final_split = pd.DataFrame({\n",
    "                'srch_id': df_sample['srch_id'],\n",
    "                'index_orig': df_sample.index\n",
    "            }).drop_duplicates(subset=['srch_id'])\n",
    "\n",
    "            final_train_srch_ids, final_val_srch_ids = np.split(\n",
    "                temp_df_for_final_split['srch_id'].sample(frac=1, random_state=RANDOM_STATE),\n",
    "                [int(0.9 * len(temp_df_for_final_split))] # 90/10 split for final model's early stopping\n",
    "            )\n",
    "            \n",
    "            final_train_indices = df_sample[df_sample['srch_id'].isin(final_train_srch_ids)].index\n",
    "            final_val_indices = df_sample[df_sample['srch_id'].isin(final_val_srch_ids)].index\n",
    "\n",
    "            X_final_train, X_final_val = X_full_sample.loc[final_train_indices], X_full_sample.loc[final_val_indices]\n",
    "            y_final_train, y_final_val = y_full_sample.loc[final_train_indices], y_full_sample.loc[final_val_indices]\n",
    "            \n",
    "            groups_final_train = df_sample.loc[final_train_indices].groupby('srch_id').size().to_numpy()\n",
    "            groups_final_val = df_sample.loc[final_val_indices].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            if not X_final_val.empty and len(groups_final_val) > 0:\n",
    "                 print(f\"Fitting final model on {len(X_final_train)} samples, validating on {len(X_final_val)} samples.\")\n",
    "                 final_model.fit(\n",
    "                    X_final_train, y_final_train, group=groups_final_train,\n",
    "                    eval_set=[(X_final_val, y_final_val)],\n",
    "                    eval_group=[groups_final_val],\n",
    "                    eval_metric='ndcg',\n",
    "                    callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "                )\n",
    "            else: # Fallback if validation set is too small or problematic\n",
    "                print(\"Validation set for final model is empty/problematic, fitting on all sampled data without early stopping.\")\n",
    "                final_model_params.pop('eval_set', None) # Remove eval params if not using\n",
    "                final_model_params.pop('eval_group', None)\n",
    "                final_model_params.pop('eval_metric', None)\n",
    "                final_model_params.pop('callbacks', None) # No early stopping\n",
    "                # Ensure n_estimators is set to a fixed number if no early stopping\n",
    "                final_model_params['n_estimators'] = best_params_from_tuning.get('n_estimators', 300) # Use tuned or default\n",
    "                final_model = lgb.LGBMRanker(**final_model_params)\n",
    "                final_model.fit(X_full_sample, y_full_sample, group=groups_full_sample)\n",
    "\n",
    "            print(\"Final model training completed.\")\n",
    "\n",
    "            # --- 8d. Make Predictions on Test Data ---\n",
    "            print(\"\\nMaking predictions on the test set...\")\n",
    "            test_predictions = final_model.predict(X_test)\n",
    "            df_test['predicted_score'] = test_predictions\n",
    "\n",
    "            # --- 8e. Format Predictions for Submission ---\n",
    "            print(\"\\nFormatting predictions for submission...\")\n",
    "            submission_list = []\n",
    "            # Group by srch_id and sort by predicted_score\n",
    "            for srch_id, group_df in df_test.groupby('srch_id'):\n",
    "                # Sort properties within each search by the predicted score in descending order\n",
    "                ranked_properties = group_df.sort_values('predicted_score', ascending=False)\n",
    "                for _, row in ranked_properties.iterrows():\n",
    "                    submission_list.append({'srch_id': int(row['srch_id']), 'prop_id': int(row['prop_id'])})\n",
    "            \n",
    "            df_submission = pd.DataFrame(submission_list)\n",
    "\n",
    "            # --- 8f. Create Submission File ---\n",
    "            SUBMISSION_FILE = 'submission.csv'\n",
    "            df_submission.to_csv(SUBMISSION_FILE, index=False)\n",
    "            print(f\"\\nSubmission file '{SUBMISSION_FILE}' created successfully.\")\n",
    "            print(df_submission.head())\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping prediction and submission due to test data processing issues.\")\n",
    "    else:\n",
    "        print(\"Skipping submission generation due to test data loading issues.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Kaggle submission part: Prerequisite data (df_sample, X, best_params_from_tuning) not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# --- 6. Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None:\n",
    "    print(\"\\n--- 6. Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint as sp_randint\n",
    "    from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': sp_randint(100, 500),\n",
    "        'learning_rate': sp_uniform(0.01, 0.19), # Upper bound <0.2 for uniform\n",
    "        'num_leaves': sp_randint(20, 100),      \n",
    "        'max_depth': sp_randint(3, 12),        \n",
    "        'min_child_samples': sp_randint(5, 50), \n",
    "        'subsample': sp_uniform(0.6, 0.4),      # Sum of loc + scale should be <= 1.0. Here, 0.6 + 0.4 = 1.0\n",
    "        'colsample_bytree': sp_uniform(0.6, 0.4), \n",
    "        'reg_alpha': sp_uniform(0, 1),          \n",
    "        'reg_lambda': sp_uniform(0, 1),         \n",
    "    }\n",
    "\n",
    "    base_ranker = lgb.LGBMRanker(\n",
    "        objective='lambdarank',\n",
    "        metric='ndcg', # LightGBM will use this for its internal evaluation\n",
    "        label_gain=[0, 1, 5],\n",
    "        eval_at=[5],\n",
    "        importance_type='gain',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1 # Be cautious with -1 for n_jobs in RandomizedSearchCV if memory is an issue\n",
    "    )\n",
    "\n",
    "    gkf_for_tuning = GroupKFold(n_splits=3) # Using 3 splits for tuning to speed it up\n",
    "\n",
    "    # RandomizedSearchCV setup\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_ranker,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,  # Number of parameter settings sampled. Increase for more thorough search.\n",
    "                    # Set to a small number like 5-10 for quick test, 25-50 for better search.\n",
    "        cv=list(gkf_for_tuning.split(X, y, groups=df_sample['srch_id'])), # Pass the list of splits\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=1, # Start with 1 to avoid potential memory issues, then try increasing.\n",
    "        verbose=2,\n",
    "        # scoring: If None, estimator's score method is used. LGBMRanker's score method should work.\n",
    "        # It calculates NDCG@eval_at based on its parameters.\n",
    "        refit=True # Refits the best estimator on the whole dataset (X,y) passed to fit.\n",
    "                   # For ranking, this full dataset refit will also need group info.\n",
    "    )\n",
    "\n",
    "    print(\"Starting RandomizedSearchCV for hyperparameter tuning...\")\n",
    "    # Pass `groups` to `fit`. This will be used by GroupKFold inside RandomizedSearchCV.\n",
    "    # And `LGBMRanker.fit` will also receive this `groups` argument for each fold.\n",
    "    \n",
    "    # For early stopping inside RandomizedSearchCV, you'd typically pass fit_params.\n",
    "    # This is more complex because eval_set/eval_group change per fold.\n",
    "    # For now, n_estimators is part of the search space.\n",
    "    \n",
    "    best_params_from_tuning = {}\n",
    "    try:\n",
    "        # RandomizedSearchCV will use the `groups` for splitting via the `cv` object\n",
    "        # And `LGBMRanker.fit` will receive the `group` parameter for each fold.\n",
    "        random_search.fit(X, y, groups=df_sample['srch_id']) \n",
    "        \n",
    "        print(\"\\nBest parameters found by RandomizedSearchCV:\")\n",
    "        print(random_search.best_params_)\n",
    "        # The best_score_ will be based on the internal scoring of LGBMRanker (NDCG@5 here)\n",
    "        print(f\"Best score from RandomizedSearchCV (NDCG@5): {random_search.best_score_:.4f}\")\n",
    "        best_params_from_tuning = random_search.best_params_\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RandomizedSearchCV: {e}\")\n",
    "        print(\"Falling back to default parameters for the final model evaluation.\")\n",
    "        # Re-initialize with default in case of error\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "            'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "            'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "        }\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping hyperparameter tuning due to earlier data processing issues.\")\n",
    "    best_params_from_tuning = {\n",
    "        'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "        'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "        'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "    }\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
