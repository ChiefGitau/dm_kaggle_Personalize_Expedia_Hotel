{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:03:15.505240Z",
     "start_time": "2025-05-18T21:00:29.521905Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "Main Experiment Notebook for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb # Keep for type hints if needed, but direct use will be less\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# Import the modularized model functions\n",
    "import lightgbm_ranker_model as lgbm_model\n",
    "import warnings # For managing warnings from Optuna/LightGBM if needed\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'training_set_VU_DM_feature_engin.csv')\n",
    "TEST_FILE = os.path.join(DATA_DIR, 'test_set_VU_DM_feature_engin.csv') # Defined TEST_FILE path\n",
    "SUBMISSION_FILENAME = '../data/submission_modular.csv'  # Defined submission filename\n",
    "\n",
    "SAMPLE_FRACTION = 0.1 # Use 10% of the data for faster runs during development\n",
    "N_FOLDS_CV = 5         # Number of folds for general cross-validation\n",
    "N_FOLDS_TUNING = 3     # Number of folds for Optuna trials (can be smaller for speed)\n",
    "N_OPTUNA_TRIALS = 20   # Number of Optuna trials\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "try:\n",
    "    df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Training file not found at {TRAIN_FILE}\")\n",
    "    df_train_full = None\n",
    "except Exception as e:\n",
    "    print(f\"Error loading training data: {e}\")\n",
    "    df_train_full = None\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2 # Map to 0, 1, 2 for label_gain [0,1,5]\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation as df_train_full is None.\")\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "df_sample = None\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    if len(unique_srch_ids) > 0:\n",
    "        sampled_srch_ids_count = int(len(unique_srch_ids) * SAMPLE_FRACTION)\n",
    "        if sampled_srch_ids_count > 0:\n",
    "            sampled_srch_ids = np.random.choice(unique_srch_ids, size=sampled_srch_ids_count, replace=False)\n",
    "            df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "            print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "        else:\n",
    "            print(\"Sample fraction resulted in zero search IDs. Check SAMPLE_FRACTION or dataset size.\")\n",
    "            df_sample = df_train_full.copy() # Fallback to full if sample is too small\n",
    "            print(f\"Using full dataset instead. Shape: {df_sample.shape}\")\n",
    "    else:\n",
    "        print(\"No unique search IDs found in the training data.\")\n",
    "    del df_train_full # Optional: free up memory\n",
    "else:\n",
    "    print(\"Skipping sampling as df_train_full is None.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 10.0% of the data based on srch_id...\n",
      "Sampled data shape: (496233, 150)\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:03:44.692974Z",
     "start_time": "2025-05-18T21:03:44.500507Z"
    }
   },
   "source": [
    "df_sample"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0              1  2013-04-04 08:32:15       12                          187   \n",
       "1              1  2013-04-04 08:32:15       12                          187   \n",
       "2              1  2013-04-04 08:32:15       12                          187   \n",
       "3              1  2013-04-04 08:32:15       12                          187   \n",
       "4              1  2013-04-04 08:32:15       12                          187   \n",
       "...          ...                  ...      ...                          ...   \n",
       "4958284   332781  2013-03-03 15:12:59        5                          219   \n",
       "4958285   332781  2013-03-03 15:12:59        5                          219   \n",
       "4958286   332781  2013-03-03 15:12:59        5                          219   \n",
       "4958287   332781  2013-03-03 15:12:59        5                          219   \n",
       "4958288   332781  2013-03-03 15:12:59        5                          219   \n",
       "\n",
       "         visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  \\\n",
       "0                            0.0                   0.0              219   \n",
       "1                            0.0                   0.0              219   \n",
       "2                            0.0                   0.0              219   \n",
       "3                            0.0                   0.0              219   \n",
       "4                            0.0                   0.0              219   \n",
       "...                          ...                   ...              ...   \n",
       "4958284                      0.0                   0.0              219   \n",
       "4958285                      0.0                   0.0              219   \n",
       "4958286                      0.0                   0.0              219   \n",
       "4958287                      0.0                   0.0              219   \n",
       "4958288                      0.0                   0.0              219   \n",
       "\n",
       "         prop_id  prop_starrating  prop_review_score  ...  pca_component_11  \\\n",
       "0            893                3                3.5  ...          0.981932   \n",
       "1          10404                4                4.0  ...          0.988707   \n",
       "2          21315                3                4.5  ...          0.850245   \n",
       "3          27348                2                4.0  ...          0.310608   \n",
       "4          29604                4                3.5  ...          0.992220   \n",
       "...          ...              ...                ...  ...               ...   \n",
       "4958284   101770                2                2.5  ...         -1.045460   \n",
       "4958285   115831                2                3.0  ...         -0.951428   \n",
       "4958286   120379                2                4.5  ...         -0.381578   \n",
       "4958287   132031                3                3.0  ...         -0.546721   \n",
       "4958288   133044                2                2.0  ...         -1.055612   \n",
       "\n",
       "         pca_component_12  pca_component_13  pca_component_14  \\\n",
       "0               -0.516538         -1.190681         -1.023253   \n",
       "1               -0.425684         -1.321125         -1.039236   \n",
       "2               -0.955603         -0.539876         -0.977969   \n",
       "3               -2.978943          2.130284         -0.935462   \n",
       "4               -0.409590         -1.355921         -1.051223   \n",
       "...                   ...               ...               ...   \n",
       "4958284         -0.107420          0.072370          2.339506   \n",
       "4958285         -0.502378          0.505545          2.459365   \n",
       "4958286          0.634069         -0.552459          2.511653   \n",
       "4958287          1.071254         -1.382976          2.340486   \n",
       "4958288         -0.174314          0.164006          2.318485   \n",
       "\n",
       "         pca_component_15  pca_component_16  pca_component_17  \\\n",
       "0                0.182242          1.970431          7.004859   \n",
       "1               -0.338496          1.947880          7.236945   \n",
       "2                0.416192          2.070974          6.729867   \n",
       "3                3.075464          4.726963          4.654473   \n",
       "4               -0.059442          2.176942          7.228963   \n",
       "...                   ...               ...               ...   \n",
       "4958284          0.356839          1.133789          0.236386   \n",
       "4958285          0.630931         -0.582768         -0.137827   \n",
       "4958286         -0.651917         -1.308794          0.194617   \n",
       "4958287          0.169470         -0.973796          0.200937   \n",
       "4958288          0.843916          1.824108         -0.147485   \n",
       "\n",
       "         pca_component_18  pca_component_19  pca_component_20  \n",
       "0                5.290167          3.927173         -2.013700  \n",
       "1                5.279294          3.560058         -1.476805  \n",
       "2                5.022959          4.760550         -2.141155  \n",
       "3                6.934904          5.937608          0.821386  \n",
       "4                5.356038          3.300087         -1.728475  \n",
       "...                   ...               ...               ...  \n",
       "4958284         -0.097847         -0.967310         -0.802455  \n",
       "4958285          0.068793          0.329227         -0.510961  \n",
       "4958286         -0.747472          1.426646         -0.125837  \n",
       "4958287          0.455543         -0.453089          0.159024  \n",
       "4958288          0.737128         -1.219975          0.275987  \n",
       "\n",
       "[496233 rows x 150 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_component_11</th>\n",
       "      <th>pca_component_12</th>\n",
       "      <th>pca_component_13</th>\n",
       "      <th>pca_component_14</th>\n",
       "      <th>pca_component_15</th>\n",
       "      <th>pca_component_16</th>\n",
       "      <th>pca_component_17</th>\n",
       "      <th>pca_component_18</th>\n",
       "      <th>pca_component_19</th>\n",
       "      <th>pca_component_20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981932</td>\n",
       "      <td>-0.516538</td>\n",
       "      <td>-1.190681</td>\n",
       "      <td>-1.023253</td>\n",
       "      <td>0.182242</td>\n",
       "      <td>1.970431</td>\n",
       "      <td>7.004859</td>\n",
       "      <td>5.290167</td>\n",
       "      <td>3.927173</td>\n",
       "      <td>-2.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988707</td>\n",
       "      <td>-0.425684</td>\n",
       "      <td>-1.321125</td>\n",
       "      <td>-1.039236</td>\n",
       "      <td>-0.338496</td>\n",
       "      <td>1.947880</td>\n",
       "      <td>7.236945</td>\n",
       "      <td>5.279294</td>\n",
       "      <td>3.560058</td>\n",
       "      <td>-1.476805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.850245</td>\n",
       "      <td>-0.955603</td>\n",
       "      <td>-0.539876</td>\n",
       "      <td>-0.977969</td>\n",
       "      <td>0.416192</td>\n",
       "      <td>2.070974</td>\n",
       "      <td>6.729867</td>\n",
       "      <td>5.022959</td>\n",
       "      <td>4.760550</td>\n",
       "      <td>-2.141155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.310608</td>\n",
       "      <td>-2.978943</td>\n",
       "      <td>2.130284</td>\n",
       "      <td>-0.935462</td>\n",
       "      <td>3.075464</td>\n",
       "      <td>4.726963</td>\n",
       "      <td>4.654473</td>\n",
       "      <td>6.934904</td>\n",
       "      <td>5.937608</td>\n",
       "      <td>0.821386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.992220</td>\n",
       "      <td>-0.409590</td>\n",
       "      <td>-1.355921</td>\n",
       "      <td>-1.051223</td>\n",
       "      <td>-0.059442</td>\n",
       "      <td>2.176942</td>\n",
       "      <td>7.228963</td>\n",
       "      <td>5.356038</td>\n",
       "      <td>3.300087</td>\n",
       "      <td>-1.728475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958284</th>\n",
       "      <td>332781</td>\n",
       "      <td>2013-03-03 15:12:59</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>101770</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.045460</td>\n",
       "      <td>-0.107420</td>\n",
       "      <td>0.072370</td>\n",
       "      <td>2.339506</td>\n",
       "      <td>0.356839</td>\n",
       "      <td>1.133789</td>\n",
       "      <td>0.236386</td>\n",
       "      <td>-0.097847</td>\n",
       "      <td>-0.967310</td>\n",
       "      <td>-0.802455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958285</th>\n",
       "      <td>332781</td>\n",
       "      <td>2013-03-03 15:12:59</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>115831</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.951428</td>\n",
       "      <td>-0.502378</td>\n",
       "      <td>0.505545</td>\n",
       "      <td>2.459365</td>\n",
       "      <td>0.630931</td>\n",
       "      <td>-0.582768</td>\n",
       "      <td>-0.137827</td>\n",
       "      <td>0.068793</td>\n",
       "      <td>0.329227</td>\n",
       "      <td>-0.510961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958286</th>\n",
       "      <td>332781</td>\n",
       "      <td>2013-03-03 15:12:59</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>120379</td>\n",
       "      <td>2</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.381578</td>\n",
       "      <td>0.634069</td>\n",
       "      <td>-0.552459</td>\n",
       "      <td>2.511653</td>\n",
       "      <td>-0.651917</td>\n",
       "      <td>-1.308794</td>\n",
       "      <td>0.194617</td>\n",
       "      <td>-0.747472</td>\n",
       "      <td>1.426646</td>\n",
       "      <td>-0.125837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958287</th>\n",
       "      <td>332781</td>\n",
       "      <td>2013-03-03 15:12:59</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>132031</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.546721</td>\n",
       "      <td>1.071254</td>\n",
       "      <td>-1.382976</td>\n",
       "      <td>2.340486</td>\n",
       "      <td>0.169470</td>\n",
       "      <td>-0.973796</td>\n",
       "      <td>0.200937</td>\n",
       "      <td>0.455543</td>\n",
       "      <td>-0.453089</td>\n",
       "      <td>0.159024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958288</th>\n",
       "      <td>332781</td>\n",
       "      <td>2013-03-03 15:12:59</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219</td>\n",
       "      <td>133044</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.055612</td>\n",
       "      <td>-0.174314</td>\n",
       "      <td>0.164006</td>\n",
       "      <td>2.318485</td>\n",
       "      <td>0.843916</td>\n",
       "      <td>1.824108</td>\n",
       "      <td>-0.147485</td>\n",
       "      <td>0.737128</td>\n",
       "      <td>-1.219975</td>\n",
       "      <td>0.275987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>496233 rows × 150 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:05:59.779124Z",
     "start_time": "2025-05-18T21:05:59.463970Z"
    }
   },
   "source": "# --- 4. Initial Feature Selection & Preparation ---\nX = None\ny = None\ngroups_for_splitting = None # This will be df_sample['srch_id'] for GroupKFold.split\nfeature_columns = [] # Initialize\n\nif df_sample is not None:\n    print(\"\\nDefining initial feature set and preparing X, y, groups...\")\n    feature_columns = [\n        # Base features\n        'visitor_location_country_id', 'prop_country_id',\n        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n        'price_usd', 'promotion_flag', 'orig_destination_distance',\n        \n        # Property features\n        'hotel_quality', 'star_review_gap', 'combined_location_score', \n        'location_quality', 'distance_category', 'prop_popularity',\n        'prop_country_rank_pct',\n        \n        # Price features\n        'price_per_night', 'value_for_money_normalized', 'price_normalized',\n        'price_rank_pct', 'price_tier', 'has_promotion', 'price_discount',\n        \n        # Competitive features\n        'competitors_count', 'better_price_count', 'price_comp_ratio', 'comp_advantage',\n        \n        # User features\n        'is_domestic_search', 'total_travelers', 'is_family',\n        'is_short_stay', 'is_long_stay', 'is_last_minute', 'is_early_booking',\n        \n        # Time features\n        'is_weekend', 'is_holiday_season',\n        \n        # Interaction features\n        'user_prop_country_match', 'star_rating_for_price', 'review_for_price',\n        'location_for_price', 'quality_price_ratio'\n    ]\n\n    # Ensure all selected feature columns exist in df_sample\n    existing_feature_columns = [col for col in feature_columns if col in df_sample.columns]\n    if len(existing_feature_columns) != len(feature_columns):\n        print(f\"Warning: Some feature columns not found. Using: {existing_feature_columns}\")\n    feature_columns = existing_feature_columns\n\n    if not feature_columns:\n        print(\"Error: No feature columns selected or available. Stopping.\")\n    else:\n        X = df_sample[feature_columns].copy()\n        y = df_sample['relevance'].copy()\n        groups_for_splitting = df_sample['srch_id'] # Used by GroupKFold for splitting\n\n        # Basic Imputation (should ideally be done based on EDA insights and training set stats)\n        # This imputation is done on the *sampled* data (X).\n        # For test set imputation later, medians from this X will be used.\n        print(\"Performing basic median imputation for numerical features in X...\")\n        for col in X.columns:\n            if X[col].isnull().any():\n                if pd.api.types.is_numeric_dtype(X[col]):\n                    median_val = X[col].median()\n                    X[col].fillna(median_val, inplace=True)\n                    # print(f\"Imputed NaNs in '{col}' with median: {median_val}\")\n                # Add mode imputation for categorical if any, or use a placeholder string\n        \n        print(f\"Selected {len(feature_columns)} features: {feature_columns}\")\n        print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n        print(f\"NaNs remaining in X after imputation: {X.isnull().sum().sum()}\")\n        if groups_for_splitting is not None:\n            print(f\"Number of unique groups for splitting: {groups_for_splitting.nunique()}\")\nelse:\n    print(\"Skipping feature selection as df_sample is None.\")\n\n# Display X's head to verify\nif X is not None:\n    display(X.head())",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set and preparing X, y, groups...\n",
      "Performing basic median imputation for numerical features in X...\n",
      "Selected 43 features: ['visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'orig_destination_distance', 'hotel_quality', 'star_review_gap', 'combined_location_score', 'location_quality', 'distance_category', 'prop_popularity', 'prop_country_rank_pct', 'price_per_night', 'value_for_money_normalized', 'price_normalized', 'price_rank_pct', 'price_tier', 'has_promotion', 'price_discount', 'competitors_count', 'better_price_count', 'price_comp_ratio', 'comp_advantage', 'is_domestic_search', 'total_travelers', 'is_family', 'is_short_stay', 'is_long_stay', 'is_last_minute', 'is_early_booking', 'is_weekend', 'is_holiday_season', 'user_prop_country_match', 'star_rating_for_price', 'review_for_price', 'location_for_price', 'quality_price_ratio']\n",
      "Shape of X: (496233, 43), Shape of y: (496233,)\n",
      "NaNs remaining in X after imputation: 0\n",
      "Number of unique groups for splitting: 19979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   visitor_location_country_id  prop_country_id  prop_starrating  \\\n",
       "0                          187              219                3   \n",
       "1                          187              219                4   \n",
       "2                          187              219                3   \n",
       "3                          187              219                2   \n",
       "4                          187              219                4   \n",
       "\n",
       "   prop_review_score  prop_brand_bool  prop_location_score1  \\\n",
       "0                3.5                1                  2.83   \n",
       "1                4.0                1                  2.20   \n",
       "2                4.5                1                  2.20   \n",
       "3                4.0                1                  2.83   \n",
       "4                3.5                1                  2.64   \n",
       "\n",
       "   prop_location_score2  prop_log_historical_price  price_usd  promotion_flag  \\\n",
       "0                0.0438                       4.95     104.77               0   \n",
       "1                0.0149                       5.03     170.74               0   \n",
       "2                0.0245                       4.92     179.80               0   \n",
       "3                0.0125                       4.39     602.77               0   \n",
       "4                0.1241                       4.93     143.58               0   \n",
       "\n",
       "   ...  is_long_stay  is_last_minute  is_early_booking  is_weekend  \\\n",
       "0  ...             0               1                 0           0   \n",
       "1  ...             0               1                 0           0   \n",
       "2  ...             0               1                 0           0   \n",
       "3  ...             0               1                 0           0   \n",
       "4  ...             0               1                 0           0   \n",
       "\n",
       "   is_holiday_season  user_prop_country_match  star_rating_for_price  \\\n",
       "0                  0                        0               0.643602   \n",
       "1                  0                        0               0.777306   \n",
       "2                  0                        0               0.577213   \n",
       "3                  0                        0               0.312344   \n",
       "4                  0                        0               0.804209   \n",
       "\n",
       "   review_for_price  location_for_price  quality_price_ratio  \n",
       "0          0.750869            0.063706            17.335567  \n",
       "1          0.777306            0.043743             4.848542  \n",
       "2          0.865819            0.043864             4.109393  \n",
       "3          0.624688            0.044909             0.600000  \n",
       "4          0.703683            0.060715             6.666380  \n",
       "\n",
       "[5 rows x 43 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>prop_brand_bool</th>\n",
       "      <th>prop_location_score1</th>\n",
       "      <th>prop_location_score2</th>\n",
       "      <th>prop_log_historical_price</th>\n",
       "      <th>price_usd</th>\n",
       "      <th>promotion_flag</th>\n",
       "      <th>...</th>\n",
       "      <th>is_long_stay</th>\n",
       "      <th>is_last_minute</th>\n",
       "      <th>is_early_booking</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday_season</th>\n",
       "      <th>user_prop_country_match</th>\n",
       "      <th>star_rating_for_price</th>\n",
       "      <th>review_for_price</th>\n",
       "      <th>location_for_price</th>\n",
       "      <th>quality_price_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>4.95</td>\n",
       "      <td>104.77</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.643602</td>\n",
       "      <td>0.750869</td>\n",
       "      <td>0.063706</td>\n",
       "      <td>17.335567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>5.03</td>\n",
       "      <td>170.74</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.777306</td>\n",
       "      <td>0.777306</td>\n",
       "      <td>0.043743</td>\n",
       "      <td>4.848542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.20</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>4.92</td>\n",
       "      <td>179.80</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.577213</td>\n",
       "      <td>0.865819</td>\n",
       "      <td>0.043864</td>\n",
       "      <td>4.109393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.83</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>4.39</td>\n",
       "      <td>602.77</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.312344</td>\n",
       "      <td>0.624688</td>\n",
       "      <td>0.044909</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>187</td>\n",
       "      <td>219</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.64</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>4.93</td>\n",
       "      <td>143.58</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.804209</td>\n",
       "      <td>0.703683</td>\n",
       "      <td>0.060715</td>\n",
       "      <td>6.666380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:06:21.797160Z",
     "start_time": "2025-05-18T21:06:05.109055Z"
    }
   },
   "source": [
    "# --- 5. Cross-Validation (using modular function) ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*')\n",
    "mean_cv_ndcg = 0\n",
    "std_cv_ndcg = 0\n",
    "cv_feature_importances = pd.Series(dtype=float) # Initialize as an empty Series\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    # Basic LGBM params for initial CV\n",
    "    # These will be merged with/override defaults in the perform_cross_validation function\n",
    "    initial_lgbm_params = {\n",
    "        'n_estimators': 100, # Example: function's default might be different\n",
    "        'learning_rate': 0.1, # Example\n",
    "        'random_state': RANDOM_STATE\n",
    "        # The modular function defines other necessary defaults like objective, metric, label_gain, eval_at etc.\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n--- Performing {N_FOLDS_CV}-Fold Cross-Validation using modular function ---\")\n",
    "    mean_cv_ndcg, std_cv_ndcg, cv_feature_importances = lgbm_model.perform_cross_validation(\n",
    "        X, y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # Pass df_sample, as it contains 'srch_id' needed for group counts\n",
    "        n_folds=N_FOLDS_CV,\n",
    "        lgbm_params=initial_lgbm_params\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCross-Validation Mean NDCG@5: {mean_cv_ndcg:.4f} +/- {std_cv_ndcg:.4f}\")\n",
    "    if not cv_feature_importances.empty:\n",
    "        print(\"\\nAverage Feature Importances from CV:\")\n",
    "        with pd.option_context('display.max_rows', 30): # Display top 20 or all if less than 20\n",
    "            display(cv_feature_importances.head(20))\n",
    "else:\n",
    "    print(\"\\nSkipping Cross-Validation due to missing X, y, groups_for_splitting, or df_sample.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5-Fold Cross-Validation using modular function ---\n",
      "\\n--- Performing 5-Fold Cross-Validation ---\n",
      "--- Fold 1/5 ---\n",
      "Fold 1 NDCG@5: 0.3592\n",
      "--- Fold 2/5 ---\n",
      "Fold 2 NDCG@5: 0.3499\n",
      "--- Fold 3/5 ---\n",
      "Fold 3 NDCG@5: 0.3449\n",
      "--- Fold 4/5 ---\n",
      "Fold 4 NDCG@5: 0.3526\n",
      "--- Fold 5/5 ---\n",
      "Fold 5 NDCG@5: 0.3569\n",
      "Mean NDCG@5 across 5 folds: 0.3527 +/- 0.0051\n",
      "\n",
      "Cross-Validation Mean NDCG@5: 0.3527 +/- 0.0051\n",
      "\n",
      "Average Feature Importances from CV:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2         17166.577920\n",
       "price_discount                6278.243007\n",
       "price_rank_pct                5211.863006\n",
       "prop_location_score1          5116.179572\n",
       "prop_country_rank_pct         4820.261888\n",
       "star_rating_for_price         4236.158121\n",
       "quality_price_ratio           3092.458729\n",
       "promotion_flag                2815.662189\n",
       "prop_review_score             2704.097573\n",
       "price_usd                     2187.234822\n",
       "prop_popularity               1845.780967\n",
       "combined_location_score       1449.318458\n",
       "price_normalized              1437.484595\n",
       "review_for_price              1391.433720\n",
       "prop_log_historical_price     1164.376938\n",
       "location_for_price            1046.999037\n",
       "price_per_night                956.641599\n",
       "orig_destination_distance      922.797661\n",
       "comp_advantage                 475.334661\n",
       "prop_starrating                444.431342\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:09:19.740214Z",
     "start_time": "2025-05-18T21:06:21.811415Z"
    }
   },
   "source": [
    "# --- 6. Hyperparameter Tuning with Optuna (using modular function) ---\n",
    "best_params_from_tuning = {} # Initialize\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None:\n",
    "    print(f\"\\n--- Tuning Hyperparameters with Optuna ({N_OPTUNA_TRIALS} trials, {N_FOLDS_TUNING} CV folds each) using modular function ---\")\n",
    "    \n",
    "    # Suppress Optuna's verbosity if it's too much, and LightGBM warnings during tuning.\n",
    "    # import optuna # Optuna is imported within lgbm_model.py where tune_hyperparameters_optuna is defined.\n",
    "    # optuna.logging.set_verbosity(optuna.logging.WARNING) # You can set this in lgbm_model.py if desired globally for the function\n",
    "    \n",
    "    # It's good practice to manage warnings that might clutter the output during tuning.\n",
    "    # The lgbm_model.py file could also handle these internally if preferred.\n",
    "    warnings.filterwarnings('ignore', message='Found \\'eval_at\\' in params.*') # Suppress LightGBM's specific warning\n",
    "    warnings.filterwarnings('ignore', message='Overriding the init_model argument.*') # Another potential LightGBM warning\n",
    "\n",
    "    best_params_from_tuning = lgbm_model.tune_hyperparameters_optuna(\n",
    "        X, \n",
    "        y, \n",
    "        groups_for_splitting=groups_for_splitting, # This is df_sample['srch_id']\n",
    "        df_full_for_group_counts=df_sample, # df_sample for calculating group sizes within folds\n",
    "        n_trials=N_OPTUNA_TRIALS, \n",
    "        n_cv_folds=N_FOLDS_TUNING\n",
    "    )\n",
    "    \n",
    "    if best_params_from_tuning:\n",
    "        print(\"\\nBest parameters found by Optuna:\")\n",
    "        for key, value in best_params_from_tuning.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "    else:\n",
    "        print(\"\\nOptuna tuning did not return parameters. Using default parameters for the final model evaluation.\")\n",
    "        # Fallback to some sensible defaults if tuning fails or is skipped\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "            'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "            'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "            # Add other necessary LGBM parameters if not covered by the module's defaults\n",
    "        } \n",
    "else:\n",
    "    print(\"\\nSkipping Hyperparameter Tuning due to missing X, y, groups_for_splitting, or df_sample.\")\n",
    "    # Fallback parameters if tuning is skipped\n",
    "    best_params_from_tuning = { \n",
    "        'n_estimators': 200, 'learning_rate': 0.05, 'num_leaves': 31, \n",
    "        'max_depth': 7, 'min_child_samples': 20, 'subsample': 0.8,\n",
    "        'colsample_bytree':0.8, 'reg_alpha':0.1, 'reg_lambda':0.1\n",
    "    }\n",
    "\n",
    "# Reset warnings to default behavior if they were changed\n",
    "warnings.resetwarnings()"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 23:06:21,824] A new study created in memory with name: lgbm_ranker_tuning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) using modular function ---\n",
      "\\n--- Tuning Hyperparameters with Optuna (20 trials, 3 CV folds each) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 23:06:28,625] Trial 0 finished with value: 0.3485648708465034 and parameters: {'n_estimators': 600, 'learning_rate': 0.0383392091560084, 'num_leaves': 128, 'max_depth': 3, 'min_child_samples': 74, 'subsample': 0.7453794190423586, 'colsample_bytree': 0.8731285867855868, 'reg_alpha': 0.09468407076718369, 'reg_lambda': 1.1011701308126212}. Best is trial 0 with value: 0.3485648708465034.\n",
      "[I 2025-05-18 23:06:35,843] Trial 1 finished with value: 0.3473504476712286 and parameters: {'n_estimators': 100, 'learning_rate': 0.022596638359231994, 'num_leaves': 92, 'max_depth': 3, 'min_child_samples': 5, 'subsample': 0.9691376826312474, 'colsample_bytree': 0.8593038865950324, 'reg_alpha': 1.5904990786068438, 'reg_lambda': 3.075669681051407}. Best is trial 0 with value: 0.3485648708465034.\n",
      "[I 2025-05-18 23:06:43,393] Trial 2 finished with value: 0.3542603381842788 and parameters: {'n_estimators': 600, 'learning_rate': 0.09678351755889147, 'num_leaves': 61, 'max_depth': 4, 'min_child_samples': 82, 'subsample': 0.9300089804461755, 'colsample_bytree': 0.9366796867622699, 'reg_alpha': 0.008756302386909708, 'reg_lambda': 0.0034301129197774425}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:06:54,349] Trial 3 finished with value: 0.3521266682602302 and parameters: {'n_estimators': 400, 'learning_rate': 0.014758488034949837, 'num_leaves': 117, 'max_depth': 5, 'min_child_samples': 52, 'subsample': 0.9204123809086626, 'colsample_bytree': 0.5542400713002671, 'reg_alpha': 0.001989321804593724, 'reg_lambda': 2.654280818683127}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:04,042] Trial 4 finished with value: 0.3434442169583957 and parameters: {'n_estimators': 100, 'learning_rate': 0.12980657601434678, 'num_leaves': 145, 'max_depth': 7, 'min_child_samples': 53, 'subsample': 0.8727336376895038, 'colsample_bytree': 0.5197635484873147, 'reg_alpha': 0.22417079705300533, 'reg_lambda': 1.1437767074774647}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:11,842] Trial 5 finished with value: 0.35114986754374206 and parameters: {'n_estimators': 550, 'learning_rate': 0.03088589116582221, 'num_leaves': 102, 'max_depth': 3, 'min_child_samples': 10, 'subsample': 0.7425387856648031, 'colsample_bytree': 0.8788845210294352, 'reg_alpha': 0.004335005325532373, 'reg_lambda': 0.1768871772536228}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:20,319] Trial 6 finished with value: 0.3514184748229698 and parameters: {'n_estimators': 100, 'learning_rate': 0.017328295004796997, 'num_leaves': 126, 'max_depth': 5, 'min_child_samples': 9, 'subsample': 0.6349500312993641, 'colsample_bytree': 0.6008182896096468, 'reg_alpha': 0.997050860300588, 'reg_lambda': 0.36296271646306016}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:29,175] Trial 7 finished with value: 0.3440698929538953 and parameters: {'n_estimators': 500, 'learning_rate': 0.17000205446811173, 'num_leaves': 65, 'max_depth': 10, 'min_child_samples': 99, 'subsample': 0.7469664282081283, 'colsample_bytree': 0.7217919827062349, 'reg_alpha': 1.5948918139519541, 'reg_lambda': 0.15388310064989158}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:42,537] Trial 8 finished with value: 0.3494021296765673 and parameters: {'n_estimators': 550, 'learning_rate': 0.029578564816509306, 'num_leaves': 82, 'max_depth': 10, 'min_child_samples': 90, 'subsample': 0.6351946759941051, 'colsample_bytree': 0.8168050926097707, 'reg_alpha': 3.998378347301571, 'reg_lambda': 0.16514618257430383}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:07:52,101] Trial 9 finished with value: 0.34519026812976045 and parameters: {'n_estimators': 450, 'learning_rate': 0.08689913321033622, 'num_leaves': 85, 'max_depth': 10, 'min_child_samples': 92, 'subsample': 0.8374547730695926, 'colsample_bytree': 0.7493952634760535, 'reg_alpha': 0.5539907355039416, 'reg_lambda': 0.0012616378946981007}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:00,246] Trial 10 finished with value: 0.3499452585391427 and parameters: {'n_estimators': 700, 'learning_rate': 0.06758271574494057, 'num_leaves': 26, 'max_depth': 12, 'min_child_samples': 64, 'subsample': 0.5281144420661407, 'colsample_bytree': 0.9480533190323863, 'reg_alpha': 0.017408236385991243, 'reg_lambda': 0.006245800952241776}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:08,750] Trial 11 finished with value: 0.34760618632740353 and parameters: {'n_estimators': 300, 'learning_rate': 0.010763767347588533, 'num_leaves': 49, 'max_depth': 6, 'min_child_samples': 34, 'subsample': 0.9894306282479162, 'colsample_bytree': 0.9910674325923012, 'reg_alpha': 0.0012154829225186744, 'reg_lambda': 0.018964758674518342}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:16,942] Trial 12 finished with value: 0.3517185699799866 and parameters: {'n_estimators': 350, 'learning_rate': 0.06256149900029144, 'num_leaves': 54, 'max_depth': 5, 'min_child_samples': 38, 'subsample': 0.9007041061540129, 'colsample_bytree': 0.6518163283071841, 'reg_alpha': 0.01572111887648923, 'reg_lambda': 0.021390251270054575}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:26,129] Trial 13 finished with value: 0.3532848437018683 and parameters: {'n_estimators': 250, 'learning_rate': 0.01261188564177991, 'num_leaves': 109, 'max_depth': 5, 'min_child_samples': 76, 'subsample': 0.9095589384375455, 'colsample_bytree': 0.5190049000563296, 'reg_alpha': 0.0010184384563919366, 'reg_lambda': 6.402817287742677}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:36,078] Trial 14 finished with value: 0.35347614285607487 and parameters: {'n_estimators': 250, 'learning_rate': 0.09010686306044108, 'num_leaves': 28, 'max_depth': 8, 'min_child_samples': 76, 'subsample': 0.813677091089686, 'colsample_bytree': 0.6754741183504521, 'reg_alpha': 0.007725982554573526, 'reg_lambda': 8.51462883408515}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:44,382] Trial 15 finished with value: 0.3523269050526972 and parameters: {'n_estimators': 200, 'learning_rate': 0.1151919689515183, 'num_leaves': 21, 'max_depth': 8, 'min_child_samples': 78, 'subsample': 0.8085814066277577, 'colsample_bytree': 0.6760845258947901, 'reg_alpha': 0.02295689198858424, 'reg_lambda': 0.0017153281391678557}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:08:51,340] Trial 16 finished with value: 0.3454195474051042 and parameters: {'n_estimators': 700, 'learning_rate': 0.19223389017436432, 'num_leaves': 44, 'max_depth': 8, 'min_child_samples': 63, 'subsample': 0.8143691172029116, 'colsample_bytree': 0.7925798501276319, 'reg_alpha': 0.0065981566021014616, 'reg_lambda': 0.02506084172311138}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:09:00,742] Trial 17 finished with value: 0.35408062803667245 and parameters: {'n_estimators': 200, 'learning_rate': 0.056852913174492654, 'num_leaves': 38, 'max_depth': 9, 'min_child_samples': 86, 'subsample': 0.9570680062646888, 'colsample_bytree': 0.6510444712345052, 'reg_alpha': 0.05903250618695891, 'reg_lambda': 0.0039256206368777115}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:09:11,614] Trial 18 finished with value: 0.34823161906921235 and parameters: {'n_estimators': 650, 'learning_rate': 0.05008234933869618, 'num_leaves': 66, 'max_depth': 12, 'min_child_samples': 100, 'subsample': 0.9574795019907139, 'colsample_bytree': 0.6096344914158747, 'reg_alpha': 0.05938278703654381, 'reg_lambda': 0.004805275846672218}. Best is trial 2 with value: 0.3542603381842788.\n",
      "[I 2025-05-18 23:09:19,736] Trial 19 finished with value: 0.35110214971262743 and parameters: {'n_estimators': 400, 'learning_rate': 0.05318347222265874, 'num_leaves': 39, 'max_depth': 9, 'min_child_samples': 85, 'subsample': 0.9917737233487345, 'colsample_bytree': 0.9278672068782191, 'reg_alpha': 0.05200397883856865, 'reg_lambda': 0.005304567666755759}. Best is trial 2 with value: 0.3542603381842788.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study finished. Best trial NDCG@5: 0.3543\n",
      "Best parameters: {'n_estimators': 600, 'learning_rate': 0.09678351755889147, 'num_leaves': 61, 'max_depth': 4, 'min_child_samples': 82, 'subsample': 0.9300089804461755, 'colsample_bytree': 0.9366796867622699, 'reg_alpha': 0.008756302386909708, 'reg_lambda': 0.0034301129197774425}\n",
      "\n",
      "Best parameters found by Optuna:\n",
      "    n_estimators: 600\n",
      "    learning_rate: 0.09678351755889147\n",
      "    num_leaves: 61\n",
      "    max_depth: 4\n",
      "    min_child_samples: 82\n",
      "    subsample: 0.9300089804461755\n",
      "    colsample_bytree: 0.9366796867622699\n",
      "    reg_alpha: 0.008756302386909708\n",
      "    reg_lambda: 0.0034301129197774425\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:09:23.276313Z",
     "start_time": "2025-05-18T21:09:19.817978Z"
    }
   },
   "source": [
    "# --- 7. Train Final Model on Full Sampled Data (using modular function) ---\n",
    "final_trained_model = None\n",
    "\n",
    "if X is not None and y is not None and groups_for_splitting is not None and df_sample is not None and best_params_from_tuning:\n",
    "    print(\"\\\\n--- Training Final Model with Best Tuned Parameters ---\")\n",
    "    \n",
    "    # groups_train_full is needed for the lgbm_model.train_final_model function\n",
    "    # It should represent the group sizes for the entire X, y that's being passed\n",
    "    # This X is df_sample[feature_columns]\n",
    "    groups_train_full = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "    if len(groups_train_full) > 0:\n",
    "        final_trained_model = lgbm_model.train_final_model(\n",
    "            X_train_full=X,  # This is the full X from df_sample\n",
    "            y_train_full=y,  # This is the full y from df_sample\n",
    "            groups_train_full=groups_train_full,\n",
    "            df_full_for_group_counts=df_sample, # df_sample contains 'srch_id' for early stopping split\n",
    "            best_params=best_params_from_tuning\n",
    "        )\n",
    "        if final_trained_model:\n",
    "            print(\"Final model successfully trained.\")\n",
    "        else:\n",
    "            print(\"Final model training failed or returned None.\")\n",
    "    else:\n",
    "        print(\"Cannot train final model: No groups found in the training data.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping final model training due to missing data, groups, or best_params_from_tuning.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Training Final Model with Best Tuned Parameters ---\n",
      "\\n--- Training Final Model ---\n",
      "Final model parameters for training:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'verbosity': -1, 'n_estimators': 600, 'learning_rate': 0.09678351755889147, 'num_leaves': 61, 'max_depth': 4, 'min_child_samples': 82, 'subsample': 0.9300089804461755, 'colsample_bytree': 0.9366796867622699, 'reg_alpha': 0.008756302386909708, 'reg_lambda': 0.0034301129197774425}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiddemakimei/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model with early stopping on 90/10 split of training data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[69]\tvalid_0's ndcg@5: 0.364245\n",
      "Final model training completed.\n",
      "Final model successfully trained.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T22:08:24.370106Z",
     "start_time": "2025-05-18T21:56:46.641064Z"
    }
   },
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission (using modular function) ---\n",
    "\n",
    "if final_trained_model is not None and X is not None and 'feature_columns' in locals() and feature_columns:\n",
    "    print(\"\\\\n--- Preparing Test Data and Generating Kaggle Submission ---\")\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    print(f\"Loading test data from: {TEST_FILE}...\")\n",
    "    try:\n",
    "        df_test_raw = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test_raw.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Test file not found at {TEST_FILE}\")\n",
    "        df_test_raw = None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading test data: {e}\")\n",
    "        df_test_raw = None\n",
    "\n",
    "    if df_test_raw is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\\\nPreprocessing test data...\")\n",
    "        \n",
    "        # Ensure all selected feature columns exist in df_test_raw\n",
    "        # and handle any missing columns gracefully if necessary (e.g. by creating them with NaNs)\n",
    "        X_test_list = []\n",
    "        for col in feature_columns:\n",
    "            if col not in df_test_raw.columns:\n",
    "                print(f\"Warning: Feature column '{col}' not found in test data. Creating it with NaNs.\")\n",
    "                df_test_raw[col] = np.nan \n",
    "        \n",
    "        X_test = df_test_raw[feature_columns].copy()\n",
    "\n",
    "        # Impute missing values in X_test using medians from the TRAINING sample (X)\n",
    "        # X should be the dataframe of features used for training the final_trained_model\n",
    "        print(\"Imputing missing values in test data using training set medians...\")\n",
    "        nan_counts_before_imputation = X_test.isnull().sum()\n",
    "\n",
    "        for col in X_test.columns:\n",
    "            if X_test[col].isnull().any():\n",
    "                if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                    if col in X.columns: # Ensure the column exists in the training features X\n",
    "                        train_median = X[col].median() # Calculate median from the TRAIN features (X)\n",
    "                        X_test[col].fillna(train_median, inplace=True)\n",
    "                        # print(f\"Imputed NaNs in test column '{col}' with training median: {train_median}\")\n",
    "                    else:\n",
    "                        print(f\"Warning: Column '{col}' for median imputation not found in training X. Test NaNs may remain.\")\n",
    "                # else: # For categorical, use mode from training X\n",
    "                    # if col in X.columns:\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col].fillna(train_mode, inplace=True)\n",
    "                    # else:\n",
    "                    #     print(f\"Warning: Column '{col}' for mode imputation not found in training X. Test NaNs may remain.\")\n",
    "        \n",
    "        nan_counts_after_imputation = X_test.isnull().sum().sum()\n",
    "        print(f\"NaNs remaining in X_test after imputation: {nan_counts_after_imputation}\")\n",
    "        if nan_counts_after_imputation > 0:\n",
    "            print(\"Warning: Some NaNs remain in test features after imputation. Review missing columns or imputation logic.\")\n",
    "            print(X_test.isnull().sum()[X_test.isnull().sum() > 0])\n",
    "\n",
    "\n",
    "        # --- 8c. Generate Submission File ---\n",
    "        # The df_test_raw contains 'srch_id' and 'prop_id' needed by the submission function\n",
    "        lgbm_model.predict_and_format_submission(\n",
    "            model=final_trained_model,\n",
    "            X_test=X_test,\n",
    "            df_test_original_ids=df_test_raw, # Pass the raw test df for srch_id and prop_id\n",
    "            submission_filename=SUBMISSION_FILENAME\n",
    "        )\n",
    "    else:\n",
    "        print(\"Skipping submission generation as test data could not be loaded.\")\n",
    "else:\n",
    "    print(\"\\\\nSkipping Kaggle submission: final_trained_model, X, or feature_columns not available.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n--- Preparing Test Data and Generating Kaggle Submission ---\n",
      "Loading test data from: ../data/test_set_VU_DM_feature_engin.csv...\n",
      "Loaded test dataset with shape: (4959183, 146)\n",
      "\\nPreprocessing test data...\n",
      "Imputing missing values in test data using training set medians...\n",
      "NaNs remaining in X_test after imputation: 0\n",
      "\\n--- Predicting on Test Data and Formatting Submission ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file 'submission_modular.csv' created. Top 5 rows:\n",
      "   srch_id  prop_id\n",
      "0        1    99484\n",
      "1        1    61934\n",
      "2        1    54937\n",
      "3        1    24194\n",
      "4        1    28181\n"
     ]
    }
   ],
   "execution_count": 33
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
