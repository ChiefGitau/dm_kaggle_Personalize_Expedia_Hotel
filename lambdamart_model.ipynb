{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 4.0% of the data based on srch_id...\n",
      "Sampled data shape: (198006, 55)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "LambdaMART Model for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = '/Users/candedekoy/Desktop/Data Mining/project2/data.nosync'\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, 'train.csv')\n",
    "\n",
    "SAMPLE_FRACTION = 0.04 # Use 4% of the data\n",
    "N_FOLDS = 5 # For GroupKFold cross-validation\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    # 5 for booking, 1 for click (and not booked), 0 otherwise\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation due to data loading issues.\")\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    sampled_srch_ids = np.random.choice(unique_srch_ids, size=int(len(unique_srch_ids) * SAMPLE_FRACTION), replace=False)\n",
    "\n",
    "    df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "    print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "    # Free up memory from the full dataframe if no longer needed for this notebook scope\n",
    "    # del df_train_full \n",
    "else:\n",
    "    print(\"Skipping sampling due to data loading issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set...\n",
      "Selected 19 features.\n",
      "Feature columns: ['site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'orig_destination_distance']\n",
      "Shape of X: (198006, 19)\n",
      "Shape of y: (198006,)\n",
      "Number of groups: 7991, Min group size: 5, Max group size: 37\n",
      "\n",
      "Handling missing values (simple median imputation for numerical)...\n",
      "Imputed missing values in prop_review_score with median.\n",
      "Imputed missing values in prop_location_score2 with median.\n",
      "Imputed missing values in orig_destination_distance with median.\n",
      "NaNs remaining in X after imputation: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b5/ww14kyzx7zq1bdp4kybt0_800000gn/T/ipykernel_4968/2084995273.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[col] = X[col].fillna(X[col].median())\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- 4. Initial Feature Selection (Placeholder) ---\n",
    "# This will be refined by your friend. For now, using a subset of potentially useful features.\n",
    "if 'df_sample' in locals() and df_sample is not None:\n",
    "    print(\"\\nDefining initial feature set...\")\n",
    "    # Features identified as potentially important or commonly used, excluding IDs and target-leaking columns\n",
    "    # Also excluding competitor columns with high missing rates for now, and user history due to high missingness\n",
    "    feature_columns = [\n",
    "        'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
    "        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "        'price_usd', 'promotion_flag', 'srch_destination_id',\n",
    "        'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count',\n",
    "        'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool',\n",
    "        'orig_destination_distance'\n",
    "        # 'srch_query_affinity_score' # high missingness\n",
    "        # Add more features here as EDA suggests and after handling missing values\n",
    "    ]\n",
    "\n",
    "    # For ranking, we need features (X), relevance (y), and group/query_id\n",
    "    X = df_sample[feature_columns]\n",
    "    y = df_sample['relevance']\n",
    "    groups = df_sample.groupby('srch_id').size().to_numpy() # Size of each group\n",
    "\n",
    "    print(f\"Selected {len(feature_columns)} features.\")\n",
    "    print(\"Feature columns:\", feature_columns)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    print(f\"Number of groups: {len(groups)}, Min group size: {groups.min()}, Max group size: {groups.max()}\")\n",
    "\n",
    "    # Handle Missing Values (Simple Imputation for now)\n",
    "    # For a proper model, more sophisticated imputation or feature engineering for missingness is needed.\n",
    "    print(\"\\nHandling missing values (simple median imputation for numerical)...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "                print(f\"Imputed missing values in {col} with median.\")\n",
    "            # else: # For categorical, fill with mode or a specific placeholder\n",
    "            #     X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "    # Check if any NaNs remain (should ideally be none for numeric after this)\n",
    "    print(\"NaNs remaining in X after imputation:\", X.isnull().sum().sum())\n",
    "\n",
    "else:\n",
    "    print(\"Skipping feature selection due to data sampling issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Cross-Validation with GroupKFold ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "X_train shape: (158405, 19), y_train shape: (158405,), Num train groups: 6393\n",
      "X_val shape: (39601, 19), y_val shape: (39601,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009782 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2075\n",
      "[LightGBM] [Info] Number of data points in the train set: 158405, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's ndcg@5: 0.356316\n",
      "Fold 1 NDCG@5: 0.3491\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "X_train shape: (158401, 19), y_train shape: (158401,), Num train groups: 6392\n",
      "X_val shape: (39605, 19), y_val shape: (39605,), Num val groups: 1599\n",
      "Training LGBMRanker for fold 2...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005994 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2069\n",
      "[LightGBM] [Info] Number of data points in the train set: 158401, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's ndcg@5: 0.345563\n",
      "Fold 2 NDCG@5: 0.3436\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 3...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005315 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2082\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's ndcg@5: 0.348846\n",
      "Fold 3 NDCG@5: 0.3432\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 4...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006088 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2076\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.343837\n",
      "Fold 4 NDCG@5: 0.3416\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 5...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2059\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's ndcg@5: 0.349061\n",
      "Fold 5 NDCG@5: 0.3475\n",
      "\n",
      "Mean NDCG@5 across 5 successfully evaluated folds: 0.3450 +/- 0.0028\n",
      "\n",
      "Average Feature Importances (Cross-Validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           8347.416615\n",
       "price_usd                      4193.268586\n",
       "prop_location_score1           2448.127400\n",
       "prop_log_historical_price      1740.814016\n",
       "prop_starrating                1582.416622\n",
       "prop_review_score               885.303738\n",
       "promotion_flag                  798.520281\n",
       "srch_booking_window             603.512759\n",
       "orig_destination_distance       572.761840\n",
       "srch_destination_id             543.519841\n",
       "prop_country_id                 342.729661\n",
       "visitor_location_country_id     240.173560\n",
       "prop_brand_bool                 198.828362\n",
       "srch_length_of_stay             181.570140\n",
       "site_id                         154.760380\n",
       "srch_adults_count                76.595641\n",
       "srch_children_count              46.886420\n",
       "srch_room_count                  24.002240\n",
       "srch_saturday_night_bool         13.817100\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Hyperparameter Tuning with RandomizedSearchCV ---\n",
      "Starting RandomizedSearchCV for hyperparameter tuning...\n",
      "Error during RandomizedSearchCV: If no scoring is specified, the estimator passed should have a 'score' method. The estimator LGBMRanker(eval_at=[5], importance_type='gain', label_gain=[0, 1, 5],\n",
      "           metric='ndcg', n_jobs=-1, objective='lambdarank', random_state=42) does not.\n",
      "Falling back to default parameters for the final model evaluation.\n",
      "\n",
      "--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\n",
      "\n",
      "Final model parameters for evaluation:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "\n",
      "--- Final Model Evaluation: Fold 1/5 ---\n",
      "X_train shape: (158405, 19), y_train shape: (158405,), Num train groups: 6393\n",
      "X_val shape: (39601, 19), y_val shape: (39601,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 1...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006493 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2075\n",
      "[LightGBM] [Info] Number of data points in the train set: 158405, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[28]\tvalid_0's ndcg@5: 0.356316\n",
      "Fold 1 (Tuned Model) NDCG@5: 0.3491\n",
      "\n",
      "--- Final Model Evaluation: Fold 2/5 ---\n",
      "X_train shape: (158401, 19), y_train shape: (158401,), Num train groups: 6392\n",
      "X_val shape: (39605, 19), y_val shape: (39605,), Num val groups: 1599\n",
      "Training final tuned LGBMRanker for fold 2...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004624 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2069\n",
      "[LightGBM] [Info] Number of data points in the train set: 158401, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[7]\tvalid_0's ndcg@5: 0.345563\n",
      "Fold 2 (Tuned Model) NDCG@5: 0.3436\n",
      "\n",
      "--- Final Model Evaluation: Fold 3/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 3...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005362 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2082\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[17]\tvalid_0's ndcg@5: 0.348846\n",
      "Fold 3 (Tuned Model) NDCG@5: 0.3432\n",
      "\n",
      "--- Final Model Evaluation: Fold 4/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 4...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004711 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2076\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.343837\n",
      "Fold 4 (Tuned Model) NDCG@5: 0.3416\n",
      "\n",
      "--- Final Model Evaluation: Fold 5/5 ---\n",
      "X_train shape: (158406, 19), y_train shape: (158406,), Num train groups: 6393\n",
      "X_val shape: (39600, 19), y_val shape: (39600,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 5...\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.004215 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2059\n",
      "[LightGBM] [Info] Number of data points in the train set: 158406, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping, best iteration is:\n",
      "[34]\tvalid_0's ndcg@5: 0.349061\n",
      "Fold 5 (Tuned Model) NDCG@5: 0.3475\n",
      "\n",
      "Mean NDCG@5 for Tuned Model across 5 successfully evaluated folds: 0.3450 +/- 0.0028\n",
      "\n",
      "Average Feature Importances (Tuned Model):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           8347.416615\n",
       "price_usd                      4193.268586\n",
       "prop_location_score1           2448.127400\n",
       "prop_log_historical_price      1740.814016\n",
       "prop_starrating                1582.416622\n",
       "prop_review_score               885.303738\n",
       "promotion_flag                  798.520281\n",
       "srch_booking_window             603.512759\n",
       "orig_destination_distance       572.761840\n",
       "srch_destination_id             543.519841\n",
       "prop_country_id                 342.729661\n",
       "visitor_location_country_id     240.173560\n",
       "prop_brand_bool                 198.828362\n",
       "srch_length_of_stay             181.570140\n",
       "site_id                         154.760380\n",
       "srch_adults_count                76.595641\n",
       "srch_children_count              46.886420\n",
       "srch_room_count                  24.002240\n",
       "srch_saturday_night_bool         13.817100\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- 5. Model Training (LGBMRanker) ---\n",
    "# This section will contain the training loop using GroupKFold\n",
    "\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None: # Use df_sample for srch_id\n",
    "    print(\"\\n--- 5. Cross-Validation with GroupKFold ---\")\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    \n",
    "    fold_ndcg_scores = []\n",
    "    all_feature_importances = pd.DataFrame()\n",
    "\n",
    "    # The groups parameter for gkf.split should be the srch_id for each row in X\n",
    "    # It ensures that rows with the same srch_id are not split across train/test in a fold.\n",
    "    unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Group counts for the current training and validation fold\n",
    "        # Need to use the original df_sample with srch_id to correctly form groups for the subsets\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "        \n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "        \n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "\n",
    "        ranker_cv = lgb.LGBMRanker(\n",
    "            objective='lambdarank',\n",
    "            metric='ndcg',\n",
    "            label_gain=[0, 1, 5], # Corresponds to relevance 0, 1, 5\n",
    "            eval_at=[5], # For NDCG@5\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1,\n",
    "            importance_type='gain',\n",
    "            random_state=RANDOM_STATE + fold, \n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        print(f\"Training LGBMRanker for fold {fold+1}...\")\n",
    "        ranker_cv.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg', \n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if ranker_cv.evals_result_ and 'valid_0' in ranker_cv.evals_result_ and 'ndcg@5' in ranker_cv.evals_result_['valid_0']:\n",
    "            ndcg_at_5 = ranker_cv.evals_result_['valid_0']['ndcg@5'][-1] \n",
    "            fold_ndcg_scores.append(ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} NDCG@5: {ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': ranker_cv.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            all_feature_importances = pd.concat([all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1}. Skipping score for this fold.\")\n",
    "\n",
    "\n",
    "    if fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 across {len(fold_ndcg_scores)} successfully evaluated folds: {np.mean(fold_ndcg_scores):.4f} +/- {np.std(fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not all_feature_importances.empty:\n",
    "            mean_feature_importances = all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Cross-Validation):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed with NDCG scores.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping model training and cross-validation due to earlier data processing issues.\")\n",
    "\n",
    "\n",
    "# --- 6. Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None:\n",
    "    print(\"\\n--- 6. Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint as sp_randint\n",
    "    from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': sp_randint(100, 500),\n",
    "        'learning_rate': sp_uniform(0.01, 0.19), # Upper bound <0.2 for uniform\n",
    "        'num_leaves': sp_randint(20, 100),      \n",
    "        'max_depth': sp_randint(3, 12),        \n",
    "        'min_child_samples': sp_randint(5, 50), \n",
    "        'subsample': sp_uniform(0.6, 0.4),      # Sum of loc + scale should be <= 1.0. Here, 0.6 + 0.4 = 1.0\n",
    "        'colsample_bytree': sp_uniform(0.6, 0.4), \n",
    "        'reg_alpha': sp_uniform(0, 1),          \n",
    "        'reg_lambda': sp_uniform(0, 1),         \n",
    "    }\n",
    "\n",
    "    base_ranker = lgb.LGBMRanker(\n",
    "        objective='lambdarank',\n",
    "        metric='ndcg', # LightGBM will use this for its internal evaluation\n",
    "        label_gain=[0, 1, 5],\n",
    "        eval_at=[5],\n",
    "        importance_type='gain',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1 # Be cautious with -1 for n_jobs in RandomizedSearchCV if memory is an issue\n",
    "    )\n",
    "\n",
    "    gkf_for_tuning = GroupKFold(n_splits=3) # Using 3 splits for tuning to speed it up\n",
    "\n",
    "    # RandomizedSearchCV setup\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_ranker,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,  # Number of parameter settings sampled. Increase for more thorough search.\n",
    "                    # Set to a small number like 5-10 for quick test, 25-50 for better search.\n",
    "        cv=list(gkf_for_tuning.split(X, y, groups=df_sample['srch_id'])), # Pass the list of splits\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=1, # Start with 1 to avoid potential memory issues, then try increasing.\n",
    "        verbose=2,\n",
    "        # scoring: If None, estimator's score method is used. LGBMRanker's score method should work.\n",
    "        # It calculates NDCG@eval_at based on its parameters.\n",
    "        refit=True # Refits the best estimator on the whole dataset (X,y) passed to fit.\n",
    "                   # For ranking, this full dataset refit will also need group info.\n",
    "    )\n",
    "\n",
    "    print(\"Starting RandomizedSearchCV for hyperparameter tuning...\")\n",
    "    # Pass `groups` to `fit`. This will be used by GroupKFold inside RandomizedSearchCV.\n",
    "    # And `LGBMRanker.fit` will also receive this `groups` argument for each fold.\n",
    "    \n",
    "    # For early stopping inside RandomizedSearchCV, you'd typically pass fit_params.\n",
    "    # This is more complex because eval_set/eval_group change per fold.\n",
    "    # For now, n_estimators is part of the search space.\n",
    "    \n",
    "    best_params_from_tuning = {}\n",
    "    try:\n",
    "        # RandomizedSearchCV will use the `groups` for splitting via the `cv` object\n",
    "        # And `LGBMRanker.fit` will receive the `group` parameter for each fold.\n",
    "        random_search.fit(X, y, groups=df_sample['srch_id']) \n",
    "        \n",
    "        print(\"\\nBest parameters found by RandomizedSearchCV:\")\n",
    "        print(random_search.best_params_)\n",
    "        # The best_score_ will be based on the internal scoring of LGBMRanker (NDCG@5 here)\n",
    "        print(f\"Best score from RandomizedSearchCV (NDCG@5): {random_search.best_score_:.4f}\")\n",
    "        best_params_from_tuning = random_search.best_params_\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RandomizedSearchCV: {e}\")\n",
    "        print(\"Falling back to default parameters for the final model evaluation.\")\n",
    "        # Re-initialize with default in case of error\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "            'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "            'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "        }\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping hyperparameter tuning due to earlier data processing issues.\")\n",
    "    best_params_from_tuning = {\n",
    "        'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "        'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "        'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 7. Detailed Evaluation of Best Tuned Model ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None and best_params_from_tuning:\n",
    "    print(\"\\n--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\")\n",
    "\n",
    "    final_gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    final_fold_ndcg_scores = []\n",
    "    final_all_feature_importances = pd.DataFrame()\n",
    "    \n",
    "    # Ensure unique_group_ids_for_splitting is available\n",
    "    if 'unique_group_ids_for_splitting' not in locals():\n",
    "        unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "\n",
    "    final_ranker_params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'label_gain': [0, 1, 5],\n",
    "        'eval_at': [5],\n",
    "        'importance_type': 'gain',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    # Update with tuned parameters, ensuring they are valid\n",
    "    for key, value in best_params_from_tuning.items():\n",
    "        final_ranker_params[key] = value\n",
    "    \n",
    "    # Ensure n_estimators is present if not tuned or set to a low value by tuning\n",
    "    if 'n_estimators' not in final_ranker_params or final_ranker_params['n_estimators'] < 50:\n",
    "         final_ranker_params['n_estimators'] = 300 # Default if not well-tuned by a short search\n",
    "\n",
    "    print(\"\\nFinal model parameters for evaluation:\")\n",
    "    print(final_ranker_params)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(final_gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Final Model Evaluation: Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "\n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "            \n",
    "        final_ranker = lgb.LGBMRanker(**final_ranker_params)\n",
    "\n",
    "        print(f\"Training final tuned LGBMRanker for fold {fold+1}...\")\n",
    "        final_ranker.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg',\n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if final_ranker.evals_result_ and 'valid_0' in final_ranker.evals_result_ and 'ndcg@5' in final_ranker.evals_result_['valid_0']:\n",
    "            final_ndcg_at_5 = final_ranker.evals_result_['valid_0']['ndcg@5'][-1]\n",
    "            final_fold_ndcg_scores.append(final_ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} (Tuned Model) NDCG@5: {final_ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': final_ranker.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            final_all_feature_importances = pd.concat([final_all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1} of the tuned model.\")\n",
    "\n",
    "\n",
    "    if final_fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 for Tuned Model across {len(final_fold_ndcg_scores)} successfully evaluated folds: {np.mean(final_fold_ndcg_scores):.4f} +/- {np.std(final_fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not final_all_feature_importances.empty:\n",
    "            final_mean_feature_importances = final_all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Tuned Model):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(final_mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed for the final tuned model evaluation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nSkipping final model evaluation due to earlier issues or no tuned parameters found.\")\n",
    "\n",
    "# --- End of Notebook ---\n",
    "# Next steps would involve:\n",
    "# 1. More sophisticated feature engineering and selection.\n",
    "# 2. Training the best model on the full (sampled) data or even a larger fraction.\n",
    "# 3. Preparing the test data similarly.\n",
    "# 4. Generating predictions for the test set.\n",
    "# 5. Creating the Kaggle submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Test Data Preparation and Kaggle Submission ---\n",
      "Loading test data...\n",
      "Loaded test dataset with shape: (4959183, 50)\n",
      "\n",
      "Preprocessing test data...\n",
      "Using feature columns: ['site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'orig_destination_distance']\n",
      "Imputing missing values in test data using training set medians...\n",
      "NaNs remaining in X_test after imputation: 0\n",
      "\n",
      "Training final model on the full sampled training data (df_sample)...\n",
      "Final model parameters for prediction model:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model on 178353 samples, validating on 19653 samples.\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007073 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2082\n",
      "[LightGBM] [Info] Number of data points in the train set: 178353, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[19]\tvalid_0's ndcg@5: 0.380471\n",
      "Final model training completed.\n",
      "\n",
      "Making predictions on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/DM2/lib/python3.13/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting predictions for submission...\n",
      "\n",
      "Submission file 'submission.csv' created successfully.\n",
      "   SearchId  PropertyId\n",
      "0         1       54937\n",
      "1         1       99484\n",
      "2         1       61934\n",
      "3         1       82231\n",
      "4         1       63894\n"
     ]
    }
   ],
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission ---\n",
    "\n",
    "if 'df_sample' in locals() and df_sample is not None and \\\n",
    "   'X' in locals() and X is not None and \\\n",
    "   'best_params_from_tuning' in locals() and best_params_from_tuning:\n",
    "\n",
    "    print(\"\\n--- 8. Test Data Preparation and Kaggle Submission ---\")\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    TEST_FILE = os.path.join(DATA_DIR, 'test.csv')\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "\n",
    "        df_test = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Test file not found at {TEST_FILE}\")\n",
    "        df_test = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during test data loading: {e}\")\n",
    "        df_test = None\n",
    "\n",
    "    if df_test is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\nPreprocessing test data...\")\n",
    "        # Use the same feature_columns as defined for training\n",
    "        if 'feature_columns' not in locals() or not feature_columns:\n",
    "            print(\"Error: feature_columns not defined. Cannot preprocess test data.\")\n",
    "            df_test_processed = None\n",
    "        else:\n",
    "            print(f\"Using feature columns: {feature_columns}\")\n",
    "            X_test = df_test[feature_columns].copy() # Ensure it's a copy\n",
    "\n",
    "            # Impute missing values using medians from the TRAINING sample (X)\n",
    "            # This is crucial to prevent data leakage.\n",
    "            print(\"Imputing missing values in test data using training set medians...\")\n",
    "            for col in X_test.columns:\n",
    "                if X_test[col].isnull().any():\n",
    "                    if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                        # Get median from the original X (training sample before it was split into folds)\n",
    "                        train_median = X[col].median() # X should be the full sample used for training/tuning\n",
    "                        X_test[col] = X_test[col].fillna(train_median)\n",
    "                        # print(f\"Imputed missing values in test column {col} with training median: {train_median}\")\n",
    "                    # else: # For categorical, use mode from training set\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col] = X_test[col].fillna(train_mode)\n",
    "            \n",
    "            print(\"NaNs remaining in X_test after imputation:\", X_test.isnull().sum().sum())\n",
    "            df_test_processed = True\n",
    "\n",
    "\n",
    "        if df_test_processed:\n",
    "            # --- 8c. Train Final Model on Full Sampled Data (df_sample) ---\n",
    "            print(\"\\nTraining final model on the full sampled training data (df_sample)...\")\n",
    "            \n",
    "            # Parameters for the final model\n",
    "            final_model_params = {\n",
    "                'objective': 'lambdarank',\n",
    "                'metric': 'ndcg',\n",
    "                'label_gain': [0, 1, 5], # If using remapped (0,1,2) relevance, gain still [0,1,5]\n",
    "                'eval_at': [5],\n",
    "                'importance_type': 'gain',\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "            final_model_params.update(best_params_from_tuning)\n",
    "            if 'n_estimators' not in final_model_params or final_model_params['n_estimators'] < 50:\n",
    "                final_model_params['n_estimators'] = 300 # A reasonable default for early stopping\n",
    "            \n",
    "            print(\"Final model parameters for prediction model:\")\n",
    "            print(final_model_params)\n",
    "\n",
    "            # Data for final model training\n",
    "            X_full_sample = X # This is df_sample[feature_columns] with imputations\n",
    "            y_full_sample = y # This is df_sample['relevance'] (remapped to 0,1,2 if you implemented that)\n",
    "            groups_full_sample = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            final_model = lgb.LGBMRanker(**final_model_params)\n",
    "            \n",
    "            # For the final model, we can use a small portion of df_sample as an eval set for early stopping\n",
    "            # This is better than no early stopping.\n",
    "            temp_df_for_final_split = pd.DataFrame({\n",
    "                'srch_id': df_sample['srch_id'],\n",
    "                'index_orig': df_sample.index\n",
    "            }).drop_duplicates(subset=['srch_id'])\n",
    "\n",
    "            final_train_srch_ids, final_val_srch_ids = np.split(\n",
    "                temp_df_for_final_split['srch_id'].sample(frac=1, random_state=RANDOM_STATE),\n",
    "                [int(0.9 * len(temp_df_for_final_split))] # 90/10 split for final model's early stopping\n",
    "            )\n",
    "            \n",
    "            final_train_indices = df_sample[df_sample['srch_id'].isin(final_train_srch_ids)].index\n",
    "            final_val_indices = df_sample[df_sample['srch_id'].isin(final_val_srch_ids)].index\n",
    "\n",
    "            X_final_train, X_final_val = X_full_sample.loc[final_train_indices], X_full_sample.loc[final_val_indices]\n",
    "            y_final_train, y_final_val = y_full_sample.loc[final_train_indices], y_full_sample.loc[final_val_indices]\n",
    "            \n",
    "            groups_final_train = df_sample.loc[final_train_indices].groupby('srch_id').size().to_numpy()\n",
    "            groups_final_val = df_sample.loc[final_val_indices].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            if not X_final_val.empty and len(groups_final_val) > 0:\n",
    "                 print(f\"Fitting final model on {len(X_final_train)} samples, validating on {len(X_final_val)} samples.\")\n",
    "                 final_model.fit(\n",
    "                    X_final_train, y_final_train, group=groups_final_train,\n",
    "                    eval_set=[(X_final_val, y_final_val)],\n",
    "                    eval_group=[groups_final_val],\n",
    "                    eval_metric='ndcg',\n",
    "                    callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "                )\n",
    "            else: # Fallback if validation set is too small or problematic\n",
    "                print(\"Validation set for final model is empty/problematic, fitting on all sampled data without early stopping.\")\n",
    "                final_model_params.pop('eval_set', None) # Remove eval params if not using\n",
    "                final_model_params.pop('eval_group', None)\n",
    "                final_model_params.pop('eval_metric', None)\n",
    "                final_model_params.pop('callbacks', None) # No early stopping\n",
    "                # Ensure n_estimators is set to a fixed number if no early stopping\n",
    "                final_model_params['n_estimators'] = best_params_from_tuning.get('n_estimators', 300) # Use tuned or default\n",
    "                final_model = lgb.LGBMRanker(**final_model_params)\n",
    "                final_model.fit(X_full_sample, y_full_sample, group=groups_full_sample)\n",
    "\n",
    "            print(\"Final model training completed.\")\n",
    "\n",
    "            # --- 8d. Make Predictions on Test Data ---\n",
    "            print(\"\\nMaking predictions on the test set...\")\n",
    "            test_predictions = final_model.predict(X_test)\n",
    "            df_test['predicted_score'] = test_predictions\n",
    "\n",
    "            # --- 8e. Format Predictions for Submission ---\n",
    "            print(\"\\nFormatting predictions for submission...\")\n",
    "            submission_list = []\n",
    "            # Group by srch_id and sort by predicted_score\n",
    "            for srch_id, group_df in df_test.groupby('srch_id'):\n",
    "                # Sort properties within each search by the predicted score in descending order\n",
    "                ranked_properties = group_df.sort_values('predicted_score', ascending=False)\n",
    "                for _, row in ranked_properties.iterrows():\n",
    "                    submission_list.append({'srch_id': int(row['srch_id']), 'prop_id': int(row['prop_id'])})\n",
    "            \n",
    "            df_submission = pd.DataFrame(submission_list)\n",
    "\n",
    "            # --- 8f. Create Submission File ---\n",
    "            SUBMISSION_FILE = 'submission.csv'\n",
    "            df_submission.to_csv(SUBMISSION_FILE, index=False)\n",
    "            print(f\"\\nSubmission file '{SUBMISSION_FILE}' created successfully.\")\n",
    "            print(df_submission.head())\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping prediction and submission due to test data processing issues.\")\n",
    "    else:\n",
    "        print(\"Skipping submission generation due to test data loading issues.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Kaggle submission part: Prerequisite data (df_sample, X, best_params_from_tuning) not available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
