{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:27:33.161825Z",
     "start_time": "2025-05-13T12:27:01.532981Z"
    }
   },
   "source": [
    "\"\"\"\n",
    "LambdaMART Model for Expedia Hotel Booking Prediction\n",
    "\n",
    "Assignment 2: Data Mining Techniques, Vrije Universiteit Amsterdam\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "TRAIN_FILE = '../data/training_set_VU_DM_imputed.csv'\n",
    "\n",
    "N_FOLDS = 5 # For GroupKFold cross-validation\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- 1. Load Data ---\n",
    "print(\"Loading training data...\")\n",
    "\n",
    "df_train_full = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# --- 2. Create Relevance Score ---\n",
    "if df_train_full is not None:\n",
    "    print(\"\\nCreating relevance score...\")\n",
    "    # 5 for booking, 1 for click (and not booked), 0 otherwise\n",
    "    df_train_full['relevance'] = 0\n",
    "    df_train_full.loc[df_train_full['click_bool'] == 1, 'relevance'] = 1\n",
    "    df_train_full.loc[df_train_full['booking_bool'] == 1, 'relevance'] = 2\n",
    "    print(\"Relevance score distribution:\")\n",
    "    print(df_train_full['relevance'].value_counts())\n",
    "else:\n",
    "    print(\"Skipping relevance score creation due to data loading issues.\")\n",
    "\n",
    "# --- 3. Data Sampling (Group-aware) ---\n",
    "if df_train_full is not None:\n",
    "    print(f\"\\nSampling {SAMPLE_FRACTION*100}% of the data based on srch_id...\")\n",
    "    unique_srch_ids = df_train_full['srch_id'].unique()\n",
    "    sampled_srch_ids = np.random.choice(unique_srch_ids, size=int(len(unique_srch_ids) * SAMPLE_FRACTION), replace=False)\n",
    "\n",
    "    df_sample = df_train_full[df_train_full['srch_id'].isin(sampled_srch_ids)].copy()\n",
    "    print(f\"Sampled data shape: {df_sample.shape}\")\n",
    "    # Free up memory from the full dataframe if no longer needed for this notebook scope\n",
    "    # del df_train_full \n",
    "else:\n",
    "    print(\"Skipping sampling due to data loading issues.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training data...\n",
      "\n",
      "Creating relevance score...\n",
      "Relevance score distribution:\n",
      "relevance\n",
      "0    4736468\n",
      "2     138390\n",
      "1      83489\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sampling 4.0% of the data based on srch_id...\n",
      "Sampled data shape: (198439, 55)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:27:49.018571Z",
     "start_time": "2025-05-13T12:27:46.771586Z"
    }
   },
   "source": [
    "df_train_full"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "0              1  2013-04-04 08:32:15       12                          187   \n",
       "1              1  2013-04-04 08:32:15       12                          187   \n",
       "2              1  2013-04-04 08:32:15       12                          187   \n",
       "3              1  2013-04-04 08:32:15       12                          187   \n",
       "4              1  2013-04-04 08:32:15       12                          187   \n",
       "...          ...                  ...      ...                          ...   \n",
       "4958342   332785  2013-06-30 19:55:18        5                          219   \n",
       "4958343   332785  2013-06-30 19:55:18        5                          219   \n",
       "4958344   332785  2013-06-30 19:55:18        5                          219   \n",
       "4958345   332785  2013-06-30 19:55:18        5                          219   \n",
       "4958346   332785  2013-06-30 19:55:18        5                          219   \n",
       "\n",
       "         visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  \\\n",
       "0                            NaN                   NaN              219   \n",
       "1                            NaN                   NaN              219   \n",
       "2                            NaN                   NaN              219   \n",
       "3                            NaN                   NaN              219   \n",
       "4                            NaN                   NaN              219   \n",
       "...                          ...                   ...              ...   \n",
       "4958342                      NaN                   NaN              219   \n",
       "4958343                      NaN                   NaN              219   \n",
       "4958344                      NaN                   NaN              219   \n",
       "4958345                      NaN                   NaN              219   \n",
       "4958346                      NaN                   NaN              219   \n",
       "\n",
       "         prop_id  prop_starrating  prop_review_score  ...  comp7_rate  \\\n",
       "0            893                3                3.5  ...         NaN   \n",
       "1          10404                4                4.0  ...         NaN   \n",
       "2          21315                3                4.5  ...         NaN   \n",
       "3          27348                2                4.0  ...         NaN   \n",
       "4          29604                4                3.5  ...         NaN   \n",
       "...          ...              ...                ...  ...         ...   \n",
       "4958342    77700                3                4.0  ...         NaN   \n",
       "4958343    88083                3                4.0  ...         NaN   \n",
       "4958344    94508                3                3.5  ...         NaN   \n",
       "4958345   128360                3                5.0  ...         NaN   \n",
       "4958346   134949                3                2.5  ...         NaN   \n",
       "\n",
       "         comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "0              NaN                      NaN         0.0        0.0   \n",
       "1              NaN                      NaN         0.0        0.0   \n",
       "2              NaN                      NaN         0.0        0.0   \n",
       "3              NaN                      NaN        -1.0        0.0   \n",
       "4              NaN                      NaN         0.0        0.0   \n",
       "...            ...                      ...         ...        ...   \n",
       "4958342        NaN                      NaN         NaN        NaN   \n",
       "4958343        NaN                      NaN         NaN        NaN   \n",
       "4958344        NaN                      NaN         NaN        NaN   \n",
       "4958345        NaN                      NaN         NaN        NaN   \n",
       "4958346        NaN                      NaN         NaN        NaN   \n",
       "\n",
       "         comp8_rate_percent_diff  click_bool  gross_bookings_usd  \\\n",
       "0                            NaN           0                 NaN   \n",
       "1                            NaN           0                 NaN   \n",
       "2                            NaN           0                 NaN   \n",
       "3                            5.0           0                 NaN   \n",
       "4                            NaN           0                 NaN   \n",
       "...                          ...         ...                 ...   \n",
       "4958342                      NaN           0                 NaN   \n",
       "4958343                      NaN           0                 NaN   \n",
       "4958344                      NaN           0                 NaN   \n",
       "4958345                      NaN           1              157.84   \n",
       "4958346                      NaN           0                 NaN   \n",
       "\n",
       "         booking_bool  relevance  \n",
       "0                   0          0  \n",
       "1                   0          0  \n",
       "2                   0          0  \n",
       "3                   0          0  \n",
       "4                   0          0  \n",
       "...               ...        ...  \n",
       "4958342             0          0  \n",
       "4958343             0          0  \n",
       "4958344             0          0  \n",
       "4958345             1          2  \n",
       "4958346             0          0  \n",
       "\n",
       "[4958347 rows x 55 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>893</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>10404</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>21315</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>27348</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-04-04 08:32:15</td>\n",
       "      <td>12</td>\n",
       "      <td>187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>29604</td>\n",
       "      <td>4</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958342</th>\n",
       "      <td>332785</td>\n",
       "      <td>2013-06-30 19:55:18</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>77700</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958343</th>\n",
       "      <td>332785</td>\n",
       "      <td>2013-06-30 19:55:18</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>88083</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958344</th>\n",
       "      <td>332785</td>\n",
       "      <td>2013-06-30 19:55:18</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>94508</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958345</th>\n",
       "      <td>332785</td>\n",
       "      <td>2013-06-30 19:55:18</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>128360</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>157.84</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958346</th>\n",
       "      <td>332785</td>\n",
       "      <td>2013-06-30 19:55:18</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>134949</td>\n",
       "      <td>3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4958347 rows × 55 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:27:49.274793Z",
     "start_time": "2025-05-13T12:27:49.107736Z"
    }
   },
   "source": [
    "df_sample"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         srch_id            date_time  site_id  visitor_location_country_id  \\\n",
       "614           52  2013-06-09 09:22:16       16                           31   \n",
       "615           52  2013-06-09 09:22:16       16                           31   \n",
       "616           52  2013-06-09 09:22:16       16                           31   \n",
       "617           52  2013-06-09 09:22:16       16                           31   \n",
       "618           52  2013-06-09 09:22:16       16                           31   \n",
       "...          ...                  ...      ...                          ...   \n",
       "4957901   332747  2013-03-03 13:03:51        5                          219   \n",
       "4957902   332747  2013-03-03 13:03:51        5                          219   \n",
       "4957903   332747  2013-03-03 13:03:51        5                          219   \n",
       "4957904   332747  2013-03-03 13:03:51        5                          219   \n",
       "4957905   332747  2013-03-03 13:03:51        5                          219   \n",
       "\n",
       "         visitor_hist_starrating  visitor_hist_adr_usd  prop_country_id  \\\n",
       "614                          NaN                   NaN              215   \n",
       "615                          NaN                   NaN              215   \n",
       "616                          NaN                   NaN              215   \n",
       "617                          NaN                   NaN              215   \n",
       "618                          NaN                   NaN              215   \n",
       "...                          ...                   ...              ...   \n",
       "4957901                      NaN                   NaN              219   \n",
       "4957902                      NaN                   NaN              219   \n",
       "4957903                      NaN                   NaN              219   \n",
       "4957904                      NaN                   NaN              219   \n",
       "4957905                      NaN                   NaN              219   \n",
       "\n",
       "         prop_id  prop_starrating  prop_review_score  ...  comp7_rate  \\\n",
       "614          462                4                4.0  ...         0.0   \n",
       "615         9193                3                4.0  ...         0.0   \n",
       "616        15671                4                4.5  ...         0.0   \n",
       "617        23365                3                3.5  ...         NaN   \n",
       "618        32237                4                4.5  ...         0.0   \n",
       "...          ...              ...                ...  ...         ...   \n",
       "4957901   124240                4                4.0  ...         NaN   \n",
       "4957902   124643                0                2.5  ...         NaN   \n",
       "4957903   126405                3                3.5  ...         NaN   \n",
       "4957904   128982                3                4.0  ...         NaN   \n",
       "4957905   135081                3                4.5  ...         NaN   \n",
       "\n",
       "         comp7_inv  comp7_rate_percent_diff  comp8_rate  comp8_inv  \\\n",
       "614            0.0                      NaN         NaN        NaN   \n",
       "615            0.0                      NaN         NaN        NaN   \n",
       "616            0.0                      NaN         NaN        NaN   \n",
       "617            0.0                      NaN         NaN        NaN   \n",
       "618            0.0                      NaN         NaN        NaN   \n",
       "...            ...                      ...         ...        ...   \n",
       "4957901        NaN                      NaN         NaN        NaN   \n",
       "4957902        NaN                      NaN         NaN        NaN   \n",
       "4957903        NaN                      NaN         NaN        NaN   \n",
       "4957904        NaN                      NaN         NaN        NaN   \n",
       "4957905        NaN                      NaN         NaN        NaN   \n",
       "\n",
       "         comp8_rate_percent_diff  click_bool  gross_bookings_usd  \\\n",
       "614                          NaN           0                 NaN   \n",
       "615                          NaN           0                 NaN   \n",
       "616                          NaN           0                 NaN   \n",
       "617                          NaN           0                 NaN   \n",
       "618                          NaN           0                 NaN   \n",
       "...                          ...         ...                 ...   \n",
       "4957901                      NaN           0                 NaN   \n",
       "4957902                      NaN           0                 NaN   \n",
       "4957903                      NaN           0                 NaN   \n",
       "4957904                      NaN           0                 NaN   \n",
       "4957905                      NaN           0                 NaN   \n",
       "\n",
       "         booking_bool  relevance  \n",
       "614                 0          0  \n",
       "615                 0          0  \n",
       "616                 0          0  \n",
       "617                 0          0  \n",
       "618                 0          0  \n",
       "...               ...        ...  \n",
       "4957901             0          0  \n",
       "4957902             0          0  \n",
       "4957903             0          0  \n",
       "4957904             0          0  \n",
       "4957905             0          0  \n",
       "\n",
       "[198439 rows x 55 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>srch_id</th>\n",
       "      <th>date_time</th>\n",
       "      <th>site_id</th>\n",
       "      <th>visitor_location_country_id</th>\n",
       "      <th>visitor_hist_starrating</th>\n",
       "      <th>visitor_hist_adr_usd</th>\n",
       "      <th>prop_country_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>prop_starrating</th>\n",
       "      <th>prop_review_score</th>\n",
       "      <th>...</th>\n",
       "      <th>comp7_rate</th>\n",
       "      <th>comp7_inv</th>\n",
       "      <th>comp7_rate_percent_diff</th>\n",
       "      <th>comp8_rate</th>\n",
       "      <th>comp8_inv</th>\n",
       "      <th>comp8_rate_percent_diff</th>\n",
       "      <th>click_bool</th>\n",
       "      <th>gross_bookings_usd</th>\n",
       "      <th>booking_bool</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>52</td>\n",
       "      <td>2013-06-09 09:22:16</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>462</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>52</td>\n",
       "      <td>2013-06-09 09:22:16</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>9193</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>52</td>\n",
       "      <td>2013-06-09 09:22:16</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>15671</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>52</td>\n",
       "      <td>2013-06-09 09:22:16</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>23365</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>52</td>\n",
       "      <td>2013-06-09 09:22:16</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>215</td>\n",
       "      <td>32237</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957901</th>\n",
       "      <td>332747</td>\n",
       "      <td>2013-03-03 13:03:51</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>124240</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957902</th>\n",
       "      <td>332747</td>\n",
       "      <td>2013-03-03 13:03:51</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>124643</td>\n",
       "      <td>0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957903</th>\n",
       "      <td>332747</td>\n",
       "      <td>2013-03-03 13:03:51</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>126405</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957904</th>\n",
       "      <td>332747</td>\n",
       "      <td>2013-03-03 13:03:51</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>128982</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4957905</th>\n",
       "      <td>332747</td>\n",
       "      <td>2013-03-03 13:03:51</td>\n",
       "      <td>5</td>\n",
       "      <td>219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>219</td>\n",
       "      <td>135081</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198439 rows × 55 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:27:51.452684Z",
     "start_time": "2025-05-13T12:27:51.395338Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "# --- 4. Initial Feature Selection (Placeholder) ---\n",
    "# This will be refined by your friend. For now, using a subset of potentially useful features.\n",
    "if 'df_sample' in locals() and df_sample is not None:\n",
    "    print(\"\\nDefining initial feature set...\")\n",
    "    # Features identified as potentially important or commonly used, excluding IDs and target-leaking columns\n",
    "    # Also excluding competitor columns with high missing rates for now, and user history due to high missingness\n",
    "    feature_columns = [\n",
    "        'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
    "        'prop_starrating', 'prop_review_score', 'prop_brand_bool',\n",
    "        'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price',\n",
    "        'price_usd', 'promotion_flag', 'srch_destination_id',\n",
    "        'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count',\n",
    "        'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool',\n",
    "        'orig_destination_distance'\n",
    "        # 'srch_query_affinity_score' # high missingness\n",
    "        # Add more features here as EDA suggests and after handling missing values\n",
    "    ]\n",
    "\n",
    "    # For ranking, we need features (X), relevance (y), and group/query_id\n",
    "    X = df_sample[feature_columns]\n",
    "    y = df_sample['relevance']\n",
    "    groups = df_sample.groupby('srch_id').size().to_numpy() # Size of each group\n",
    "\n",
    "    print(f\"Selected {len(feature_columns)} features.\")\n",
    "    print(\"Feature columns:\", feature_columns)\n",
    "    print(f\"Shape of X: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape}\")\n",
    "    print(f\"Number of groups: {len(groups)}, Min group size: {groups.min()}, Max group size: {groups.max()}\")\n",
    "\n",
    "    # Handle Missing Values (Simple Imputation for now)\n",
    "    # For a proper model, more sophisticated imputation or feature engineering for missingness is needed.\n",
    "    print(\"\\nHandling missing values (simple median imputation for numerical)...\")\n",
    "    for col in X.columns:\n",
    "        if X[col].isnull().any():\n",
    "            if pd.api.types.is_numeric_dtype(X[col]):\n",
    "                X[col] = X[col].fillna(X[col].median())\n",
    "                print(f\"Imputed missing values in {col} with median.\")\n",
    "            # else: # For categorical, fill with mode or a specific placeholder\n",
    "            #     X[col] = X[col].fillna(X[col].mode()[0])\n",
    "\n",
    "    # Check if any NaNs remain (should ideally be none for numeric after this)\n",
    "    print(\"NaNs remaining in X after imputation:\", X.isnull().sum().sum())\n",
    "\n",
    "else:\n",
    "    print(\"Skipping feature selection due to data sampling issues.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Defining initial feature set...\n",
      "Selected 19 features.\n",
      "Feature columns: ['site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'orig_destination_distance']\n",
      "Shape of X: (198439, 19)\n",
      "Shape of y: (198439,)\n",
      "Number of groups: 7991, Min group size: 5, Max group size: 36\n",
      "\n",
      "Handling missing values (simple median imputation for numerical)...\n",
      "NaNs remaining in X after imputation: 0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:28:02.285367Z",
     "start_time": "2025-05-13T12:27:53.970693Z"
    }
   },
   "source": [
    "# --- 5. Model Training (LGBMRanker) ---\n",
    "# This section will contain the training loop using GroupKFold\n",
    "\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None: # Use df_sample for srch_id\n",
    "    print(\"\\n--- 5. Cross-Validation with GroupKFold ---\")\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    \n",
    "    fold_ndcg_scores = []\n",
    "    all_feature_importances = pd.DataFrame()\n",
    "\n",
    "    # The groups parameter for gkf.split should be the srch_id for each row in X\n",
    "    # It ensures that rows with the same srch_id are not split across train/test in a fold.\n",
    "    unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        # Group counts for the current training and validation fold\n",
    "        # Need to use the original df_sample with srch_id to correctly form groups for the subsets\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "        \n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "        \n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "\n",
    "        ranker_cv = lgb.LGBMRanker(\n",
    "            objective='lambdarank',\n",
    "            metric='ndcg',\n",
    "            label_gain=[0, 1, 5], # Corresponds to relevance 0, 1, 5\n",
    "            eval_at=[5], # For NDCG@5\n",
    "            n_estimators=100, \n",
    "            learning_rate=0.1,\n",
    "            importance_type='gain',\n",
    "            random_state=RANDOM_STATE + fold, \n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        print(f\"Training LGBMRanker for fold {fold+1}...\")\n",
    "        ranker_cv.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg', \n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if ranker_cv.evals_result_ and 'valid_0' in ranker_cv.evals_result_ and 'ndcg@5' in ranker_cv.evals_result_['valid_0']:\n",
    "            ndcg_at_5 = ranker_cv.evals_result_['valid_0']['ndcg@5'][-1] \n",
    "            fold_ndcg_scores.append(ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} NDCG@5: {ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': ranker_cv.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            all_feature_importances = pd.concat([all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1}. Skipping score for this fold.\")\n",
    "\n",
    "\n",
    "    if fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 across {len(fold_ndcg_scores)} successfully evaluated folds: {np.mean(fold_ndcg_scores):.4f} +/- {np.std(fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not all_feature_importances.empty:\n",
    "            mean_feature_importances = all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Cross-Validation):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed with NDCG scores.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping model training and cross-validation due to earlier data processing issues.\")\n",
    "\n",
    "\n",
    "# --- 6. Hyperparameter Tuning with RandomizedSearchCV ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None:\n",
    "    print(\"\\n--- 6. Hyperparameter Tuning with RandomizedSearchCV ---\")\n",
    "    from sklearn.model_selection import RandomizedSearchCV\n",
    "    from scipy.stats import randint as sp_randint\n",
    "    from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "    param_dist = {\n",
    "        'n_estimators': sp_randint(100, 500),\n",
    "        'learning_rate': sp_uniform(0.01, 0.19), # Upper bound <0.2 for uniform\n",
    "        'num_leaves': sp_randint(20, 100),      \n",
    "        'max_depth': sp_randint(3, 12),        \n",
    "        'min_child_samples': sp_randint(5, 50), \n",
    "        'subsample': sp_uniform(0.6, 0.4),      # Sum of loc + scale should be <= 1.0. Here, 0.6 + 0.4 = 1.0\n",
    "        'colsample_bytree': sp_uniform(0.6, 0.4), \n",
    "        'reg_alpha': sp_uniform(0, 1),          \n",
    "        'reg_lambda': sp_uniform(0, 1),         \n",
    "    }\n",
    "\n",
    "    base_ranker = lgb.LGBMRanker(\n",
    "        objective='lambdarank',\n",
    "        metric='ndcg', # LightGBM will use this for its internal evaluation\n",
    "        label_gain=[0, 1, 5],\n",
    "        eval_at=[5],\n",
    "        importance_type='gain',\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1 # Be cautious with -1 for n_jobs in RandomizedSearchCV if memory is an issue\n",
    "    )\n",
    "\n",
    "    gkf_for_tuning = GroupKFold(n_splits=3) # Using 3 splits for tuning to speed it up\n",
    "\n",
    "    # RandomizedSearchCV setup\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=base_ranker,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,  # Number of parameter settings sampled. Increase for more thorough search.\n",
    "                    # Set to a small number like 5-10 for quick test, 25-50 for better search.\n",
    "        cv=list(gkf_for_tuning.split(X, y, groups=df_sample['srch_id'])), # Pass the list of splits\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=1, # Start with 1 to avoid potential memory issues, then try increasing.\n",
    "        verbose=2,\n",
    "        # scoring: If None, estimator's score method is used. LGBMRanker's score method should work.\n",
    "        # It calculates NDCG@eval_at based on its parameters.\n",
    "        refit=True # Refits the best estimator on the whole dataset (X,y) passed to fit.\n",
    "                   # For ranking, this full dataset refit will also need group info.\n",
    "    )\n",
    "\n",
    "    print(\"Starting RandomizedSearchCV for hyperparameter tuning...\")\n",
    "    # Pass `groups` to `fit`. This will be used by GroupKFold inside RandomizedSearchCV.\n",
    "    # And `LGBMRanker.fit` will also receive this `groups` argument for each fold.\n",
    "    \n",
    "    # For early stopping inside RandomizedSearchCV, you'd typically pass fit_params.\n",
    "    # This is more complex because eval_set/eval_group change per fold.\n",
    "    # For now, n_estimators is part of the search space.\n",
    "    \n",
    "    best_params_from_tuning = {}\n",
    "    try:\n",
    "        # RandomizedSearchCV will use the `groups` for splitting via the `cv` object\n",
    "        # And `LGBMRanker.fit` will receive the `group` parameter for each fold.\n",
    "        random_search.fit(X, y, groups=df_sample['srch_id']) \n",
    "        \n",
    "        print(\"\\nBest parameters found by RandomizedSearchCV:\")\n",
    "        print(random_search.best_params_)\n",
    "        # The best_score_ will be based on the internal scoring of LGBMRanker (NDCG@5 here)\n",
    "        print(f\"Best score from RandomizedSearchCV (NDCG@5): {random_search.best_score_:.4f}\")\n",
    "        best_params_from_tuning = random_search.best_params_\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during RandomizedSearchCV: {e}\")\n",
    "        print(\"Falling back to default parameters for the final model evaluation.\")\n",
    "        # Re-initialize with default in case of error\n",
    "        best_params_from_tuning = { \n",
    "            'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "            'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "            'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "        }\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping hyperparameter tuning due to earlier data processing issues.\")\n",
    "    best_params_from_tuning = {\n",
    "        'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, \n",
    "        'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0,\n",
    "        'colsample_bytree':1.0, 'reg_alpha':0.0, 'reg_lambda':0.0\n",
    "    }\n",
    "\n",
    "\n",
    "# --- 7. Detailed Evaluation of Best Tuned Model ---\n",
    "if 'X' in locals() and 'y' in locals() and df_sample is not None and X is not None and best_params_from_tuning:\n",
    "    print(\"\\n--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\")\n",
    "\n",
    "    final_gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "    final_fold_ndcg_scores = []\n",
    "    final_all_feature_importances = pd.DataFrame()\n",
    "    \n",
    "    # Ensure unique_group_ids_for_splitting is available\n",
    "    if 'unique_group_ids_for_splitting' not in locals():\n",
    "        unique_group_ids_for_splitting = df_sample['srch_id']\n",
    "\n",
    "\n",
    "    final_ranker_params = {\n",
    "        'objective': 'lambdarank',\n",
    "        'metric': 'ndcg',\n",
    "        'label_gain': [0, 1, 5],\n",
    "        'eval_at': [5],\n",
    "        'importance_type': 'gain',\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_jobs': -1,\n",
    "    }\n",
    "    # Update with tuned parameters, ensuring they are valid\n",
    "    for key, value in best_params_from_tuning.items():\n",
    "        final_ranker_params[key] = value\n",
    "    \n",
    "    # Ensure n_estimators is present if not tuned or set to a low value by tuning\n",
    "    if 'n_estimators' not in final_ranker_params or final_ranker_params['n_estimators'] < 50:\n",
    "         final_ranker_params['n_estimators'] = 300 # Default if not well-tuned by a short search\n",
    "\n",
    "    print(\"\\nFinal model parameters for evaluation:\")\n",
    "    print(final_ranker_params)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(final_gkf.split(X, y, groups=unique_group_ids_for_splitting)):\n",
    "        print(f\"\\n--- Final Model Evaluation: Fold {fold+1}/{N_FOLDS} ---\")\n",
    "        \n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "        train_groups = df_sample.iloc[train_idx].groupby('srch_id').size().to_numpy()\n",
    "        val_groups = df_sample.iloc[val_idx].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, Num train groups: {len(train_groups)}\")\n",
    "        print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}, Num val groups: {len(val_groups)}\")\n",
    "\n",
    "        if len(train_groups) == 0 or len(val_groups) == 0 or X_train.empty or X_val.empty:\n",
    "            print(\"Skipping fold due to empty train or validation groups/data.\")\n",
    "            continue\n",
    "            \n",
    "        final_ranker = lgb.LGBMRanker(**final_ranker_params)\n",
    "\n",
    "        print(f\"Training final tuned LGBMRanker for fold {fold+1}...\")\n",
    "        final_ranker.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            group=train_groups,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            eval_group=[val_groups],\n",
    "            eval_metric='ndcg',\n",
    "            callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "        )\n",
    "        \n",
    "        if final_ranker.evals_result_ and 'valid_0' in final_ranker.evals_result_ and 'ndcg@5' in final_ranker.evals_result_['valid_0']:\n",
    "            final_ndcg_at_5 = final_ranker.evals_result_['valid_0']['ndcg@5'][-1]\n",
    "            final_fold_ndcg_scores.append(final_ndcg_at_5)\n",
    "            print(f\"Fold {fold+1} (Tuned Model) NDCG@5: {final_ndcg_at_5:.4f}\")\n",
    "\n",
    "            fold_importances = pd.DataFrame({\n",
    "                'feature': X_train.columns,\n",
    "                'importance': final_ranker.feature_importances_,\n",
    "                'fold': fold + 1\n",
    "            })\n",
    "            final_all_feature_importances = pd.concat([final_all_feature_importances, fold_importances], ignore_index=True)\n",
    "        else:\n",
    "            print(f\"Could not retrieve NDCG@5 for fold {fold+1} of the tuned model.\")\n",
    "\n",
    "\n",
    "    if final_fold_ndcg_scores:\n",
    "        print(f\"\\nMean NDCG@5 for Tuned Model across {len(final_fold_ndcg_scores)} successfully evaluated folds: {np.mean(final_fold_ndcg_scores):.4f} +/- {np.std(final_fold_ndcg_scores):.4f}\")\n",
    "        \n",
    "        if not final_all_feature_importances.empty:\n",
    "            final_mean_feature_importances = final_all_feature_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "            print(\"\\nAverage Feature Importances (Tuned Model):\")\n",
    "            with pd.option_context('display.max_rows', 30):\n",
    "                display(final_mean_feature_importances.head(20))\n",
    "    else:\n",
    "        print(\"No folds were successfully processed for the final tuned model evaluation.\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nSkipping final model evaluation due to earlier issues or no tuned parameters found.\")\n",
    "\n",
    "# --- End of Notebook ---\n",
    "# Next steps would involve:\n",
    "# 1. More sophisticated feature engineering and selection.\n",
    "# 2. Training the best model on the full (sampled) data or even a larger fraction.\n",
    "# 3. Preparing the test data similarly.\n",
    "# 4. Generating predictions for the test set.\n",
    "# 5. Creating the Kaggle submission file."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Cross-Validation with GroupKFold ---\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009457 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2104\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's ndcg@5: 0.346871\n",
      "Fold 1 NDCG@5: 0.3379\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007729 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2101\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.335688\n",
      "Fold 2 NDCG@5: 0.3328\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011203 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2107\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's ndcg@5: 0.346766\n",
      "Fold 3 NDCG@5: 0.3407\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training LGBMRanker for fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006223 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2112\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.339617\n",
      "Fold 4 NDCG@5: 0.3379\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "X_train shape: (158748, 19), y_train shape: (158748,), Num train groups: 6392\n",
      "X_val shape: (39691, 19), y_val shape: (39691,), Num val groups: 1599\n",
      "Training LGBMRanker for fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005773 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2110\n",
      "[LightGBM] [Info] Number of data points in the train set: 158748, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's ndcg@5: 0.338168\n",
      "Fold 5 NDCG@5: 0.3339\n",
      "\n",
      "Mean NDCG@5 across 5 successfully evaluated folds: 0.3366 +/- 0.0029\n",
      "\n",
      "Average Feature Importances (Cross-Validation):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           6425.485961\n",
       "price_usd                      4805.286522\n",
       "prop_location_score1           2768.727621\n",
       "prop_log_historical_price      2192.717624\n",
       "prop_starrating                1917.017336\n",
       "prop_review_score              1067.955743\n",
       "orig_destination_distance       948.177619\n",
       "promotion_flag                  916.317781\n",
       "srch_destination_id             592.740678\n",
       "srch_booking_window             541.364219\n",
       "prop_country_id                 491.710098\n",
       "srch_length_of_stay             376.001439\n",
       "visitor_location_country_id     237.730740\n",
       "site_id                         220.742760\n",
       "prop_brand_bool                 131.627481\n",
       "srch_adults_count                73.448220\n",
       "srch_children_count              56.576361\n",
       "srch_room_count                  40.845460\n",
       "srch_saturday_night_bool         34.889260\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Hyperparameter Tuning with RandomizedSearchCV ---\n",
      "Starting RandomizedSearchCV for hyperparameter tuning...\n",
      "Error during RandomizedSearchCV: If no scoring is specified, the estimator passed should have a 'score' method. The estimator LGBMRanker(eval_at=[5], importance_type='gain', label_gain=[0, 1, 5],\n",
      "           metric='ndcg', n_jobs=-1, objective='lambdarank', random_state=42) does not.\n",
      "Falling back to default parameters for the final model evaluation.\n",
      "\n",
      "--- 7. Detailed Evaluation of Best Tuned Model (using GroupKFold) ---\n",
      "\n",
      "Final model parameters for evaluation:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n",
      "\n",
      "--- Final Model Evaluation: Fold 1/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007490 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2104\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[20]\tvalid_0's ndcg@5: 0.346871\n",
      "Fold 1 (Tuned Model) NDCG@5: 0.3379\n",
      "\n",
      "--- Final Model Evaluation: Fold 2/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012841 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2101\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.335688\n",
      "Fold 2 (Tuned Model) NDCG@5: 0.3328\n",
      "\n",
      "--- Final Model Evaluation: Fold 3/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.012645 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2107\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[35]\tvalid_0's ndcg@5: 0.346766\n",
      "Fold 3 (Tuned Model) NDCG@5: 0.3407\n",
      "\n",
      "--- Final Model Evaluation: Fold 4/5 ---\n",
      "X_train shape: (158752, 19), y_train shape: (158752,), Num train groups: 6393\n",
      "X_val shape: (39687, 19), y_val shape: (39687,), Num val groups: 1598\n",
      "Training final tuned LGBMRanker for fold 4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011433 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2112\n",
      "[LightGBM] [Info] Number of data points in the train set: 158752, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[16]\tvalid_0's ndcg@5: 0.339617\n",
      "Fold 4 (Tuned Model) NDCG@5: 0.3379\n",
      "\n",
      "--- Final Model Evaluation: Fold 5/5 ---\n",
      "X_train shape: (158748, 19), y_train shape: (158748,), Num train groups: 6392\n",
      "X_val shape: (39691, 19), y_val shape: (39691,), Num val groups: 1599\n",
      "Training final tuned LGBMRanker for fold 5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007481 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2110\n",
      "[LightGBM] [Info] Number of data points in the train set: 158748, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[32]\tvalid_0's ndcg@5: 0.338168\n",
      "Fold 5 (Tuned Model) NDCG@5: 0.3339\n",
      "\n",
      "Mean NDCG@5 for Tuned Model across 5 successfully evaluated folds: 0.3366 +/- 0.0029\n",
      "\n",
      "Average Feature Importances (Tuned Model):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "feature\n",
       "prop_location_score2           6425.485961\n",
       "price_usd                      4805.286522\n",
       "prop_location_score1           2768.727621\n",
       "prop_log_historical_price      2192.717624\n",
       "prop_starrating                1917.017336\n",
       "prop_review_score              1067.955743\n",
       "orig_destination_distance       948.177619\n",
       "promotion_flag                  916.317781\n",
       "srch_destination_id             592.740678\n",
       "srch_booking_window             541.364219\n",
       "prop_country_id                 491.710098\n",
       "srch_length_of_stay             376.001439\n",
       "visitor_location_country_id     237.730740\n",
       "site_id                         220.742760\n",
       "prop_brand_bool                 131.627481\n",
       "srch_adults_count                73.448220\n",
       "srch_children_count              56.576361\n",
       "srch_room_count                  40.845460\n",
       "srch_saturday_night_bool         34.889260\n",
       "Name: importance, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T12:37:31.984016Z",
     "start_time": "2025-05-13T12:28:08.904082Z"
    }
   },
   "source": [
    "# --- 8. Prepare Test Data and Generate Kaggle Submission ---\n",
    "\n",
    "if 'df_sample' in locals() and df_sample is not None and \\\n",
    "   'X' in locals() and X is not None and \\\n",
    "   'best_params_from_tuning' in locals() and best_params_from_tuning:\n",
    "\n",
    "    print(\"\\n--- 8. Test Data Preparation and Kaggle Submission ---\")\n",
    "\n",
    "    # --- 8a. Load Test Data ---\n",
    "    TEST_FILE = '../data/test_set_VU_DM.csv'\n",
    "    print(\"Loading test data...\")\n",
    "    try:\n",
    "\n",
    "        df_test = pd.read_csv(TEST_FILE)\n",
    "        print(f\"Loaded test dataset with shape: {df_test.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Test file not found at {TEST_FILE}\")\n",
    "        df_test = None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during test data loading: {e}\")\n",
    "        df_test = None\n",
    "\n",
    "    if df_test is not None:\n",
    "        # --- 8b. Preprocess Test Data ---\n",
    "        print(\"\\nPreprocessing test data...\")\n",
    "        # Use the same feature_columns as defined for training\n",
    "        if 'feature_columns' not in locals() or not feature_columns:\n",
    "            print(\"Error: feature_columns not defined. Cannot preprocess test data.\")\n",
    "            df_test_processed = None\n",
    "        else:\n",
    "            print(f\"Using feature columns: {feature_columns}\")\n",
    "            X_test = df_test[feature_columns].copy() # Ensure it's a copy\n",
    "\n",
    "            # Impute missing values using medians from the TRAINING sample (X)\n",
    "            # This is crucial to prevent data leakage.\n",
    "            print(\"Imputing missing values in test data using training set medians...\")\n",
    "            for col in X_test.columns:\n",
    "                if X_test[col].isnull().any():\n",
    "                    if pd.api.types.is_numeric_dtype(X_test[col]):\n",
    "                        # Get median from the original X (training sample before it was split into folds)\n",
    "                        train_median = X[col].median() # X should be the full sample used for training/tuning\n",
    "                        X_test[col] = X_test[col].fillna(train_median)\n",
    "                        # print(f\"Imputed missing values in test column {col} with training median: {train_median}\")\n",
    "                    # else: # For categorical, use mode from training set\n",
    "                    #     train_mode = X[col].mode()[0]\n",
    "                    #     X_test[col] = X_test[col].fillna(train_mode)\n",
    "            \n",
    "            print(\"NaNs remaining in X_test after imputation:\", X_test.isnull().sum().sum())\n",
    "            df_test_processed = True\n",
    "\n",
    "\n",
    "        if df_test_processed:\n",
    "            # --- 8c. Train Final Model on Full Sampled Data (df_sample) ---\n",
    "            print(\"\\nTraining final model on the full sampled training data (df_sample)...\")\n",
    "            \n",
    "            # Parameters for the final model\n",
    "            final_model_params = {\n",
    "                'objective': 'lambdarank',\n",
    "                'metric': 'ndcg',\n",
    "                'label_gain': [0, 1, 5], # If using remapped (0,1,2) relevance, gain still [0,1,5]\n",
    "                'eval_at': [5],\n",
    "                'importance_type': 'gain',\n",
    "                'random_state': RANDOM_STATE,\n",
    "                'n_jobs': -1,\n",
    "            }\n",
    "            final_model_params.update(best_params_from_tuning)\n",
    "            if 'n_estimators' not in final_model_params or final_model_params['n_estimators'] < 50:\n",
    "                final_model_params['n_estimators'] = 300 # A reasonable default for early stopping\n",
    "            \n",
    "            print(\"Final model parameters for prediction model:\")\n",
    "            print(final_model_params)\n",
    "\n",
    "            # Data for final model training\n",
    "            X_full_sample = X # This is df_sample[feature_columns] with imputations\n",
    "            y_full_sample = y # This is df_sample['relevance'] (remapped to 0,1,2 if you implemented that)\n",
    "            groups_full_sample = df_sample.groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            final_model = lgb.LGBMRanker(**final_model_params)\n",
    "            \n",
    "            # For the final model, we can use a small portion of df_sample as an eval set for early stopping\n",
    "            # This is better than no early stopping.\n",
    "            temp_df_for_final_split = pd.DataFrame({\n",
    "                'srch_id': df_sample['srch_id'],\n",
    "                'index_orig': df_sample.index\n",
    "            }).drop_duplicates(subset=['srch_id'])\n",
    "\n",
    "            final_train_srch_ids, final_val_srch_ids = np.split(\n",
    "                temp_df_for_final_split['srch_id'].sample(frac=1, random_state=RANDOM_STATE),\n",
    "                [int(0.9 * len(temp_df_for_final_split))] # 90/10 split for final model's early stopping\n",
    "            )\n",
    "            \n",
    "            final_train_indices = df_sample[df_sample['srch_id'].isin(final_train_srch_ids)].index\n",
    "            final_val_indices = df_sample[df_sample['srch_id'].isin(final_val_srch_ids)].index\n",
    "\n",
    "            X_final_train, X_final_val = X_full_sample.loc[final_train_indices], X_full_sample.loc[final_val_indices]\n",
    "            y_final_train, y_final_val = y_full_sample.loc[final_train_indices], y_full_sample.loc[final_val_indices]\n",
    "            \n",
    "            groups_final_train = df_sample.loc[final_train_indices].groupby('srch_id').size().to_numpy()\n",
    "            groups_final_val = df_sample.loc[final_val_indices].groupby('srch_id').size().to_numpy()\n",
    "\n",
    "            if not X_final_val.empty and len(groups_final_val) > 0:\n",
    "                 print(f\"Fitting final model on {len(X_final_train)} samples, validating on {len(X_final_val)} samples.\")\n",
    "                 final_model.fit(\n",
    "                    X_final_train, y_final_train, group=groups_final_train,\n",
    "                    eval_set=[(X_final_val, y_final_val)],\n",
    "                    eval_group=[groups_final_val],\n",
    "                    eval_metric='ndcg',\n",
    "                    callbacks=[lgb.early_stopping(10, verbose=1)]\n",
    "                )\n",
    "            else: # Fallback if validation set is too small or problematic\n",
    "                print(\"Validation set for final model is empty/problematic, fitting on all sampled data without early stopping.\")\n",
    "                final_model_params.pop('eval_set', None) # Remove eval params if not using\n",
    "                final_model_params.pop('eval_group', None)\n",
    "                final_model_params.pop('eval_metric', None)\n",
    "                final_model_params.pop('callbacks', None) # No early stopping\n",
    "                # Ensure n_estimators is set to a fixed number if no early stopping\n",
    "                final_model_params['n_estimators'] = best_params_from_tuning.get('n_estimators', 300) # Use tuned or default\n",
    "                final_model = lgb.LGBMRanker(**final_model_params)\n",
    "                final_model.fit(X_full_sample, y_full_sample, group=groups_full_sample)\n",
    "\n",
    "            print(\"Final model training completed.\")\n",
    "\n",
    "            # --- 8d. Make Predictions on Test Data ---\n",
    "            print(\"\\nMaking predictions on the test set...\")\n",
    "            test_predictions = final_model.predict(X_test)\n",
    "            df_test['predicted_score'] = test_predictions\n",
    "\n",
    "            # --- 8e. Format Predictions for Submission ---\n",
    "            print(\"\\nFormatting predictions for submission...\")\n",
    "            submission_list = []\n",
    "            # Group by srch_id and sort by predicted_score\n",
    "            for srch_id, group_df in df_test.groupby('srch_id'):\n",
    "                # Sort properties within each search by the predicted score in descending order\n",
    "                ranked_properties = group_df.sort_values('predicted_score', ascending=False)\n",
    "                for _, row in ranked_properties.iterrows():\n",
    "                    submission_list.append({'srch_id': int(row['srch_id']), 'prop_id': int(row['prop_id'])})\n",
    "            \n",
    "            df_submission = pd.DataFrame(submission_list)\n",
    "\n",
    "            # --- 8f. Create Submission File ---\n",
    "            SUBMISSION_FILE = '../data/submission.csv'\n",
    "            df_submission.to_csv(SUBMISSION_FILE, index=False)\n",
    "            print(f\"\\nSubmission file '{SUBMISSION_FILE}' created successfully.\")\n",
    "            print(df_submission.head())\n",
    "\n",
    "        else:\n",
    "            print(\"Skipping prediction and submission due to test data processing issues.\")\n",
    "    else:\n",
    "        print(\"Skipping submission generation due to test data loading issues.\")\n",
    "else:\n",
    "    print(\"\\nSkipping Kaggle submission part: Prerequisite data (df_sample, X, best_params_from_tuning) not available.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 8. Test Data Preparation and Kaggle Submission ---\n",
      "Loading test data...\n",
      "Loaded test dataset with shape: (4959183, 50)\n",
      "\n",
      "Preprocessing test data...\n",
      "Using feature columns: ['site_id', 'visitor_location_country_id', 'prop_country_id', 'prop_starrating', 'prop_review_score', 'prop_brand_bool', 'prop_location_score1', 'prop_location_score2', 'prop_log_historical_price', 'price_usd', 'promotion_flag', 'srch_destination_id', 'srch_length_of_stay', 'srch_booking_window', 'srch_adults_count', 'srch_children_count', 'srch_room_count', 'srch_saturday_night_bool', 'orig_destination_distance']\n",
      "Imputing missing values in test data using training set medians...\n",
      "NaNs remaining in X_test after imputation: 0\n",
      "\n",
      "Training final model on the full sampled training data (df_sample)...\n",
      "Final model parameters for prediction model:\n",
      "{'objective': 'lambdarank', 'metric': 'ndcg', 'label_gain': [0, 1, 5], 'eval_at': [5], 'importance_type': 'gain', 'random_state': 42, 'n_jobs': -1, 'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': -1, 'min_child_samples': 20, 'subsample': 1.0, 'colsample_bytree': 1.0, 'reg_alpha': 0.0, 'reg_lambda': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hiddemakimei/Library/Python/3.9/lib/python/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting final model on 178864 samples, validating on 19575 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007176 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2117\n",
      "[LightGBM] [Info] Number of data points in the train set: 178864, number of used features: 19\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "Early stopping, best iteration is:\n",
      "[5]\tvalid_0's ndcg@5: 0.349422\n",
      "Final model training completed.\n",
      "\n",
      "Making predictions on the test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.9/site-packages/lightgbm/sklearn.py:861: UserWarning: Found 'eval_at' in params. Will use it instead of 'eval_at' argument\n",
      "  _log_warning(f\"Found '{alias}' in params. Will use it instead of 'eval_at' argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatting predictions for submission...\n",
      "\n",
      "Submission file 'submission.csv' created successfully.\n",
      "   srch_id  prop_id\n",
      "0        1    54937\n",
      "1        1    61934\n",
      "2        1    99484\n",
      "3        1   123675\n",
      "4        1    63894\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
